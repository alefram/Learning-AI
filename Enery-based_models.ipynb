{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJPLI9ftMAaw",
    "tags": []
   },
   "source": [
    "# Energy-Based models in structured prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assignment of deep learning NYU course.\n",
    "\n",
    "Structured prediction broadly refers to any problem involving predicting structured values, as opposed to plain scalars. Examples of structured outputs include graphs and text.\n",
    "\n",
    "We're going to work with text. The task is to transcribe a word from an image. The difficulty here is that different words have different lengths, so we can't just have fixed number of outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw, ImageFont\n",
    "import string\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image # PIL is a library to process images\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Usefull methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot energies of the model\n",
    "def plot_energies(ce):\n",
    "    fig=plt.figure(dpi=200)\n",
    "    ax = plt.axes()\n",
    "    im = ax.imshow(ce.cpu().T)\n",
    "    \n",
    "    ax.set_xlabel('window locations →')\n",
    "    ax.set_ylabel('← classes')\n",
    "    ax.xaxis.set_label_position('top') \n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    cax = fig.add_axes([ax.get_position().x1+0.01,ax.get_position().y0,0.02,ax.get_position().height])\n",
    "    plt.colorbar(im, cax=cax) \n",
    "    \n",
    "def cross_entropy(energies, *args, **kwargs):\n",
    "    \"\"\" We use energies, and therefore we need to use log soft arg min instead\n",
    "        of log soft arg max. To do that we just multiply energies by -1. \"\"\"\n",
    "    return nn.functional.cross_entropy(-1 * energies, *args, **kwargs)\n",
    "\n",
    "def simple_collate_fn(samples):\n",
    "    images, annotations = zip(*samples)\n",
    "    images = list(images)\n",
    "    annotations = list(annotations)\n",
    "    annotations = list(map(lambda c : torch.tensor(ord(c) - ord('a')), annotations))\n",
    "    m_width = max(18, max([i.shape[1] for i in images]))\n",
    "    for i in range(len(images)):\n",
    "        images[i] = torch.nn.functional.pad(images[i], (0, m_width - images[i].shape[-1]))\n",
    "        \n",
    "    if len(images) == 1:\n",
    "        return images[0].unsqueeze(0), torch.stack(annotations)\n",
    "    else:\n",
    "        return torch.stack(images), torch.stack(annotations)\n",
    "    \n",
    "#train the model on the one-character dataset\n",
    "def train_model(model, epochs, dataloader, criterion, optimizer):\n",
    "    # TODO\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    for e in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for _, (img, target) in enumerate(dataloader):\n",
    "            img = img.unsqueeze(1).to(device)\n",
    "            target = target.to(device)\n",
    "            out = model(img)\n",
    "            # print(out.device)\n",
    "            # print(target.device)\n",
    "            loss = criterion(out[:, 0, :], target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        print('Epoch {}, Loss {}'.format(e+1, epoch_loss))\n",
    "        \n",
    "def get_accuracy(model, dataset):\n",
    "    cnt = 0\n",
    "    for i, l in dataset:\n",
    "        energies = model(i.unsqueeze(0).unsqueeze(0).cuda())[0, 0]\n",
    "        x = energies.argmin(dim=-1)\n",
    "        cnt += int(x == (ord(l[0]) - ord('a')))\n",
    "    return cnt / len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KREhDlCMAa7",
    "tags": []
   },
   "source": [
    "## Load data\n",
    "\n",
    "Dataset that creates images of random words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jg1AZlskMF8d",
    "outputId": "08801c0c-47ba-4bd7-92ec-5ad589949b76"
   },
   "outputs": [],
   "source": [
    "! mkdir fonts\n",
    "! curl --output fonts/font.zip https://www.fontsquirrel.com/fonts/download/Anonymous\n",
    "! unzip -n fonts/font.zip -d fonts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "id": "hhX6fPd7MAa7",
    "outputId": "8d242e4c-6c90-4273-c52e-f3150860b6c9"
   },
   "outputs": [],
   "source": [
    "simple_transforms = transforms.Compose([\n",
    "                                    transforms.ToTensor(), \n",
    "                                ])\n",
    "\n",
    "class SimpleWordsDataset(torch.utils.data.IterableDataset):\n",
    "\n",
    "    def __init__(self, max_length, len=100, jitter=False, noise=False):\n",
    "        self.max_length = max_length\n",
    "        self.transforms = transforms.ToTensor()\n",
    "        self.len = len\n",
    "        self.jitter = jitter\n",
    "        self.noise = noise\n",
    "  \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __iter__(self):\n",
    "        for _ in range(self.len):\n",
    "            text = ''.join([random.choice(string.ascii_lowercase) for i in range(self.max_length)])\n",
    "            img = self.draw_text(text, jitter=self.jitter, noise=self.noise)\n",
    "            \n",
    "            yield img, text\n",
    "  \n",
    "    def draw_text(self, text, length=None, jitter=False, noise=False):\n",
    "        if length == None:\n",
    "            length = 18 * len(text)\n",
    "  \n",
    "        img = Image.new('L', (length, 32))\n",
    "        fnt = ImageFont.truetype(\"fonts/Anonymous.ttf\", 20)\n",
    "\n",
    "        d = ImageDraw.Draw(img)\n",
    "        pos = (0, 5)\n",
    "    \n",
    "        if jitter:\n",
    "            pos = (random.randint(0, 7), 5)\n",
    "        else:\n",
    "            pos = (0, 5)\n",
    "        \n",
    "        d.text(pos, text, fill=1, font=fnt)\n",
    "\n",
    "        img = self.transforms(img)\n",
    "        img[img > 0] = 1 \n",
    "    \n",
    "        if noise:\n",
    "            img += torch.bernoulli(torch.ones_like(img) * 0.1)\n",
    "            img = img.clamp(0, 1)\n",
    "        \n",
    "\n",
    "        return img[0]\n",
    "\n",
    "sds = SimpleWordsDataset(1, jitter=True, noise=False)\n",
    "img = next(iter(sds))[0]\n",
    "print(img.shape)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEc69CZ_MAbK"
   },
   "source": [
    "We can look at what the entire alphabet looks like in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 640
    },
    "id": "j2QnA4u0MAbK",
    "outputId": "3de03a7e-7bb1-404e-855a-5110e13ae654"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 9, figsize=(12, 6), dpi=200)\n",
    "\n",
    "for i, c in enumerate(string.ascii_lowercase):\n",
    "    row = i // 9\n",
    "    col = i % 9\n",
    "    ax[row][col].imshow(sds.draw_text(c))\n",
    "    ax[row][col].axis('off')\n",
    "ax[2][8].axis('off')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SG9jGHccMAbL"
   },
   "source": [
    "We can also put the entire alphabet in one image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "id": "8BMT8uIFMAbM",
    "outputId": "f6466bf1-1d33-490e-8610-a4af6eda1798"
   },
   "outputs": [],
   "source": [
    "alphabet = sds.draw_text(string.ascii_lowercase, 340)\n",
    "plt.figure(dpi=200)\n",
    "plt.imshow(alphabet)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MreUfYMLMAbM",
    "tags": []
   },
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we define the model, we define the size of our alphabet. Our alphabet consists of lowercase English letters, and additionally a special character used for space between symbols or before and after the word. For the first part of this assignment, we don't need that extra character.\n",
    "\n",
    "Our end goal is to learn to transcribe words of arbitrary length. However, first, we pre-train our simple convolutional neural net to recognize single characters. In order to be able to use the same model for one character and for entire words, we are going to design the model in a way that makes sure that the output size for one character (or when input image size is 32x18) is 1x27, and Kx27 whenever the input image is wider. K here will depend on particular architecture of the network, and is affected by strides, poolings, among other things. \n",
    "A little bit more formally, our model $f_\\theta$, for an input image $x$ gives output energies $l = f_\\theta(x)$. If $x \\in \\mathbb{R}^{32 \\times 18}$, then $l \\in \\mathbb{R}^{1 \\times 27}$.\n",
    "If $x \\in \\mathbb{R}^{32 \\times 100}$ for example, our model may output $l \\in \\mathbb{R}^{10 \\times 27}$, where $l_i$ corresponds to a particular window in $x$, for example from $x_{0, 9i}$ to $x_{32, 9i + 18}$ (again, this will depend on the particular architecture).\n",
    "\n",
    "Below is a drawing that explains the sliding window concept. We use the same neural net with the same weights to get $l_1, l_2, l_3$, the only difference is receptive field. $l_1$ is looks at the leftmost part, at character 'c', $l_2$ looks at 'a', and $l_3$ looks at 't'. The receptive field may or may not overlap, depending on how you design your convolutions.\n",
    "\n",
    "![cat.png](https://i.imgur.com/JByfyKh.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQWNtXbkMAbO"
   },
   "outputs": [],
   "source": [
    "# constants for number of classes in total, and for the special extra character for empty space\n",
    "ALPHABET_SIZE = 27\n",
    "BETWEEN = 26\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nzj7g7fXMAbO"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class SimpleNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.window_size = 10\n",
    "        self.window_interval = 4\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64*8*(self.window_size//4), 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 27),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        n_windows = (x.shape[-1]-self.window_size) // self.window_interval\n",
    "        ret = torch.zeros(x.shape[0], n_windows, 27)\n",
    "        for i in range(n_windows):\n",
    "            temp = self.layer1(x[:, :, :, i*self.window_interval:i*self.window_interval+self.window_size])\n",
    "            temp = self.layer2(temp)\n",
    "            temp = self.layer3(temp)\n",
    "            temp = temp.view(-1, 64*8*(self.window_size//4))\n",
    "            temp = self.fc(temp)\n",
    "            ret[:, i, :] = temp\n",
    "        return ret.to(device)\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     x = self.cnn_block(x)\n",
    "    #     # after applying cnn_block, x.shape should be:\n",
    "    #     # batch_size, alphabet_size, 1, width\n",
    "\n",
    "\n",
    "    #     return x[:, :, 0, :].permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YbW54IbwMAbP"
   },
   "source": [
    "Let's initalize the model and apply it to the alphabet image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "id": "iKTeZ3V1MAbP",
    "outputId": "250c1237-0b15-43df-c086-248a01afd6bd"
   },
   "outputs": [],
   "source": [
    "model = SimpleNet()\n",
    "alphabet_energies = model(alphabet.view(1, 1, *alphabet.shape))\n",
    "\n",
    "    \n",
    "plot_energies(alphabet_energies[0].detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wcVdVdZCMAbQ"
   },
   "source": [
    "So far we only see random outputs, because the classifier is untrained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tm7ANbUIMAbQ",
    "tags": []
   },
   "source": [
    "## Training with one character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HN5j048fMAbR"
   },
   "source": [
    "Now we train the model we've created on a dataset where images contain only single characters. Note the changed cross_entropy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DccqG9qKMAbR",
    "outputId": "ad22e135-6355-4e60-b24c-0aaa17c7aec6"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "sds = SimpleWordsDataset(1, len=1000, jitter=True, noise=False)\n",
    "dataloader = torch.utils.data.DataLoader(sds, batch_size=16, num_workers=0, collate_fn=simple_collate_fn)\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "#initialize optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#train the model on the one-character dataset\n",
    "train_model(model, 10, dataloader, cross_entropy, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vBVKgV5dMAbR"
   },
   "outputs": [],
   "source": [
    "tds = SimpleWordsDataset(1, len=100)\n",
    "assert get_accuracy(model, tds) == 1.0, 'Your model doesn\\'t achieve 100% accuracy for 1 character'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7A8Z2pIMAbT"
   },
   "source": [
    "Now, to see how our model would work with more than one character, we apply the model to a bigger input - the image of the alphabet we saw earlier. We extract the energies for each window and show them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "k27gXaZgMAbT",
    "outputId": "88898f64-53aa-4aca-a4d7-0f5334e0a914"
   },
   "outputs": [],
   "source": [
    "alphabet_energies_post_train = model(alphabet.cuda().view(1, 1, *alphabet.shape))\n",
    "plot_energies(alphabet_energies_post_train[0].detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fV02yzglMAbT"
   },
   "source": [
    "Explain any classes that are lit up. What is still missing to be able to use it for transcription of words?\n",
    "\n",
    "Answer:\n",
    "\n",
    "    Because it is the energy map, so the lowest energy is the corresponding class. And the lit-up class is the class 27, which is the special character. So the special character is missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hnCgx5k-MAbT",
    "tags": []
   },
   "source": [
    "## Training with multiple characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to train our model to not only recognize the letters, but also to recognize space in-between so that we can use it for transcription later.\n",
    "\n",
    "This is where complications begin. When transcribing a word from an image, we don't know beforehand how long the word is going to be. We can use our convolutioThisnal neural network we've pretrained on single characters to get prediction of character probabilities for all the positions of an input window in the new input image, but we don't know beforehand how to match those predictions with the target label. Training with incorrect matching can lead to wrong model, so in order to be able to train a network to transcribe words, we need a way to find these pairings.\n",
    "\n",
    "![dl.png](https://i.imgur.com/7pnodfV.png)\n",
    "\n",
    "The importance of pairings can be demonstrated by the drawing above. If we map $l_1, l_2, l_3, l_4$ to 'c', 'a', 't', '_' respectively, we'll correctly train the system, but if we put $l_1, l_2, l_3, l_4$ with 'a', 'a', 't', 't', we'd have a very wrong classifier.\n",
    "\n",
    "To formalize this, we use energy-based models' framework. Let's define the energy $E(x, y, z)$ as the sum of cross-entropies for a particular pairing between probabilities our model gives for input image $x$ and text transcription $y$, and pairing $z$. $z$ is a function $z : \\{1, 2, \\dots, \\vert l \\vert \\} \\to \\{1, 2, \\dots, \\vert y \\vert)$, $l$ here is the energies output of our convolutional neural net $l = f_\\theta(x)$. $z$ maps each energy vector in $l$ to an element in the output sequence $y$. We want the mappings to make sense, so $z$ should be a non-decreasing function $z(i) \\leq z(i+1)$, and it shouldn't skip characters, i.e. $\\forall_i \\exists_j z(j)=i$.\n",
    "\n",
    "Energy is then $E(x, y, z) = C(z) + \\sum_{i=1}^{\\vert l \\vert} l_i[z(i)]$\n",
    ",  $C(z)$ is some extra term that allows us to penalize certain pairings, and $l_i[z(i)]$ is the energy of $z(i)$-th symbol on position $i$.\n",
    "\n",
    "In this particular context, we define $C(z)$ to be infinity for impossible pairings:\n",
    "$$C(z) = \\begin{cases}\n",
    "\\infty \\; \\text{if} \\; z(1) \\neq 1 \\vee z(\\vert l \\vert) \\neq \\vert y \\vert \\vee \\exists_{i, 1\\leq 1 \\leq \\vert l \\vert - 1} z(i) > z(i+1) \\vee z(i) < z(i+1) - 1\\\\\n",
    "0 \\; \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "Then, the free energy $F(x, y) = \\arg \\min_z E(x, y, z)$. In other words, the free energy is the energy of the best pairing between the probabilities provided by our model, and the target labels.\n",
    "\n",
    "When training, we are going to use cross-entropies along the best path: $\\ell(x, y, z) = \\sum_{i=1}^{\\vert l \\vert}H(y_{z(i)}, \\sigma(l_i))$, where $H$ is cross-entropy, $\\sigma$ is soft-argmin needed to convert energies to a distribution.\n",
    "\n",
    "First, let's write functions that would calculate the needed cross entropies $H(y_{z(i)}, \\sigma(l_i))$, and energies for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D5WJxWjZMAbV"
   },
   "outputs": [],
   "source": [
    "def build_path_matrix(energies, targets):\n",
    "    # inputs: \n",
    "    #    energies, shape is BATCH_SIZE x L x 27\n",
    "    #    targets, shape is BATCH_SIZE x T\n",
    "    # L is \\vert l \\vert\n",
    "    # T is \\vert y \\vert\n",
    "    # \n",
    "    # outputs:\n",
    "    #    a matrix of shape BATCH_SIZE x L x T\n",
    "    #    where output[i, j, k] = energies[i, j, targets[i, k]]\n",
    "    #\n",
    "    # Note: you're not allowed to use for loops. The calculation has to be vectorized.\n",
    "    # you may want to use repeat and repeat_interleave.\n",
    "    # TODO\n",
    "    target = targets.long().to(device)\n",
    "    filter_matrix = torch.nn.functional.one_hot(target, 27).permute(0, 2, 1).float()\n",
    "    output = torch.matmul(energies.to(device), filter_matrix)\n",
    "\n",
    "    return output\n",
    "\n",
    "def build_ce_matrix(energies, targets):\n",
    "    # inputs: \n",
    "    #    energies, shape is BATCH_SIZE x L x 27\n",
    "    #    targets, shape is BATCH_SIZE x T\n",
    "    # L is \\vert l \\vert\n",
    "    # T is \\vert y \\vert\n",
    "    # \n",
    "    # outputs:\n",
    "    #    a matrix ce of shape BATCH_SIZE x L x T\n",
    "    #    where ce[i, j, k] = cross_entropy(energies[i, j], targets[i, k])\n",
    "    #\n",
    "    # Note: you're not allowed to use for loops. The calculation has to be vectorized.\n",
    "    # you may want to use repeat and repeat_interleave.\n",
    "    # TODO\n",
    "    bs = energies.shape[0] #batch size\n",
    "    L = energies.shape[1]\n",
    "    T = targets.shape[1]\n",
    "    e = torch.repeat_interleave(energies.view(-1, 27), targets.shape[-1], 0)\n",
    "    t = torch.repeat_interleave(targets.unsqueeze(1), energies.shape[1], 1).view(-1)\n",
    "    output = cross_entropy(e, t, reduction='none').view(bs, L, T)\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKZLjSCFMAbV"
   },
   "source": [
    "Another thing we will need is a transformation for our label $y$. We don't want to use it as is, we want to insert some special label after each character, so, for example 'cat' becomes 'c_a_t_'. This extra '_' models the separation between words, allowing our model to distinguish between strings 'aa' and 'a' in its output. This is then used in inference - we can just get the most likely character for each position from $l = f_\\theta(x)$ (for example 'aa_bb_ccc_'), and then remove duplicate characters ('a_b_c_'), and then remove _ (abc). \n",
    "Let's implement a function that would change the string in this manner, and then map all characters to values from 0 to 26, with 0 to 25 corresponding to a-z, and 26 corresponding to _:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wE4oZRDAMAbV"
   },
   "outputs": [],
   "source": [
    "def transform_word(s):\n",
    "    # input: a string\n",
    "    # output: a tensor of shape 2*len(s)\n",
    "    # TODO\n",
    "    output = torch.zeros(2*len(s))\n",
    "\n",
    "    for i in range(len(s)):\n",
    "        output[2*i] = ord(s[i]) - ord('a')\n",
    "        output[2*i + 1] = 26\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-txyVZkMAbV"
   },
   "source": [
    "Now, let's plot energy table built on our model's prediction for alphabet image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "xgJsmkzmMAbX",
    "outputId": "b2a7d28c-7f5d-4c94-dbf4-79dd23aa2995"
   },
   "outputs": [],
   "source": [
    "def plot_pm(pm, path=None):\n",
    "    fig=plt.figure(dpi=200)\n",
    "    ax = plt.axes()\n",
    "    im = ax.imshow(pm.cpu().T)\n",
    "    \n",
    "    ax.set_xlabel('window locations →')\n",
    "    ax.set_ylabel('← label characters')\n",
    "    ax.xaxis.set_label_position('top') \n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    if path is not None:\n",
    "        for i in range(len(path) - 1):\n",
    "            ax.plot(*path[i], *path[i+1], marker = 'o', markersize=0.5, linewidth=10, color='r', alpha=1)\n",
    "\n",
    "    cax = fig.add_axes([ax.get_position().x1+0.01,ax.get_position().y0,0.02,ax.get_position().height])\n",
    "    plt.colorbar(im, cax=cax) \n",
    "    \n",
    "energies = model(alphabet.cuda().view(1, 1, *alphabet.shape))\n",
    "targets = transform_word(string.ascii_lowercase).unsqueeze(0)\n",
    "\n",
    "pm = build_path_matrix(energies, targets)\n",
    "plot_pm(energies[0].detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_0IlGAzMAba"
   },
   "source": [
    "What do you see? What does the model classify correctly, and what does it have problems with?\n",
    "\n",
    "Answer:\n",
    "\n",
    "    Searching for a good pairing $z$ is same as searching for a trajectory with a small sum of it's values in this `pm` matrix. Where does the trajectory start, and where does it end? What other properties does the trajectory have? Can you see where an optimal trajecotry would be passing through in the plot above?\n",
    "\n",
    "Answer:\n",
    "\n",
    "    Now let's implement a function that would tell us the energy of a particular path (i.e. pairing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Wxaq_x9MAbb"
   },
   "outputs": [],
   "source": [
    "def path_energy(pm, path):\n",
    "    # inputs:\n",
    "    #   pm - a matrix of energies \n",
    "    #    L - energies length\n",
    "    #    T - targets length\n",
    "    #   path - list of length L that maps each energy vector to an element in T\n",
    "    # returns:\n",
    "    #   energy - sum of energies on the path, or 2**30 if the mapping is invalid\n",
    "    \n",
    "    if len(path) != pm.shape[0]:\n",
    "        return 2**30\n",
    "\n",
    "    energy = 0\n",
    "\n",
    "    for i in range(pm.shape[0]):\n",
    "        energy += pm[i, path[i]]\n",
    "  \n",
    "    return energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-xYfH4OMAbb"
   },
   "source": [
    "Now we can check some randomly generated paths and see the associated energies for our alphabet image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 706
    },
    "id": "rnBl2XQGMAbc",
    "outputId": "b115dbd2-98b4-4bf5-e566-a3625964288c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = torch.zeros(energies.shape[1] - 1)\n",
    "path[:targets.shape[1] - 1] = 1\n",
    "path = [0] + list(map(lambda x : x.int().item(), path[torch.randperm(path.shape[0])].cumsum(dim=-1)))\n",
    "points = list(zip(range(energies.shape[1]), path))\n",
    "\n",
    "plot_pm(pm[0].detach(), points)\n",
    "print('energy is', path_energy(pm[0], path).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJrzwsTVTbl1"
   },
   "source": [
    "Now, generate two paths with the worst possible energy, print their energies and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "93Km7zUQTlxe",
    "outputId": "08bd9abe-6786-461d-cbf5-81452f4e2500"
   },
   "outputs": [],
   "source": [
    "# good path\n",
    "path = torch.zeros(energies.shape[1] - 1)\n",
    "path[:targets.shape[1] - 1] = 0.5\n",
    "path = [0] + list(map(lambda x : x.int().item(), path[torch.randperm(path.shape[0])].cumsum(dim=-1)))\n",
    "points = list(zip(range(energies.shape[1]), path))\n",
    "\n",
    "# bad path\n",
    "path2 = torch.zeros(energies.shape[1] - 1)\n",
    "path2[:targets.shape[1] - 1] = 0.2\n",
    "path2 = [0] + list(map(lambda x : x.int().item(), path2[torch.randperm(path2.shape[0])].cumsum(dim=-1)))\n",
    "points2 = list(zip(range(energies.shape[1]), path2))\n",
    "\n",
    "print('energy 1 is', path_energy(pm[0], path).item())\n",
    "plot_pm(pm[0].detach(), points)\n",
    "print('energy 2 is', path_energy(pm[0], path2).item())\n",
    "plot_pm(pm[0].detach(), points2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnrQtZ-kMAbd",
    "tags": []
   },
   "source": [
    "### Optimal path finding\n",
    "Now, we're going to implement the finding of the optimal path. To do that, we're going to use Viterbi algorithm, which in this case is a simple dynamic programming problem.\n",
    "In this context, it's a simple dynamic programming algorithm that for each pair i, j, calculates the minimum cost of the path that goes from 0-th index in the energies and 0-th index in the target, to i-th index in the energies, and j-th index in the target. We can memorize the values in a 2-dimensional array, let's call it `dp`. Then we have the following transitions:\n",
    "```\n",
    "dp[0, 0] = pm[0, 0]\n",
    "dp[i, j] = min(dp[i - 1, j], dp[i - 1, j - 1]) + pm[i, j]\n",
    "```\n",
    "\n",
    "The optimal path can be recovered if we memorize which cell we came from for each `dp[i, j]`.\n",
    "\n",
    "Below, you'll need to implement this algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tyyipYDqMAbd"
   },
   "outputs": [],
   "source": [
    "def find_path(pm):\n",
    "    # inputs:\n",
    "    #   pm - a tensor of shape LxT with energies\n",
    "    #     L is length of energies array\n",
    "    #     T is target sequence length\n",
    "    # NOTE: this is slow because it's not vectorized to work with batches.\n",
    "    #  output:\n",
    "    #     a tuple of three elements:\n",
    "    #         1. sum of energies on the best path,\n",
    "    #         2. list of tuples - points of the best path in the pm matrix \n",
    "    #         3. the dp array\n",
    "\n",
    "    # TODO\n",
    "    dp = torch.zeros(*pm.shape).to(device)\n",
    "    dp[0, 1:] = 200\n",
    "    path = torch.zeros(*pm.shape)   # 0 - start, 1 - from up, 2 - from left up\n",
    "    points = []\n",
    "    energies = torch.tensor(0.0)\n",
    "    for i in range(1, dp.shape[0]):\n",
    "        for j in range(dp.shape[1]):\n",
    "            if j == 0:\n",
    "                dp[i, j] = dp[i-1, j] + pm[i, j]\n",
    "                path[i, j] = 1\n",
    "            else:\n",
    "                if dp[i-1, j] < dp[i-1, j-1]:\n",
    "                    dp[i, j] = dp[i-1, j] + pm[i, j]\n",
    "                    path[i, j] = 1\n",
    "                else:\n",
    "                    dp[i, j] = dp[i-1, j-1] + pm[i, j]\n",
    "                    path[i, j] = 2\n",
    "    p = [dp.shape[0]-1, dp.shape[1]-1]\n",
    "    points.append(tuple(p))\n",
    "    energies += pm[p[0], p[1]].detach().cpu()\n",
    "    while p != [0, 0]:\n",
    "        if path[p[0], p[1]] == 1:\n",
    "            p = [p[0]-1, p[1]]\n",
    "        elif path[p[0], p[1]] == 2:\n",
    "            p = [p[0]-1, p[1]-1]\n",
    "        points.append(tuple(p))\n",
    "        energies += pm[p[0], p[1]].detach().cpu()\n",
    "\n",
    "    return energies, points, dp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "svhNOp8XMAbd"
   },
   "source": [
    "Let's take a look at the best path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 706
    },
    "id": "_LcKBvPBMAbd",
    "outputId": "0b7d5294-2fc7-41b6-a55e-ef75852b8416"
   },
   "outputs": [],
   "source": [
    "free_energy, path, d = find_path(pm[0])\n",
    "plot_pm(pm[0].detach(), path)\n",
    "print('free energy is', free_energy.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aBlQsXB0oLpu"
   },
   "source": [
    "We can also visualize the dp array. You may need to tune clamping to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "id": "4g2ZAP5p92zt",
    "outputId": "85cce987-32d2-4766-faf0-139c962c1faf"
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=200)\n",
    "plt.imshow(d.cpu().detach().T.clamp(0, 750))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmIhBQ5vMAbe"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is time to train the network using our best path finder. We're going to use the energy loss function:\n",
    "$$\\ell(x, y) = \\sum_i H(y_{z(i)}, l_i)$$\n",
    "Where $z$ is the best path we've found. This is akin to pushing down on the free energy $F(x, y)$, while pushing up everywhere else by nature of cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "793bc04085824a148b66ddde3237fdc2",
      "a3e15923275948d486e9d26b9661258a",
      "2b79f2f8fbd842ca8a808a9019159bc9",
      "36a40f4be7ed436291f202a840d83060",
      "4bf6fd2e0731463896ee9197dc5f05c1",
      "39d5fa36939d4fc498fa2d801d96770d",
      "fcc77ed8efd142f0b0872e3daa5f8e96",
      "99bce4ec31404b048c1ee876ae01d6f0",
      "08c8f20b75864fb7ade70f05bf701178",
      "5d2502c8dc1741b8b7bf7e7ed124da89",
      "390adc5744694f0b82c5908af4d6661f"
     ]
    },
    "id": "B2HLpKuVMAbe",
    "outputId": "b6b9de2d-04bb-4acf-a5f9-1670a4f3890d"
   },
   "outputs": [],
   "source": [
    "import copy, time\n",
    "\n",
    "def collate_fn(samples):\n",
    "    \"\"\" A function to collate samples into batches for multi-character case\"\"\"\n",
    "    images, annotations = zip(*samples)\n",
    "    images = list(images)\n",
    "    annotations = list(annotations)\n",
    "    annotations = list(map(transform_word, annotations))\n",
    "    m_width = max(18, max([i.shape[1] for i in images]))\n",
    "    m_length = max(3, max([s.shape[0] for s in annotations]))\n",
    "    for i in range(len(images)):\n",
    "        images[i] = torch.nn.functional.pad(images[i], (0, m_width - images[i].shape[-1]))\n",
    "        annotations[i] = torch.nn.functional.pad(annotations[i], (0, m_length - annotations[i].shape[0]), value=BETWEEN)\n",
    "    if len(images) == 1:\n",
    "        return images[0].unsqueeze(0), torch.stack(annotations)\n",
    "    else:\n",
    "        return torch.stack(images), torch.stack(annotations)\n",
    "    \n",
    "def train_ebm_model(model, num_epochs, train_loader, criterion, optimizer):\n",
    "    ''' Train EBM Model using find_path()'''\n",
    "    pbar = tqdm(range(num_epochs))\n",
    "    size = 0\n",
    "    free_energies = []\n",
    "    paths = []\n",
    "    model.train()\n",
    "    for epoch in pbar:\n",
    "        start_time = time.time()\n",
    "        total_train_loss = 0.0\n",
    "        # TODO: implement the training loop\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.unsqueeze(1).to(device), target.to(device)\n",
    "            energies = model(data)\n",
    "            pm = build_path_matrix(energies, target)\n",
    "            cm = build_ce_matrix(energies, target.long())\n",
    "            p = torch.zeros(data.shape[0], energies.shape[1], 2)\n",
    "            ce = torch.zeros(data.shape[0], energies.shape[1])\n",
    "            for b in range(data.shape[0]):\n",
    "                free_energy, path, d = find_path(pm[b])\n",
    "                p[b, :, :] = torch.tensor(path)\n",
    "                for m in range(len(path)):\n",
    "                    ce[b, m] = cm[b, path[m][0], path[m][1]]\n",
    "            \n",
    "            loss = torch.sum(ce)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        pbar.set_postfix({'train_loss': total_train_loss / len(sds), 'Epoch Time': epoch_time})\n",
    "\n",
    "sds = SimpleWordsDataset(2, 2500) # for simplicity, we're training only on words of length two\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "dataloader = torch.utils.data.DataLoader(sds, batch_size=BATCH_SIZE, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "# TODO: train the model\n",
    "# note: remember that our best path finding algorithm is not batched, so you'll\n",
    "# need a for loop to do loss calculation. \n",
    "# This is not ideal, as for loops are very slow, but for \n",
    "# demonstration purposes it will suffice. In practice, this will be\n",
    "# unusable for any real problem unless it handles batching.\n",
    "\n",
    "# also: remember that the loss is the sum of cross_entropies along the path, not \n",
    "# energies!\n",
    "\n",
    "# Make a copy of the model and re-initialize optimizer\n",
    "model = SimpleNet()\n",
    "model.to(device)\n",
    "model.load_state_dict(model.state_dict())\n",
    "optimizer2 = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# training\n",
    "train_ebm_model(model, 5, dataloader, build_ce_matrix, optimizer2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8N_ACMfMAbg"
   },
   "source": [
    "Let's check what the energy matrix looks like for the alphabet image now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 706
    },
    "id": "Rf7yuVcWMAbg",
    "outputId": "29fe7537-2f46-4702-e7d0-94f2848765b7"
   },
   "outputs": [],
   "source": [
    "energies = model(alphabet.unsqueeze(0).unsqueeze(0).cuda())\n",
    "targets = transform_word(string.ascii_lowercase)\n",
    "pm = build_path_matrix(energies, targets.unsqueeze(0))\n",
    "\n",
    "free_energy, path, _ = find_path(pm[0])\n",
    "plot_pm(pm[0].detach(), path)\n",
    "print('free energy is', free_energy.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUoXSmvQMAbg"
   },
   "source": [
    "Explain how the free energy changed, and why.\n",
    "\n",
    "Answer:\n",
    "\n",
    "    Now the special characters are dim and the free energy is dropped extensively because we trained the model to push down the free energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7QcTNZp6MAbg"
   },
   "source": [
    "We can also look at raw energies output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "id": "qgUyDNBSMAbg",
    "outputId": "994592be-d026-43bf-a8e7-b7a1651493fa"
   },
   "outputs": [],
   "source": [
    "alphabet_energy_post_train_viterbi = model(alphabet.cuda().view(1, 1, *alphabet.shape))\n",
    "\n",
    "plt.figure(dpi=200, figsize=(40, 10))\n",
    "plt.imshow(alphabet_energy_post_train_viterbi.cpu().data[0].T)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWDtL_AoMAbh"
   },
   "source": [
    "How does this compare to the energies we had after training only on one-character dataset?\n",
    "\n",
    "Answer:\n",
    "\n",
    "    the special character energy is low between alphabet characters, and it's not lit-up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6x7kaFxTMAbh"
   },
   "source": [
    "## Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9w8ZojnnMAbh"
   },
   "source": [
    "Now we can use the model for decoding a word from an image. Let's pick some word, apply the model to it, and see energies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 855
    },
    "id": "HaTiwIthMAbh",
    "outputId": "9bbe56e1-6c7d-4d93-800c-b3c09b15586f"
   },
   "outputs": [],
   "source": [
    "img = sds.draw_text('hello')\n",
    "energies = model(img.cuda().unsqueeze(0).unsqueeze(0))\n",
    "plt.imshow(img)\n",
    "plot_energies(energies[0].detach().cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxTbUNkYMAbh"
   },
   "source": [
    "You should see some characters light up. Now, let's implement a simple decoding algorithm. To decode, first we want to get most likely classes for all energies, and then do two things:\n",
    "1. segment strings using the divisors (our special character with index 26), and for each segment replace it with the most common character in that segment. Example: aaab_bab_ -> a_b. If some characters are equally common, you can pick random.\n",
    "2. remove all special divisor characters: a_b -> ab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iiamf7wzMAbi",
    "outputId": "a5276914-13d4-4852-b655-7e2d1617f323"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def indices_to_str(indices):\n",
    "    # inputs: indices - a tensor of most likely class indices\n",
    "    # outputs: decoded string\n",
    "    \n",
    "    # TODO\n",
    "    s = ''\n",
    "    for idx in indices:\n",
    "        if idx < 26:\n",
    "            s += chr(ord('a')+idx)\n",
    "        else:\n",
    "            s += '_'\n",
    "    spl = s.split('_')\n",
    "    if spl[-1] != '':\n",
    "        spl = spl[:-1]\n",
    "    ret = ''\n",
    "    for p in spl:\n",
    "        if p == '':\n",
    "            continue\n",
    "        else:\n",
    "            q = Counter(p)\n",
    "            q = max(q, key=q.get)\n",
    "            ret += q\n",
    "    return ret\n",
    "    \n",
    "min_indices = energies[0].argmin(dim=-1)\n",
    "print(indices_to_str(min_indices))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "7b6db0c1d442fe597d9b481cd2ea939a45b3fa778adc3bd0e8ea6ed6edc9a97e"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "08c8f20b75864fb7ade70f05bf701178": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2b79f2f8fbd842ca8a808a9019159bc9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_99bce4ec31404b048c1ee876ae01d6f0",
      "max": 5,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_08c8f20b75864fb7ade70f05bf701178",
      "value": 5
     }
    },
    "36a40f4be7ed436291f202a840d83060": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5d2502c8dc1741b8b7bf7e7ed124da89",
      "placeholder": "​",
      "style": "IPY_MODEL_390adc5744694f0b82c5908af4d6661f",
      "value": " 5/5 [00:50&lt;00:00, 10.13s/it, train_loss=0.0013, Epoch Time=10.1]"
     }
    },
    "390adc5744694f0b82c5908af4d6661f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "39d5fa36939d4fc498fa2d801d96770d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4bf6fd2e0731463896ee9197dc5f05c1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d2502c8dc1741b8b7bf7e7ed124da89": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "793bc04085824a148b66ddde3237fdc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a3e15923275948d486e9d26b9661258a",
       "IPY_MODEL_2b79f2f8fbd842ca8a808a9019159bc9",
       "IPY_MODEL_36a40f4be7ed436291f202a840d83060"
      ],
      "layout": "IPY_MODEL_4bf6fd2e0731463896ee9197dc5f05c1"
     }
    },
    "99bce4ec31404b048c1ee876ae01d6f0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a3e15923275948d486e9d26b9661258a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_39d5fa36939d4fc498fa2d801d96770d",
      "placeholder": "​",
      "style": "IPY_MODEL_fcc77ed8efd142f0b0872e3daa5f8e96",
      "value": "100%"
     }
    },
    "fcc77ed8efd142f0b0872e3daa5f8e96": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
