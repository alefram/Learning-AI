{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Learning neural network stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Neural Networks\n",
    "\n",
    "## what is a input data?\n",
    "\n",
    "It is a number that you recorded in the real world somewhere.\n",
    "\n",
    "It is important that, the datapoints that you pass at the same time to the neural network depends on when you think that the neural network can be accurate with this data. \n",
    "\n",
    "Also we can put negative numbers in the data.\n",
    "\n",
    "## what is a prediction?\n",
    "\n",
    "A prediction is what the neural network tells you. It not always is right but it can learn from it. It can predict with multiple inputs and multiple outputs.\n",
    "\n",
    "## what is a weight?\n",
    "\n",
    "It is like the sensitive of the net. When you train the neural net it means to adjust the weights for have an accurate prediction how we can do it?\n",
    "\n",
    "The objective of build a neural network is to compute a error function with a bunch of weight. Then you will have a relationship of the error and the weight of the network and with that information, you can change the weight to reduce the error to 0.\n",
    "\n",
    "You will  use the alpha variable to reduce weight update so it doesn’t overshoot.\n",
    "\n",
    "## Learning methods\n",
    "\n",
    "Hot and cold: it consist on wiggling the weight to see when it reduce the error\n",
    "\n",
    "gradient descent: it consist on to find the minimums error for learning.\n",
    "\n",
    "Exists 3 gradient descent. When we talk about stochastic gradient descent, it updates the weights one example at the time. The full gradient descent, it updates one dataset at the time. Batch gradient descent, in this case you choose a batch size for updates the weight. \n",
    "\n",
    "## Overfitting\n",
    "\n",
    "\"Error is shared among all the weights. If a particular configuration of weights accidentally creates perfect correlation between the prediction and the output dataset without giving the heaviest weight to the best inputs, the neural network\n",
    "will stop learnin\". andrew trask\n",
    "\n",
    "The greatest challenge is convincing my neural network to generalize instead of just memorize\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "It consist  on moving delta around to take a correlation of input and output. More technicaly it is about calculating deltas for intermediate layers for perform gradient descent \n",
    "\n",
    "\n",
    "## tips regularizations(ignore noise)\n",
    "\n",
    "<ul>\n",
    "    <li>neural networks find and create correlation</li>\n",
    "    <li>stop training the NN when it getting worse to dont overfit</li>\n",
    "    <li>\n",
    "        use validation dataset for see when to stop train the neural net\n",
    "    </li>\n",
    "    <li>\n",
    "        dropout: radomly turn off the neurons(set them to 0) during training (multiply the layer values by a random matrix of 1s and 0s)\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "## Correlation summarization\n",
    "\n",
    "\"Neural networks seek to find direct and indirect correlation between an input layer and an output layer, which are determined by the input \n",
    "and output datasets, respectively\". andrew Trask\n",
    "\n",
    "## Local Correlation summarization\n",
    "\n",
    "\"Any given set of weights optimizes to learn how to correlate its input layer with what the output layer says it should be.\" andrew Trask\n",
    "\n",
    "## Global correlation summarization\n",
    "\n",
    "\"What an earlier layer says it should be can be determined by taking what a later layer says it should be and multiplying it by the weights in between them. This way, later layers can tell earlier layers what kind of signal they need, to ultimately find correlation with the output. This cross-communication is called backpropagation.\" andrew Trask\n",
    "\n",
    "## notation\n",
    "\n",
    "<ul>\n",
    "    <li>\n",
    "        I0W0: Take the layer 0 vector and perform vector-\n",
    "        matrix multiplication with the weight matrix 0.”\n",
    "    </li>\n",
    "    <li>\n",
    "        l1 = relu(l0W0): To create the layer 1 vector, take the layer 0 vector and perform vector-matrix multiplication with the weight matrix 0; then perform the relu function on the output (setting all negative numbers to 0).\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "# Activation Functions\n",
    "\n",
    "It is a function applied to the neurons in a layer during the prediction.\n",
    "\n",
    "## Constraints\n",
    "\n",
    "<ul>\n",
    "    <li>The function must be continuous and infinite in domain.</li>\n",
    "    <li>Good activation functions are monotonic, never changing direction</li>\n",
    "    <li>Good activation functions are nonlinear (they squiggle or turn)</li>\n",
    "    <li>\n",
    "        Good activation functions (and their derivatives) should be efficiently computable.\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "## Typical activation functions\n",
    "\n",
    "## Sigmoid\n",
    "\n",
    "![](/assets/images/sigmoid.png \"sigmoid\")\n",
    "\n",
    "## tanh\n",
    "\n",
    "It is better for hidden layers\n",
    "\n",
    "![](/assets/images/tanh.png \"tanh\")\n",
    "\n",
    "## softmax\n",
    "\n",
    "for predict a single lable\n",
    "\n",
    "![](/assets/images/softmax.png \"softmax\")\n",
    "\n",
    "## typical configurations output layer\n",
    "\n",
    "### configuration 1 Predicting raw data into values(no act function)\n",
    "\n",
    "One case could be people want to train a neural network to transform one matrix into another where the range of output is something is a probability. It means we want the right answer no 0 or 1.\n",
    "\n",
    "### configuration 2  predicting unrelated (yes/no) probabilities(sigmoid)\n",
    "\n",
    "If we want to make multiple binary probabilities  in one neural network, it’s best to use the sigmoid activation function, because it models individual probabilities separately for each output node.\n",
    "\n",
    "### configuration 3 predicting which-one probabilities(softmax) [chapter 9]\n",
    "\n",
    "The most common use of neural networks is predicting a single label out of many. softmax asks, “Which digit seems like the best fit for this input?\" softmax raises each input value exponentially and then\n",
    "divides by the layer’s sum.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools that i will use\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The streetlight problem\n",
    "\n",
    "we want to create a neural network to understand the pattern of a wear streetlight with input of the streetlight pattern and the output of when to walk  or not to walk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stochastic gradient descent\n",
    "here  we  try to update the weights each time we get a new sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:-0.19999999999999996\n",
      "Prediction:-0.21799999999999997\n",
      "Prediction:-0.68582\n",
      "Prediction:0.3152182000000001\n",
      "Prediction:-0.17308616399999993\n",
      "Prediction:-0.1515353023599999\n",
      "Error:3.861893282040811\n",
      "\n",
      "Prediction:-0.1485045963128\n",
      "Prediction:-0.14662404173327193\n",
      "Prediction:-0.6459164809559392\n",
      "Prediction:0.394615820897681\n",
      "Prediction:-0.10512471250700073\n",
      "Prediction:-0.10445016845253541\n",
      "Error:3.352708880768109\n",
      "\n",
      "Prediction:-0.1023611650834848\n",
      "Prediction:-0.08095410492150057\n",
      "Prediction:-0.6094745728457114\n",
      "Prediction:0.46672989501849804\n",
      "Prediction:-0.04257487499498336\n",
      "Prediction:-0.06231850415456308\n",
      "Error:2.9256218108072027\n",
      "\n",
      "Prediction:-0.06107213407147172\n",
      "Prediction:-0.020489471112823288\n",
      "Prediction:-0.5761825762241007\n",
      "Prediction:0.5322189236168611\n",
      "Prediction:0.015037765599337005\n",
      "Prediction:-0.024678727045003623\n",
      "Error:2.5666937055426917\n",
      "\n",
      "Prediction:-0.02418515250410358\n",
      "Prediction:0.035225649082841426\n",
      "Prediction:-0.545756935049359\n",
      "Prediction:0.5916819345561876\n",
      "Prediction:0.06814506676055454\n",
      "Prediction:0.008888774046914572\n",
      "Error:2.2643813714182675\n",
      "\n",
      "Prediction:0.008710998565976347\n",
      "Prediction:0.0866061676992147\n",
      "Prediction:-0.5179396951151537\n",
      "Prediction:0.6456638533292004\n",
      "Prediction:0.117140164229798\n",
      "Prediction:0.03876543515993425\n",
      "Error:2.009124055573947\n",
      "\n",
      "Prediction:0.037990126456735496\n",
      "Prediction:0.1340298053290353\n",
      "Prediction:-0.49249619200904926\n",
      "Prediction:0.6946603890259049\n",
      "Prediction:0.16238096336202695\n",
      "Prediction:0.06529797038026258\n",
      "Error:1.7930018804140009\n",
      "\n",
      "Prediction:0.06399201097265733\n",
      "Prediction:0.17784044428125717\n",
      "Prediction:-0.4692129478691799\n",
      "Prediction:0.7391224790538953\n",
      "Prediction:0.20419331529324586\n",
      "Prediction:0.08880151305507306\n",
      "Error:1.6094531720558618\n",
      "\n",
      "Prediction:0.0870254827939716\n",
      "Prediction:0.2183511790288905\n",
      "Prediction:-0.44789575808273885\n",
      "Prediction:0.7794603324596823\n",
      "Prediction:0.24287390637994655\n",
      "Prediction:0.10956247321563761\n",
      "Error:1.453040526225844\n",
      "\n",
      "Prediction:0.10737122375132485\n",
      "Prediction:0.255847091282678\n",
      "Prediction:-0.4283679507728041\n",
      "Prediction:0.8160471081010281\n",
      "Prediction:0.27869288680273197\n",
      "Prediction:0.12784113684115173\n",
      "Error:1.3192572067474089\n",
      "\n",
      "Prediction:0.12528431410432866\n",
      "Prediction:0.2905877745572225\n",
      "Prediction:-0.41046880346914066\n",
      "Prediction:0.84922226064658\n",
      "Prediction:0.31189626188783787\n",
      "Prediction:0.14387403027955137\n",
      "Error:1.2043669212669943\n",
      "\n",
      "Prediction:0.1409965496739603\n",
      "Prediction:0.32280963085054604\n",
      "Prediction:-0.39405210276783403\n",
      "Prediction:0.8792945844010231\n",
      "Prediction:0.34270806757319294\n",
      "Prediction:0.15787607103590162\n",
      "Error:1.1052712183785078\n",
      "\n",
      "Prediction:0.1547185496151836\n",
      "Prediction:0.3527279600152182\n",
      "Prediction:-0.3789848340665609\n",
      "Prediction:0.9065449822448681\n",
      "Prediction:0.37133234951068206\n",
      "Prediction:0.1700425242233891\n",
      "Error:1.0193997429531787\n",
      "\n",
      "Prediction:0.1666416737389213\n",
      "Prediction:0.38053886054084535\n",
      "Prediction:-0.3651459896284824\n",
      "Prediction:0.9312289845135301\n",
      "Prediction:0.39795496353604265\n",
      "Prediction:0.1805507822293882\n",
      "Error:0.9446194079338792\n",
      "\n",
      "Prediction:0.17693976658480048\n",
      "Prediction:0.40642095877717993\n",
      "Prediction:-0.352425484288607\n",
      "Prediction:0.9535790403984621\n",
      "Prediction:0.42274521363655315\n",
      "Prediction:0.18956198356388387\n",
      "Error:0.8791592206366557\n",
      "\n",
      "Prediction:0.18577074389260623\n",
      "Prediction:0.43053698208925717\n",
      "Prediction:-0.34072316908152855\n",
      "Prediction:0.9738066024136773\n",
      "Prediction:0.4458573420900138\n",
      "Prediction:0.19722248541550313\n",
      "Error:0.8215480641495108\n",
      "\n",
      "Prediction:0.19327803570719304\n",
      "Prediction:0.4530351900369865\n",
      "Prediction:-0.32994793394734706\n",
      "Prediction:0.9921040226157465\n",
      "Prediction:0.4674318851234053\n",
      "Prediction:0.2036652031286038\n",
      "Error:0.7705631999483074\n",
      "\n",
      "Prediction:0.19959189906603175\n",
      "Prediction:0.47405067639899084\n",
      "Prediction:-0.3200168914712014\n",
      "Prediction:1.0086462775776457\n",
      "Prediction:0.48759690623417007\n",
      "Prediction:0.20901082862153858\n",
      "Error:0.7251876430938405\n",
      "\n",
      "Prediction:0.20483061204910785\n",
      "Prediction:0.49370655370278027\n",
      "Prediction:-0.31085463433834176\n",
      "Prediction:1.0235925375815478\n",
      "Prediction:0.5064691182204771\n",
      "Prediction:0.21336893868064555\n",
      "Error:0.6845748801788673\n",
      "\n",
      "Prediction:0.2091015599070326\n",
      "Prediction:0.5121150308701908\n",
      "Prediction:-0.30239255984755725\n",
      "Prediction:1.03708759409901\n",
      "Prediction:0.5241549039692823\n",
      "Prediction:0.21683900307699266\n",
      "Error:0.6480196640174676\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# each node work individual they only share the error measure\n",
    "weights = np.array([0.5, 0.48, -0.7])\n",
    "alpha = 0.01\n",
    "streetlights = np.array([   [1, 0, 1], \n",
    "                            [0, 1, 1], \n",
    "                            [0, 0, 1],\n",
    "                            [1, 1, 1],\n",
    "                            [0, 1, 1],\n",
    "                            [1, 0, 1] ])\n",
    "walk_vs_stop = np.array([0,1,0,1,1,0])\n",
    "input = streetlights[0]\n",
    "goal_prediction = walk_vs_stop[0]\n",
    "\n",
    "for interaction in range(20):\n",
    "    error_of_lights = 0\n",
    "    for row in range(len(walk_vs_stop)):\n",
    "        input = streetlights[row]\n",
    "        goal_prediction = walk_vs_stop[row]\n",
    "        prediction = np.dot(input, weights)\n",
    "        error = (goal_prediction - prediction) ** 2\n",
    "        error_of_lights += error\n",
    "\n",
    "        delta = prediction - goal_prediction\n",
    "        weights = weights - alpha * delta * input\n",
    "\n",
    "        print(\"Prediction:\" + str(prediction))\n",
    "\n",
    "    print(\"Error:\" + str(error_of_lights) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## overfitting\n",
    "\n",
    "If a particular configuration of the weights accidentally creates a perfect correlation of the prediction and the output dataset without any noise, then the model is said to be overfitting. It means that the model is not general enough to generalize to new data. It will not learn just memorize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "backpropagation is about calculating delta values for each weight in intermediate layers for perform gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:0.6342311598444467\n",
      "Error:0.35838407676317513\n",
      "Error:0.0830183113303298\n",
      "Error:0.006467054957103705\n",
      "Error:0.0003292669000750734\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "def relu(x):\n",
    "    return (x > 0) * x\n",
    "\n",
    "def relu2deriv(output): # this activates the ouputs necessary for the correlation\n",
    "    return output > 0\n",
    "\n",
    "streetlights = np.array([   [1, 0, 1], \n",
    "                            [0, 1, 1], \n",
    "                            [0, 0, 1],\n",
    "                            [1, 1, 1]])\n",
    "\n",
    "walk_vs_stop = np.array([[ 1, 1, 0, 0]]).T # goal prediction\n",
    "alpha = 0.2\n",
    "hidden_size = 4 # it means there is 4 nodes\n",
    "weights_0_1 = 2 * np.random.random((3,hidden_size)) - 1\n",
    "weights_1_2 = 2 * np.random.random((hidden_size,1)) - 1\n",
    "\n",
    "# backpropagation\n",
    "for iteration in range(50):\n",
    "    error_layer_2 = 0\n",
    "\n",
    "    for i in range(len(streetlights)):\n",
    "        layer_0 = streetlights[i:i+1]\n",
    "        layer_1 = relu(np.dot(layer_0,weights_0_1))\n",
    "        layer_2 = np.dot(layer_1,weights_1_2)\n",
    "\n",
    "        error_layer_2 += np.sum((layer_2 - walk_vs_stop[i:i+1] ) ** 2)\n",
    "        layer_2_delta = (layer_2 - walk_vs_stop[i:i+1])\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1)  # here happens de backpropagation\n",
    "\n",
    "        weights_1_2 -= alpha * layer_1.T.dot(layer_2_delta)\n",
    "        weights_0_1 -= alpha * layer_0.T.dot(layer_1_delta)\n",
    "\n",
    "    if(iteration % 10 == 9):\n",
    "        print(\"Error:\" + str(error_layer_2))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## practice matrix and vectors  operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# vectors multiplication\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m vector1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m,\u001b[39m4\u001b[39m])\n\u001b[1;32m      4\u001b[0m vector2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m,\u001b[39m4\u001b[39m,\u001b[39m5\u001b[39m])\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvector_multiplication\u001b[39m(a,b):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# vectors multiplication\n",
    "\n",
    "vector1 = np.array([0,1,2,3,4])\n",
    "vector2 = np.array([1,2,3,4,5])\n",
    "\n",
    "def vector_multiplication(a,b):\n",
    "    return np.dot(a,b)\n",
    "\n",
    "c = vector_multiplication(vector1,vector2.T)\n",
    "print(c)\n",
    "\n",
    "#matrix multiplication\n",
    "\n",
    "#define a matrix 3*3 random\n",
    "matrix = np.array([[1,2,3,4],[1,2,3,4],[1,2,3,4]])\n",
    "\n",
    "#define a matrix normal\n",
    "matrix2 = np.array([[1,2,3,4],[1,2,3,4],[1,2,3,4]])\n",
    "\n",
    "c = np.dot(matrix,matrix2.T)\n",
    "\n",
    "print(\"matrix:\" + str(matrix))\n",
    "print(\" \")\n",
    "print(\"matrix2:\" + str(matrix2.T))\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My First neural networks (forward propagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8500000000000001"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# neural network with a single datapoint\n",
    "def neural_network(input, weight):\n",
    "    prediction = input * weight\n",
    "    return prediction\n",
    "\n",
    "weight = 0.1\n",
    "data = [8.5, 9, 10, 4]\n",
    "input = data[0]\n",
    "neural_network(input,weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9800000000000001\n"
     ]
    }
   ],
   "source": [
    "# neural network with multiple inputs\n",
    "\n",
    "def weight_sum(a,b):\n",
    "    assert (len(a) == len(b))\n",
    "    output = 0\n",
    "    for i in range(len(a)):\n",
    "        output += a[i] * b[i]\n",
    "    \n",
    "    return output\n",
    "\n",
    "def neural_network(input, weight):\n",
    "    prediction = weight_sum(input, weight)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "data1 = [8.5, 9.5, 9.9, 9.0]\n",
    "data2 = [0.65, 0.8, 0.8, 0.9]\n",
    "data3 = [1.2, 1.3, 0.5, 1.0]\n",
    "\n",
    "input = [data1[0], data2[0], data3[0]];\n",
    "weights = [0.1, 0.2, 0]\n",
    "\n",
    "prediction = neural_network(input, weights)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9800000000000001\n"
     ]
    }
   ],
   "source": [
    "# neural network with multiple inputs using numpy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def neural_network(input, weights):\n",
    "    prediction = input.dot(weights)\n",
    "    return prediction\n",
    "\n",
    "data1 = np.array([8.5, 9.5, 9.9, 9.0])\n",
    "data2 = np.array([0.65, 0.8, 0.8, 0.9])\n",
    "data3 = np.array([1.2, 1.3, 0.5, 1.0])\n",
    "weights = np.array([0.1, 0.2, 0])\n",
    "input = np.array([data1[0], data2[0], data3[0]])\n",
    "\n",
    "prediction = neural_network(input, weights)\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9800000000000001\n"
     ]
    }
   ],
   "source": [
    "# neural networks with multiple outputs and one input\n",
    "\n",
    "def mul(number, vector):\n",
    "    output = [0,0,0]\n",
    "    assert(len(output) == len(vector))\n",
    "\n",
    "    for i in range(len(vector)):\n",
    "        output[i] = number * vector[i]\n",
    "    \n",
    "    return output\n",
    "\n",
    "def neural_network(input, weights):\n",
    "    predictions = mul(input,weights)\n",
    "    return prediction\n",
    "\n",
    "data1 = [0.65, 0.8, 0.8, 0.9]\n",
    "weight = [0.3, 0.2, 0.9]\n",
    "input = data1[0]\n",
    "\n",
    "prediction = neural_network(input, weight)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.496  1.256 -0.286]\n"
     ]
    }
   ],
   "source": [
    "# a neural network with multiple inputs and ouputs\n",
    "# it neural net has a hidden layer \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def neural_network(input, weights):\n",
    "    hidden_prediction = input.dot(weights[0])\n",
    "    prediction = hidden_prediction.dot(weights[1])\n",
    "\n",
    "    return prediction\n",
    "\n",
    "input_to_hidden_weight = np.array([\n",
    "                                [0.1, 0.2, -0.1],\n",
    "                                [-0.1,0.1, 0.9],\n",
    "                                [0.1, 0.4, 0.1]])\n",
    "\n",
    "hidden_to_output_weight = np.array([\n",
    "                                [0.3, 1.1, -0.3],\n",
    "                                [0.1, 0.2, 0.0],\n",
    "                                [0.0, 1.3, 0.1]])\n",
    "\n",
    "weights = [input_to_hidden_weight, hidden_to_output_weight]\n",
    "\n",
    "data1 = np.array([8.5, 9.5, 9.9, 9.0])\n",
    "data2 = np.array([0.65,0.8, 0.8, 0.9])\n",
    "data3 = np.array([1.2, 1.3, 0.5, 1.0])\n",
    "input = np.array([data1[0],data2[0],data3[0]])\n",
    "\n",
    "prediction = neural_network(input, weights)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('learningAI': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "7b6db0c1d442fe597d9b481cd2ea939a45b3fa778adc3bd0e8ea6ed6edc9a97e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
