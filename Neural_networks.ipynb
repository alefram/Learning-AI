{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Learning neural network stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Neural Networks\n",
    "\n",
    "## what is a input data?\n",
    "\n",
    "It is a number that you recorded in the real world somewhere.\n",
    "\n",
    "It is important that, the datapoints that you pass at the same time to the neural network depends on when you think that the neural network can be accurate with this data. \n",
    "\n",
    "Also we can put negative numbers in the data.\n",
    "\n",
    "## what is a prediction?\n",
    "\n",
    "A prediction is what the neural network tells you. It not always is right but it can learn from it. It can predict with multiple inputs and multiple outputs.\n",
    "\n",
    "## what is a weight?\n",
    "\n",
    "It is like the sensitive of the net. When you train the neural net it means to adjust the weights for have an accurate prediction how we can do it?\n",
    "\n",
    "The objective of build a neural network is to compute a error function with a bunch of weight. Then you will have a relationship of the error and the weight of the network and with that information, you can change the weight to reduce the error to 0.\n",
    "\n",
    "You will  use the alpha variable to reduce weight update so it doesn’t overshoot.\n",
    "\n",
    "## Learning methods\n",
    "\n",
    "Hot and cold: it consist on wiggling the weight to see when it reduce the error\n",
    "\n",
    "gradient descent: it consist on to find the minimums error for learning.\n",
    "\n",
    "Exists 3 gradient descent. When we talk about stochastic gradient descent, it updates the weights one example at the time. The full gradient descent, it updates one dataset at the time. Batch gradient descent, in this case you choose a batch size for updates the weight. \n",
    "\n",
    "## Overfitting\n",
    "\n",
    "\"Error is shared among all the weights. If a particular configuration of weights accidentally creates perfect correlation between the prediction and the output dataset without giving the heaviest weight to the best inputs, the neural network\n",
    "will stop learnin\". andrew trask\n",
    "\n",
    "The greatest challenge is convincing my neural network to generalize instead of just memorize\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "It consist  on moving delta around to take a correlation of input and output. More technicaly it is about calculating deltas for intermediate layers for perform gradient descent \n",
    "\n",
    "\n",
    "## tips regularizations(ignore noise)\n",
    "\n",
    "<ul>\n",
    "    <li>neural networks find and create correlation</li>\n",
    "    <li>stop training the NN when it getting worse to dont overfit</li>\n",
    "    <li>\n",
    "        use validation dataset for see when to stop train the neural net\n",
    "    </li>\n",
    "    <li>\n",
    "        dropout: radomly turn off the neurons(set them to 0) during training (multiply the layer values by a random matrix of 1s and 0s)\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "## Correlation summarization\n",
    "\n",
    "\"Neural networks seek to find direct and indirect correlation between an input layer and an output layer, which are determined by the input \n",
    "and output datasets, respectively\". andrew Trask\n",
    "\n",
    "## Local Correlation summarization\n",
    "\n",
    "\"Any given set of weights optimizes to learn how to correlate its input layer with what the output layer says it should be.\" andrew Trask\n",
    "\n",
    "## Global correlation summarization\n",
    "\n",
    "\"What an earlier layer says it should be can be determined by taking what a later layer says it should be and multiplying it by the weights in between them. This way, later layers can tell earlier layers what kind of signal they need, to ultimately find correlation with the output. This cross-communication is called backpropagation.\" andrew Trask\n",
    "\n",
    "## notation\n",
    "\n",
    "<ul>\n",
    "    <li>\n",
    "        I0W0: Take the layer 0 vector and perform vector-\n",
    "        matrix multiplication with the weight matrix 0.”\n",
    "    </li>\n",
    "    <li>\n",
    "        l1 = relu(l0W0): To create the layer 1 vector, take the layer 0 vector and perform vector-matrix multiplication with the weight matrix 0; then perform the relu function on the output (setting all negative numbers to 0).\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "# Activation Functions\n",
    "\n",
    "It is a function applied to the neurons in a layer during the prediction.\n",
    "\n",
    "## Constraints\n",
    "\n",
    "<ul>\n",
    "    <li>The function must be continuous and infinite in domain.</li>\n",
    "    <li>Good activation functions are monotonic, never changing direction</li>\n",
    "    <li>Good activation functions are nonlinear (they squiggle or turn)</li>\n",
    "    <li>\n",
    "        Good activation functions (and their derivatives) should be efficiently computable.\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "## Typical activation functions\n",
    "\n",
    "## Sigmoid\n",
    "\n",
    "![](/assets/images/sigmoid.png \"sigmoid\")\n",
    "\n",
    "## tanh\n",
    "\n",
    "It is better for hidden layers\n",
    "\n",
    "![](/assets/images/tanh.png \"tanh\")\n",
    "\n",
    "## softmax\n",
    "\n",
    "for predict a single lable\n",
    "\n",
    "![](/assets/images/softmax.png \"softmax\")\n",
    "\n",
    "## typical configurations output layer\n",
    "\n",
    "### configuration 1 Predicting raw data into values(no act function)\n",
    "\n",
    "One case could be people want to train a neural network to transform one matrix into another where the range of output is something is a probability. It means we want the right answer no 0 or 1.\n",
    "\n",
    "### configuration 2  predicting unrelated (yes/no) probabilities(sigmoid)\n",
    "\n",
    "If we want to make multiple binary probabilities  in one neural network, it’s best to use the sigmoid activation function, because it models individual probabilities separately for each output node.\n",
    "\n",
    "### configuration 3 predicting which-one probabilities(softmax) [chapter 9]\n",
    "\n",
    "The most common use of neural networks is predicting a single label out of many. softmax asks, “Which digit seems like the best fit for this input?\" softmax raises each input value exponentially and then\n",
    "divides by the layer’s sum.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## overfitting\n",
    "\n",
    "If a particular configuration of the weights accidentally creates a perfect correlation of the prediction and the output dataset without any noise, then the model is said to be overfitting. It means that the model is not general enough to generalize to new data. It will not learn just memorize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "backpropagation is about calculating delta values for each weight in intermediate layers for perform gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools that i will use\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "def relu(x):\n",
    "    return (x > 0) * x\n",
    "\n",
    "def relu2deriv(output): # this activates the ouputs necessary for the correlation\n",
    "    return output > 0\n",
    "\n",
    "streetlights = np.array([   [1, 0, 1], \n",
    "                            [0, 1, 1], \n",
    "                            [0, 0, 1],\n",
    "                            [1, 1, 1]])\n",
    "\n",
    "walk_vs_stop = np.array([[ 1, 1, 0, 0]]).T # goal prediction\n",
    "alpha = 0.2\n",
    "hidden_size = 4 # it means there is 4 nodes\n",
    "weights_0_1 = 2 * np.random.random((3,hidden_size)) - 1\n",
    "weights_1_2 = 2 * np.random.random((hidden_size,1)) - 1\n",
    "\n",
    "# backpropagation\n",
    "for iteration in range(50):\n",
    "    error_layer_2 = 0\n",
    "\n",
    "    for i in range(len(streetlights)):\n",
    "        layer_0 = streetlights[i:i+1]\n",
    "        layer_1 = relu(np.dot(layer_0,weights_0_1))\n",
    "        layer_2 = np.dot(layer_1,weights_1_2)\n",
    "\n",
    "        error_layer_2 += np.sum((layer_2 - walk_vs_stop[i:i+1] ) ** 2)\n",
    "        layer_2_delta = (layer_2 - walk_vs_stop[i:i+1])\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1)  # here happens de backpropagation\n",
    "\n",
    "        weights_1_2 -= alpha * layer_1.T.dot(layer_2_delta)\n",
    "        weights_0_1 -= alpha * layer_0.T.dot(layer_1_delta)\n",
    "\n",
    "    if(iteration % 10 == 9):\n",
    "        print(\"Error:\" + str(error_layer_2))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network with a single datapoint\n",
    "def neural_network(input, weight):\n",
    "    prediction = input * weight\n",
    "    return prediction\n",
    "\n",
    "weight = 0.1\n",
    "data = [8.5, 9, 10, 4]\n",
    "input = data[0]\n",
    "neural_network(input,weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network with multiple inputs\n",
    "\n",
    "def weight_sum(a,b):\n",
    "    assert (len(a) == len(b))\n",
    "    output = 0\n",
    "    for i in range(len(a)):\n",
    "        output += a[i] * b[i]\n",
    "    \n",
    "    return output\n",
    "\n",
    "def neural_network(input, weight):\n",
    "    prediction = weight_sum(input, weight)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "data1 = [8.5, 9.5, 9.9, 9.0]\n",
    "data2 = [0.65, 0.8, 0.8, 0.9]\n",
    "data3 = [1.2, 1.3, 0.5, 1.0]\n",
    "\n",
    "input = [data1[0], data2[0], data3[0]];\n",
    "weights = [0.1, 0.2, 0]\n",
    "\n",
    "prediction = neural_network(input, weights)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network with multiple inputs using numpy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def neural_network(input, weights):\n",
    "    prediction = input.dot(weights)\n",
    "    return prediction\n",
    "\n",
    "data1 = np.array([8.5, 9.5, 9.9, 9.0])\n",
    "data2 = np.array([0.65, 0.8, 0.8, 0.9])\n",
    "data3 = np.array([1.2, 1.3, 0.5, 1.0])\n",
    "weights = np.array([0.1, 0.2, 0])\n",
    "input = np.array([data1[0], data2[0], data3[0]])\n",
    "\n",
    "prediction = neural_network(input, weights)\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural networks with multiple outputs and one input\n",
    "\n",
    "def mul(number, vector):\n",
    "    output = [0,0,0]\n",
    "    assert(len(output) == len(vector))\n",
    "\n",
    "    for i in range(len(vector)):\n",
    "        output[i] = number * vector[i]\n",
    "    \n",
    "    return output\n",
    "\n",
    "def neural_network(input, weights):\n",
    "    predictions = mul(input,weights)\n",
    "    return prediction\n",
    "\n",
    "data1 = [0.65, 0.8, 0.8, 0.9]\n",
    "weight = [0.3, 0.2, 0.9]\n",
    "input = data1[0]\n",
    "\n",
    "prediction = neural_network(input, weight)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a neural network with multiple inputs and ouputs\n",
    "# it neural net has a hidden layer \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def neural_network(input, weights):\n",
    "    hidden_prediction = input.dot(weights[0])\n",
    "    prediction = hidden_prediction.dot(weights[1])\n",
    "\n",
    "    return prediction\n",
    "\n",
    "input_to_hidden_weight = np.array([\n",
    "                                [0.1, 0.2, -0.1],\n",
    "                                [-0.1,0.1, 0.9],\n",
    "                                [0.1, 0.4, 0.1]])\n",
    "\n",
    "hidden_to_output_weight = np.array([\n",
    "                                [0.3, 1.1, -0.3],\n",
    "                                [0.1, 0.2, 0.0],\n",
    "                                [0.0, 1.3, 0.1]])\n",
    "\n",
    "weights = [input_to_hidden_weight, hidden_to_output_weight]\n",
    "\n",
    "data1 = np.array([8.5, 9.5, 9.9, 9.0])\n",
    "data2 = np.array([0.65,0.8, 0.8, 0.9])\n",
    "data3 = np.array([1.2, 1.3, 0.5, 1.0])\n",
    "input = np.array([data1[0],data2[0],data3[0]])\n",
    "\n",
    "prediction = neural_network(input, weights)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "How neural networks learn?\n",
    "\n",
    "Learning means adjusting the weight to reduce the error to 0, one technique is using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as '_psutil_linux' could not be imported from 'most likely due to a circular import'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# ERROR OF A SINGLE NEURAL NETWORK \n",
    "\n",
    "weight = 0.5\n",
    "input = 0.5\n",
    "goal_prediction = 0.8\n",
    "\n",
    "prediction = input * weight\n",
    "\n",
    "error = (prediction - goal_prediction) ** 2 #this is a \"pure error the \"**2\" force the error to be positive\"\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE HOT AND COLD LEARNING\n",
    "\n",
    "\"\"\"\n",
    "some problems that have this method is:\n",
    "    1- it is inefficient because you have to predict multiple times for a single weight\n",
    "    2- sometimes it is possible to predict the goal_prediction good\n",
    "\"\"\"\n",
    "  \n",
    "weight = 0.5\n",
    "input = 0.5\n",
    "goal_prediction = 0.8\n",
    "\n",
    "step_amount = 0.001 # how much the weights are moving in an iteraction\n",
    "\n",
    "for iteraction in range(1101):\n",
    "    prediction = input * weight\n",
    "    error = (prediction - goal_prediction) ** 2\n",
    "    \n",
    "    print(\"Error:\" + str(error) + \"Prediction:\" + str(prediction))\n",
    "\n",
    "    prediction_up = input * (weight + step_amount) # trying to move up\n",
    "    error_up = (goal_prediction - prediction_up) ** 2\n",
    "\n",
    "    prediction_down = input * (weight - step_amount) # trying to move down\n",
    "    error_down = (goal_prediction - prediction_down) ** 2\n",
    "\n",
    "    if (error_down < error_up):\n",
    "        weight = weight - step_amount\n",
    "    if (error_down > error_up):\n",
    "        weight = weight + step_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ONE SIMPLE GRADIENT DESCENT\n",
    "\n",
    "weight = 0.5\n",
    "goal_prediction = 0.8\n",
    "input = 0.5\n",
    "\n",
    "for iteraction in range(20):\n",
    "    prediction = input * weight\n",
    "    error = (prediction - goal_prediction) ** 2\n",
    "    direction_and_amount = (prediction - goal_prediction) * input # this represents the sensitivity (the update of the weight) \n",
    "    weight = weight - direction_and_amount\n",
    "\n",
    "    print(\"Error:\" + str(error) + \"Prediction:\" + str(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A COMPLETE GRADIENT DESCENT\n",
    "# play with it and change the variables you will find strange things\n",
    "\n",
    "weight, goal_prediction, input = (1, 0.8, 5)  #with this input the neural network divergence\n",
    "\n",
    "for iteration in range(4):\n",
    "    print(\"------\\nWeight:\" + str(weight))\n",
    "    \n",
    "    prediction = input * weight\n",
    "    error = (prediction - goal_prediction) ** 2 #absolute error\n",
    "    delta = prediction - goal_prediction #delta Error\n",
    "    weight_delta = delta * input \n",
    "    weight = weight - weight_delta #update of the weight reducing the delta\n",
    "\n",
    "    print(\"Error:\" + str(error) + \"Prediction:\" + str(prediction))\n",
    "    print(\"Delta:\" + str(delta) + \"Weight Delta:\" + str(weight_delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADIENDT DESCENT WITH ALPHA\n",
    "# alpha prevent divergence\n",
    "\n",
    "weight, goal_prediction, input, alpha = (0.5, 0.8, 2, 0.1)\n",
    "\n",
    "for iteraction in range(20):\n",
    "    print(\"------\\nWeight:\" + str(weight))\n",
    "\n",
    "    prediction = input * weight\n",
    "    error = (prediction - goal_prediction) ** 2\n",
    "    delta = (prediction - goal_prediction)\n",
    "    weight_delta = input * delta # devirative\n",
    "\n",
    "    weight = weight - (weight_delta * alpha) #there is our alpha \n",
    "    \n",
    "    print(\"Error:\" + str(error) + \"Prediction:\" + str(prediction))\n",
    "    print(\"Delta:\" + str(delta) + \"Weight Delta:\" + str(weight_delta))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADIENT DESCENT WITH MULTIPLE INPUTS\n",
    "\n",
    "def neural_network(input, weights):\n",
    "    prediction = 0\n",
    "    for i in range(len(input)):\n",
    "        prediction += input[i] * weights[i]\n",
    "\n",
    "    return prediction\n",
    "\n",
    "def vector_multiplication(scalar, vector):\n",
    "    output = [0,0,0]\n",
    "    for i in range(len(output)):\n",
    "        output[i] = vector[i] * scalar\n",
    "\n",
    "    return output\n",
    "\n",
    "#data\n",
    "data1 = [8.5, 9.5, 9.9, 9.0]\n",
    "data2 = [0.65, 0.8, 0.8, 0.9]\n",
    "data3 = [1.2, 1.3, 0.5, 1.0]\n",
    "\n",
    "goal_prediction = [1, 1, 0, 1]\n",
    "first_goal = goal_prediction[0]\n",
    "\n",
    "alpha= 0.1\n",
    "weights = [0.1, 0.2, -.1]\n",
    "input = [data1[0], data2[0], data3[0]]\n",
    "\n",
    "#gradient descent\n",
    "for iteraction in range(3):\n",
    "    prediction = neural_network(input, weights)\n",
    "\n",
    "    error = (prediction - first_goal) ** 2\n",
    "    delta = (prediction - first_goal)\n",
    "\n",
    "    weight_deltas = vector_multiplication(alpha, input) \n",
    "\n",
    "    print(\"Iteration:\" + str(iteraction+1))\n",
    "    print(\"Prediction:\" + str(prediction))\n",
    "    print(\"Error:\" + str(error))\n",
    "    print(\"Delta:\" + str(delta))\n",
    "    print(\"Weights:\" + str(weights))\n",
    "    print(\"Weight_Deltas:\" + str(weight_deltas))\n",
    "    print(\" \")\n",
    "    # update the weights\n",
    "    for i in range(len(weights)):\n",
    "        weights[i] -= alpha * weight_deltas[i] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADIENT DESCENT WITH MULTIPLE OUTPUTS\n",
    "\n",
    "def neural_network(input, weights):\n",
    "    prediction = vector_multiplication(input, weights)\n",
    "\n",
    "    return prediction\n",
    "\n",
    "def vector_multiplication(scalar, vector):\n",
    "    output = [0,0,0]\n",
    "\n",
    "    assert(len(output) == len(vector))\n",
    "    for i in range(len(output)):\n",
    "        output[i] = vector[i] * scalar\n",
    "\n",
    "    return output\n",
    "\n",
    "#data\n",
    "data = [8.5, 9.5, 9.9, 9.0]\n",
    "\n",
    "goal_prediction1 = [0.1, 1, 0, 0.1]\n",
    "goal_prediction2= [1, 1, 0, 1]\n",
    "goal_prediction3 = [0.1, 0, 0.1, 0.2]\n",
    "\n",
    "input = data[0]\n",
    "first_goal = [goal_prediction1[0], goal_prediction2[0], goal_prediction3[0]]\n",
    "\n",
    "error = [0,0,0]\n",
    "delta = [0,0,0]\n",
    "alpha = 0.1\n",
    "weights = [0.3, 0.2, 0.9]\n",
    "\n",
    "prediction = neural_network(input, weights)\n",
    "\n",
    "for iteraction in range(3):\n",
    "    for i in range(len(first_goal)):\n",
    "        error[i] = (prediction[i] - first_goal[i]) ** 2\n",
    "        delta[i] = prediction[i] - first_goal[i]\n",
    "\n",
    "    weight_deltas = vector_multiplication(input, weights)\n",
    "\n",
    "    for i in range(len(weights)):\n",
    "        weights[i] -= weight_deltas[i] * alpha\n",
    "\n",
    "    print(\"Iteraction:\" + str(iteraction))\n",
    "    print(\"Weights:\" + str(weights))\n",
    "    print(\"Weight Deltas:\" + str(weight_deltas))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADIENT DESCENT WITH MULTIPLE INPUTS AND OUTPUTS\n",
    "\n",
    "def weight_sum(a,b):\n",
    "    assert (len(a) == len(b))\n",
    "    output = 0\n",
    "    for i in range(len(a)):\n",
    "        output += a[i] * b[i]\n",
    "\n",
    "def matrix_multiplication(vector, matrix):\n",
    "    assert (len(vector) == len(matrix))\n",
    "    output = [0,0,0]\n",
    "    \n",
    "    for i in range(len(vector)):\n",
    "        output[i] = weight_sum(vector, matrix[i])\n",
    "    \n",
    "    return output\n",
    "\n",
    "def neural_network(input, weights):\n",
    "    prediction = matrix_multiplication(input, weights)\n",
    "\n",
    "    return prediction\n",
    "\n",
    "def product(vector_a, vector_b):\n",
    "    output = [[0, 0, 0],[0, 0, 0], [0, 0, 0]]\n",
    "\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            output[i][j] = vector_a[i] * vector_b[j]\n",
    "    \n",
    "    return output\n",
    "\n",
    "# data\n",
    "\n",
    "weights = [[0.1, 0.1, -0.3],[0.1, 0.2, 0.0], [0.0, 1.3, 0.1]]\n",
    "\n",
    "data1 = [8.5, 9.5, 9.9, 9.0]\n",
    "data2 = [0.65,0.8, 0.8, 0.9]\n",
    "data3 = [1.2, 1.3, 0.5, 1.0]\n",
    "\n",
    "goal_prediction1 = [0.1, 0.0, 0.0, 0.1]\n",
    "goal_prediction2 = [1.0, 1.0, 0.0, 1.0]\n",
    "goal_prediction3 = [0.1, 0.0, 0.1, 0.2]\n",
    "\n",
    "alpha = 0.01\n",
    "\n",
    "input = [data1[0], data2[0], data3[0]]\n",
    "first_goal = [goal_prediction1[0], goal_prediction2[0], goal_prediction3[0]]\n",
    "\n",
    "prediction = neural_network(input,weights)\n",
    "\n",
    "error = [0,0,0]\n",
    "delta = [0,0,0]\n",
    "\n",
    "for i in range(len(first_goal)):\n",
    "    error[i] = (prediction[i] - first_goal[i]) ** 2\n",
    "    delta[i] = prediction[i] - first_goal[i]\n",
    "\n",
    "weight_deltas = product(input, delta)\n",
    "\n",
    "for i in range(len(weights)):\n",
    "    for j in range(len(weights[0])):\n",
    "        weights[i][j] -= alpha * weight_deltas[i][j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The streetlight problem\n",
    "\n",
    "we want to create a neural network to understand the pattern of a wear streetlight with input of the streetlight pattern and the output of when to walk  or not to walk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOCHASTIC GRADIENT DESCENT\n",
    "# here  we  try to update the weights each time we get a new sample.\n",
    "\n",
    "\"\"\"\n",
    "***************THE STREETLIGHT PROBLEM********************\n",
    "\n",
    "we want to create a neural network to understand the pattern of a wear \n",
    "streetlight with input of the streetlight pattern and the output of when to \n",
    "walk  or not to walk.\n",
    "\"\"\"\n",
    "\n",
    "# each node work individual they only share the error measure\n",
    "weights = np.array([0.5, 0.48, -0.7])\n",
    "alpha = 0.01\n",
    "streetlights = np.array([   [1, 0, 1], \n",
    "                            [0, 1, 1], \n",
    "                            [0, 0, 1],\n",
    "                            [1, 1, 1],\n",
    "                            [0, 1, 1],\n",
    "                            [1, 0, 1] ])\n",
    "walk_vs_stop = np.array([0,1,0,1,1,0])\n",
    "input = streetlights[0]\n",
    "goal_prediction = walk_vs_stop[0]\n",
    "\n",
    "for interaction in range(20):\n",
    "    error_of_lights = 0\n",
    "    for row in range(len(walk_vs_stop)):\n",
    "        input = streetlights[row]\n",
    "        goal_prediction = walk_vs_stop[row]\n",
    "        prediction = np.dot(input, weights)\n",
    "        error = (goal_prediction - prediction) ** 2\n",
    "        error_of_lights += error\n",
    "\n",
    "        delta = prediction - goal_prediction\n",
    "        weights = weights - alpha * delta * input\n",
    "\n",
    "        print(\"Prediction:\" + str(prediction))\n",
    "\n",
    "    print(\"Error:\" + str(error_of_lights) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks with Pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network with pytorch\n",
    "from torch import nn\n",
    "image = torch.randn(3,10,20) #dataset\n",
    "d0 = image.nelement()\n",
    "\n",
    "class myNet(nn.Module):\n",
    "    def __init__(self, d0, d1, d2, d3):\n",
    "        super().__init__()\n",
    "        self.l0 = nn.Linear(d0, d1) #layer 0\n",
    "        print(self.l0)\n",
    "        self.l1 = nn.Linear(d1, d2) #layer 1\n",
    "        print(self.l1)\n",
    "        self.l2 = nn.Linear(d2, d3) #layer 2\n",
    "\n",
    "        print(self.l2)\n",
    "    def forward(self, x): # this compute the outputs with the none linear function \n",
    "        z0 = x.view(-1) #flatten input tensor\n",
    "        s1 = self.l0(z0)\n",
    "        z1 = torch.relu(s1)\n",
    "        s2 = self.l1(z1)\n",
    "        z2 = torch.relu(s2)\n",
    "        s3 = self.l2(z2)\n",
    "        return s3\n",
    "\n",
    "model = myNet(d0, 60, 40, 10)\n",
    "out = model(image)\n",
    "\n",
    "print(\"output\",out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
