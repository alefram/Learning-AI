{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Learning neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Neural Networks\n",
    "\n",
    "### what is a input data?\n",
    "\n",
    "It is a number that you recorded in the real world somewhere.\n",
    "\n",
    "It is important that, the datapoints that you pass at the same time to the neural network depends on when you think that the neural network can be accurate with this data. \n",
    "\n",
    "Also we can put negative numbers in the data.\n",
    "\n",
    "### what is a prediction?\n",
    "\n",
    "A prediction is what the neural network tells you. It not always is right but it can learn from it. It can predict with multiple inputs and multiple outputs.\n",
    "\n",
    "### what is a weight?\n",
    "\n",
    "It is like the sensitive of the net. When you train the neural net it means to adjust the weights for have an accurate prediction how we can do it?\n",
    "\n",
    "The objective of build a neural network is to compute a error function with a bunch of weight. Then you will have a relationship of the error and the weight of the network and with that information, you can change the weight to reduce the error to 0.\n",
    "\n",
    "You will  use the alpha variable to reduce weight update so it doesn’t overshoot.\n",
    "\n",
    "### Learning methods\n",
    "\n",
    "Hot and cold: it consist on wiggling the weight to see when it reduce the error\n",
    "\n",
    "gradient descent: it consist on to find the minimums error for learning.\n",
    "\n",
    "Exists 3 gradient descent. When we talk about stochastic gradient descent, it updates the weights one example at the time. The full gradient descent, it updates one dataset at the time. Batch gradient descent, in this case you choose a batch size for updates the weight. \n",
    "\n",
    "### Overfitting\n",
    "\n",
    "\"Error is shared among all the weights. If a particular configuration of weights accidentally creates perfect correlation between the prediction and the output dataset without giving the heaviest weight to the best inputs, the neural network\n",
    "will stop learnin\". andrew trask\n",
    "\n",
    "The greatest challenge is convincing my neural network to generalize instead of just memorize\n",
    "\n",
    "### Backpropagation\n",
    "\n",
    "It consist  on moving delta around to take a correlation of input and output. More technicaly it is about calculating deltas for intermediate layers for perform gradient descent \n",
    "\n",
    "\n",
    "### tips regularizations(ignore noise)\n",
    "\n",
    "<ul>\n",
    "    <li>neural networks find and create correlation</li>\n",
    "    <li>stop training the NN when it getting worse to dont overfit</li>\n",
    "    <li>\n",
    "        use validation dataset for see when to stop train the neural net\n",
    "    </li>\n",
    "    <li>\n",
    "        dropout: radomly turn off the neurons(set them to 0) during training (multiply the layer values by a random matrix of 1s and 0s)\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "### Correlation summarization\n",
    "\n",
    "\"Neural networks seek to find direct and indirect correlation between an input layer and an output layer, which are determined by the input \n",
    "and output datasets, respectively\". andrew Trask\n",
    "\n",
    "### Local Correlation summarization\n",
    "\n",
    "\"Any given set of weights optimizes to learn how to correlate its input layer with what the output layer says it should be.\" andrew Trask\n",
    "\n",
    "### Global correlation summarization\n",
    "\n",
    "\"What an earlier layer says it should be can be determined by taking what a later layer says it should be and multiplying it by the weights in between them. This way, later layers can tell earlier layers what kind of signal they need, to ultimately find correlation with the output. This cross-communication is called backpropagation.\" andrew Trask\n",
    "\n",
    "### notation\n",
    "\n",
    "<ul>\n",
    "    <li>\n",
    "        I0W0: Take the layer 0 vector and perform vector-\n",
    "        matrix multiplication with the weight matrix 0.”\n",
    "    </li>\n",
    "    <li>\n",
    "        l1 = relu(l0W0): To create the layer 1 vector, take the layer 0 vector and perform vector-matrix multiplication with the weight matrix 0; then perform the relu function on the output (setting all negative numbers to 0).\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "## Activation Functions\n",
    "\n",
    "It is a function applied to the neurons in a layer during the prediction.\n",
    "\n",
    "### Constraints\n",
    "\n",
    "<ul>\n",
    "    <li>The function must be continuous and infinite in domain.</li>\n",
    "    <li>Good activation functions are monotonic, never changing direction</li>\n",
    "    <li>Good activation functions are nonlinear (they squiggle or turn)</li>\n",
    "    <li>\n",
    "        Good activation functions (and their derivatives) should be efficiently computable.\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "### Typical activation functions\n",
    "\n",
    "- Sigmoid\n",
    "\n",
    "- tanh\n",
    "\n",
    "    It is better for hidden layers\n",
    "\n",
    "- softmax\n",
    "\n",
    "    for predict a single lable\n",
    "\n",
    "### typical configurations output layer\n",
    "\n",
    "* configuration 1 Predicting raw data into values(no act function)\n",
    "\n",
    "    One case could be people want to train a neural network to transform one matrix into another where the range of output is something is a probability. It means we want the right answer no 0 or 1.\n",
    "\n",
    "* configuration 2  predicting unrelated (yes/no) probabilities(sigmoid)\n",
    "\n",
    "    If we want to make multiple binary probabilities  in one neural network, it’s best to use the sigmoid activation function, because it models individual probabilities separately for each output node.\n",
    "\n",
    "* configuration 3 predicting which-one probabilities(softmax) [chapter 9]\n",
    "\n",
    "    The most common use of neural networks is predicting a single label out of many. softmax asks, “Which digit seems like the best fit for this input?\" softmax raises each input value exponentially and then\n",
    "    divides by the layer’s sum.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "If a particular configuration of the weights accidentally creates a perfect correlation of the prediction and the output dataset without any noise, then the model is said to be overfitting. It means that the model is not general enough to generalize to new data. It will not learn just memorize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Backpropagation\n",
    "\n",
    "backpropagation is about calculating delta values for each weight in intermediate layers for perform gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "def relu(x):\n",
    "    return (x > 0) * x\n",
    "\n",
    "def relu2deriv(output): # this activates the ouputs necessary for the correlation\n",
    "    return output > 0\n",
    "\n",
    "streetlights = np.array([   [1, 0, 1], \n",
    "                            [0, 1, 1], \n",
    "                            [0, 0, 1],\n",
    "                            [1, 1, 1]])\n",
    "\n",
    "walk_vs_stop = np.array([[ 1, 1, 0, 0]]).T # goal prediction\n",
    "alpha = 0.2\n",
    "hidden_size = 4 # it means there is 4 nodes\n",
    "weights_0_1 = 2 * np.random.random((3,hidden_size)) - 1\n",
    "weights_1_2 = 2 * np.random.random((hidden_size,1)) - 1\n",
    "\n",
    "# backpropagation\n",
    "for iteration in range(50):\n",
    "    error_layer_2 = 0\n",
    "\n",
    "    for i in range(len(streetlights)):\n",
    "        layer_0 = streetlights[i:i+1]\n",
    "        layer_1 = relu(np.dot(layer_0,weights_0_1))\n",
    "        layer_2 = np.dot(layer_1,weights_1_2)\n",
    "\n",
    "        error_layer_2 += np.sum((layer_2 - walk_vs_stop[i:i+1] ) ** 2)\n",
    "        layer_2_delta = (layer_2 - walk_vs_stop[i:i+1])\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1)  # here happens de backpropagation\n",
    "\n",
    "        weights_1_2 -= alpha * layer_1.T.dot(layer_2_delta)\n",
    "        weights_0_1 -= alpha * layer_0.T.dot(layer_1_delta)\n",
    "\n",
    "    if(iteration % 10 == 9):\n",
    "        print(\"Error:\" + str(error_layer_2))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Forward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Neural network with a single datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(input, weight):\n",
    "    prediction = input * weight\n",
    "    return prediction\n",
    "\n",
    "weight = 0.1\n",
    "data = [8.5, 9, 10, 4]\n",
    "input = data[0]\n",
    "neural_network(input,weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Neural network with multiple inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_sum(a,b):\n",
    "    assert (len(a) == len(b))\n",
    "    output = 0\n",
    "    for i in range(len(a)):\n",
    "        output += a[i] * b[i]\n",
    "    \n",
    "    return output\n",
    "\n",
    "def neural_network(input, weight):\n",
    "    prediction = weight_sum(input, weight)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "data1 = [8.5, 9.5, 9.9, 9.0]\n",
    "data2 = [0.65, 0.8, 0.8, 0.9]\n",
    "data3 = [1.2, 1.3, 0.5, 1.0]\n",
    "\n",
    "input = [data1[0], data2[0], data3[0]];\n",
    "weights = [0.1, 0.2, 0]\n",
    "\n",
    "prediction = neural_network(input, weights)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Neural network with multiple inputs using numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def neural_network(input, weights):\n",
    "    prediction = input.dot(weights)\n",
    "    return prediction\n",
    "\n",
    "data1 = np.array([8.5, 9.5, 9.9, 9.0])\n",
    "data2 = np.array([0.65, 0.8, 0.8, 0.9])\n",
    "data3 = np.array([1.2, 1.3, 0.5, 1.0])\n",
    "weights = np.array([0.1, 0.2, 0])\n",
    "input = np.array([data1[0], data2[0], data3[0]])\n",
    "\n",
    "prediction = neural_network(input, weights)\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Neural networks with multiple outputs and one input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mul(number, vector):\n",
    "    output = [0,0,0]\n",
    "    assert(len(output) == len(vector))\n",
    "\n",
    "    for i in range(len(vector)):\n",
    "        output[i] = number * vector[i]\n",
    "    \n",
    "    return output\n",
    "\n",
    "def neural_network(input, weights):\n",
    "    predictions = mul(input,weights)\n",
    "    return prediction\n",
    "\n",
    "data1 = [0.65, 0.8, 0.8, 0.9]\n",
    "weight = [0.3, 0.2, 0.9]\n",
    "input = data1[0]\n",
    "\n",
    "prediction = neural_network(input, weight)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Neural network with multiple inputs and outputs and hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(input, weights):\n",
    "    hidden_prediction = input.dot(weights[0])\n",
    "    prediction = hidden_prediction.dot(weights[1])\n",
    "\n",
    "    return prediction\n",
    "\n",
    "input_to_hidden_weight = np.array([\n",
    "                                [0.1, 0.2, -0.1],\n",
    "                                [-0.1,0.1, 0.9],\n",
    "                                [0.1, 0.4, 0.1]])\n",
    "\n",
    "hidden_to_output_weight = np.array([\n",
    "                                [0.3, 1.1, -0.3],\n",
    "                                [0.1, 0.2, 0.0],\n",
    "                                [0.0, 1.3, 0.1]])\n",
    "\n",
    "weights = [input_to_hidden_weight, hidden_to_output_weight]\n",
    "\n",
    "data1 = np.array([8.5, 9.5, 9.9, 9.0])\n",
    "data2 = np.array([0.65,0.8, 0.8, 0.9])\n",
    "data3 = np.array([1.2, 1.3, 0.5, 1.0])\n",
    "input = np.array([data1[0],data2[0],data3[0]])\n",
    "\n",
    "prediction = neural_network(input, weights)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "How neural networks learn?\n",
    "\n",
    "Learning means adjusting the weight to reduce the error to 0, one technique is using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = 0.5\n",
    "input = 0.5\n",
    "goal_prediction = 0.8\n",
    "\n",
    "prediction = input * weight\n",
    "\n",
    "error = (prediction - goal_prediction) ** 2 #this is a \"pure error the \"**2\" force the error to be positive\"\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The hot and cold learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "some problems that have this method is:\n",
    "    1- it is inefficient because you have to predict multiple times for a single weight\n",
    "    2- sometimes it is possible to predict the goal_prediction good\n",
    "\"\"\"\n",
    "  \n",
    "weight = 0.5\n",
    "input = 0.5\n",
    "goal_prediction = 0.8\n",
    "\n",
    "step_amount = 0.001 # how much the weights are moving in an iteraction\n",
    "\n",
    "for iteraction in range(1101):\n",
    "    prediction = input * weight\n",
    "    error = (prediction - goal_prediction) ** 2\n",
    "    \n",
    "    print(\"Error:\" + str(error) + \"Prediction:\" + str(prediction))\n",
    "\n",
    "    prediction_up = input * (weight + step_amount) # trying to move up\n",
    "    error_up = (goal_prediction - prediction_up) ** 2\n",
    "\n",
    "    prediction_down = input * (weight - step_amount) # trying to move down\n",
    "    error_down = (goal_prediction - prediction_down) ** 2\n",
    "\n",
    "    if (error_down < error_up):\n",
    "        weight = weight - step_amount\n",
    "    if (error_down > error_up):\n",
    "        weight = weight + step_amount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = 0.5\n",
    "goal_prediction = 0.8\n",
    "input = 0.5\n",
    "\n",
    "for iteraction in range(20):\n",
    "    prediction = input * weight\n",
    "    error = (prediction - goal_prediction) ** 2\n",
    "    direction_and_amount = (prediction - goal_prediction) * input # this represents the sensitivity (the update of the weight) \n",
    "    weight = weight - direction_and_amount\n",
    "\n",
    "    print(\"Error:\" + str(error) + \"Prediction:\" + str(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play with it and change the variables you will find strange things\n",
    "\n",
    "weight, goal_prediction, input = (1, 0.8, 5)  #with this input the neural network divergence\n",
    "\n",
    "for iteration in range(4):\n",
    "    print(\"------\\nWeight:\" + str(weight))\n",
    "    \n",
    "    prediction = input * weight\n",
    "    error = (prediction - goal_prediction) ** 2 #absolute error\n",
    "    delta = prediction - goal_prediction #delta Error\n",
    "    weight_delta = delta * input \n",
    "    weight = weight - weight_delta #update of the weight reducing the delta\n",
    "\n",
    "    print(\"Error:\" + str(error) + \"Prediction:\" + str(prediction))\n",
    "    print(\"Delta:\" + str(delta) + \"Weight Delta:\" + str(weight_delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent with alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha prevent divergence\n",
    "\n",
    "weight, goal_prediction, input, alpha = (0.5, 0.8, 2, 0.1)\n",
    "\n",
    "for iteraction in range(20):\n",
    "    print(\"------\\nWeight:\" + str(weight))\n",
    "\n",
    "    prediction = input * weight\n",
    "    error = (prediction - goal_prediction) ** 2\n",
    "    delta = (prediction - goal_prediction)\n",
    "    weight_delta = input * delta # devirative\n",
    "\n",
    "    weight = weight - (weight_delta * alpha) #there is our alpha \n",
    "    \n",
    "    print(\"Error:\" + str(error) + \"Prediction:\" + str(prediction))\n",
    "    print(\"Delta:\" + str(delta) + \"Weight Delta:\" + str(weight_delta))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent with multiple inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(input, weights):\n",
    "    prediction = 0\n",
    "    for i in range(len(input)):\n",
    "        prediction += input[i] * weights[i]\n",
    "\n",
    "    return prediction\n",
    "\n",
    "def vector_multiplication(scalar, vector):\n",
    "    output = [0,0,0]\n",
    "    for i in range(len(output)):\n",
    "        output[i] = vector[i] * scalar\n",
    "\n",
    "    return output\n",
    "\n",
    "#data\n",
    "data1 = [8.5, 9.5, 9.9, 9.0]\n",
    "data2 = [0.65, 0.8, 0.8, 0.9]\n",
    "data3 = [1.2, 1.3, 0.5, 1.0]\n",
    "\n",
    "goal_prediction = [1, 1, 0, 1]\n",
    "first_goal = goal_prediction[0]\n",
    "\n",
    "alpha= 0.1\n",
    "weights = [0.1, 0.2, -.1]\n",
    "input = [data1[0], data2[0], data3[0]]\n",
    "\n",
    "#gradient descent\n",
    "for iteraction in range(3):\n",
    "    prediction = neural_network(input, weights)\n",
    "\n",
    "    error = (prediction - first_goal) ** 2\n",
    "    delta = (prediction - first_goal)\n",
    "\n",
    "    weight_deltas = vector_multiplication(alpha, input) \n",
    "\n",
    "    print(\"Iteration:\" + str(iteraction+1))\n",
    "    print(\"Prediction:\" + str(prediction))\n",
    "    print(\"Error:\" + str(error))\n",
    "    print(\"Delta:\" + str(delta))\n",
    "    print(\"Weights:\" + str(weights))\n",
    "    print(\"Weight_Deltas:\" + str(weight_deltas))\n",
    "    print(\" \")\n",
    "    # update the weights\n",
    "    for i in range(len(weights)):\n",
    "        weights[i] -= alpha * weight_deltas[i] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent with multiple outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(input, weights):\n",
    "    prediction = vector_multiplication(input, weights)\n",
    "\n",
    "    return prediction\n",
    "\n",
    "def vector_multiplication(scalar, vector):\n",
    "    output = [0,0,0]\n",
    "\n",
    "    assert(len(output) == len(vector))\n",
    "    for i in range(len(output)):\n",
    "        output[i] = vector[i] * scalar\n",
    "\n",
    "    return output\n",
    "\n",
    "#data\n",
    "data = [8.5, 9.5, 9.9, 9.0]\n",
    "\n",
    "goal_prediction1 = [0.1, 1, 0, 0.1]\n",
    "goal_prediction2= [1, 1, 0, 1]\n",
    "goal_prediction3 = [0.1, 0, 0.1, 0.2]\n",
    "\n",
    "input = data[0]\n",
    "first_goal = [goal_prediction1[0], goal_prediction2[0], goal_prediction3[0]]\n",
    "\n",
    "error = [0,0,0]\n",
    "delta = [0,0,0]\n",
    "alpha = 0.1\n",
    "weights = [0.3, 0.2, 0.9]\n",
    "\n",
    "prediction = neural_network(input, weights)\n",
    "\n",
    "for iteraction in range(3):\n",
    "    for i in range(len(first_goal)):\n",
    "        error[i] = (prediction[i] - first_goal[i]) ** 2\n",
    "        delta[i] = prediction[i] - first_goal[i]\n",
    "\n",
    "    weight_deltas = vector_multiplication(input, weights)\n",
    "\n",
    "    for i in range(len(weights)):\n",
    "        weights[i] -= weight_deltas[i] * alpha\n",
    "\n",
    "    print(\"Iteraction:\" + str(iteraction))\n",
    "    print(\"Weights:\" + str(weights))\n",
    "    print(\"Weight Deltas:\" + str(weight_deltas))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent with multiple inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_sum(a,b):\n",
    "    assert (len(a) == len(b))\n",
    "    output = 0\n",
    "    for i in range(len(a)):\n",
    "        output += a[i] * b[i]\n",
    "\n",
    "def matrix_multiplication(vector, matrix):\n",
    "    assert (len(vector) == len(matrix))\n",
    "    output = [0,0,0]\n",
    "    \n",
    "    for i in range(len(vector)):\n",
    "        output[i] = weight_sum(vector, matrix[i])\n",
    "    \n",
    "    return output\n",
    "\n",
    "def neural_network(input, weights):\n",
    "    prediction = matrix_multiplication(input, weights)\n",
    "\n",
    "    return prediction\n",
    "\n",
    "def product(vector_a, vector_b):\n",
    "    output = [[0, 0, 0],[0, 0, 0], [0, 0, 0]]\n",
    "\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            output[i][j] = vector_a[i] * vector_b[j]\n",
    "    \n",
    "    return output\n",
    "\n",
    "# data\n",
    "\n",
    "weights = [[0.1, 0.1, -0.3],[0.1, 0.2, 0.0], [0.0, 1.3, 0.1]]\n",
    "\n",
    "data1 = [8.5, 9.5, 9.9, 9.0]\n",
    "data2 = [0.65,0.8, 0.8, 0.9]\n",
    "data3 = [1.2, 1.3, 0.5, 1.0]\n",
    "\n",
    "goal_prediction1 = [0.1, 0.0, 0.0, 0.1]\n",
    "goal_prediction2 = [1.0, 1.0, 0.0, 1.0]\n",
    "goal_prediction3 = [0.1, 0.0, 0.1, 0.2]\n",
    "\n",
    "alpha = 0.01\n",
    "\n",
    "input = [data1[0], data2[0], data3[0]]\n",
    "first_goal = [goal_prediction1[0], goal_prediction2[0], goal_prediction3[0]]\n",
    "\n",
    "prediction = neural_network(input,weights)\n",
    "\n",
    "error = [0,0,0]\n",
    "delta = [0,0,0]\n",
    "\n",
    "for i in range(len(first_goal)):\n",
    "    error[i] = (prediction[i] - first_goal[i]) ** 2\n",
    "    delta[i] = prediction[i] - first_goal[i]\n",
    "\n",
    "weight_deltas = product(input, delta)\n",
    "\n",
    "for i in range(len(weights)):\n",
    "    for j in range(len(weights[0])):\n",
    "        weights[i][j] -= alpha * weight_deltas[i][j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The streetlight problem\n",
    "\n",
    "we want to create a neural network to understand the pattern of a wear streetlight with input of the streetlight pattern and the output of when to walk  or not to walk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOCHASTIC GRADIENT DESCENT\n",
    "# here  we  try to update the weights each time we get a new sample.\n",
    "\n",
    "\"\"\"\n",
    "***************THE STREETLIGHT PROBLEM********************\n",
    "\n",
    "we want to create a neural network to understand the pattern of a wear \n",
    "streetlight with input of the streetlight pattern and the output of when to \n",
    "walk  or not to walk.\n",
    "\"\"\"\n",
    "\n",
    "# each node work individual they only share the error measure\n",
    "weights = np.array([0.5, 0.48, -0.7])\n",
    "alpha = 0.01\n",
    "streetlights = np.array([   [1, 0, 1], \n",
    "                            [0, 1, 1], \n",
    "                            [0, 0, 1],\n",
    "                            [1, 1, 1],\n",
    "                            [0, 1, 1],\n",
    "                            [1, 0, 1] ])\n",
    "walk_vs_stop = np.array([0,1,0,1,1,0])\n",
    "input = streetlights[0]\n",
    "goal_prediction = walk_vs_stop[0]\n",
    "\n",
    "for interaction in range(20):\n",
    "    error_of_lights = 0\n",
    "    for row in range(len(walk_vs_stop)):\n",
    "        input = streetlights[row]\n",
    "        goal_prediction = walk_vs_stop[row]\n",
    "        prediction = np.dot(input, weights)\n",
    "        error = (goal_prediction - prediction) ** 2\n",
    "        error_of_lights += error\n",
    "\n",
    "        delta = prediction - goal_prediction\n",
    "        weights = weights - alpha * delta * input\n",
    "\n",
    "        print(\"Prediction:\" + str(prediction))\n",
    "\n",
    "    print(\"Error:\" + str(error_of_lights) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks with Pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import random\n",
    "import math\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network with pytorch\n",
    "image = torch.randn(3,10,20) #dataset\n",
    "d0 = image.nelement()\n",
    "\n",
    "class myNet(nn.Module):\n",
    "    def __init__(self, d0, d1, d2, d3):\n",
    "        super().__init__()\n",
    "        self.l0 = nn.Linear(d0, d1) #layer 0\n",
    "        print(self.l0)\n",
    "        self.l1 = nn.Linear(d1, d2) #layer 1\n",
    "        print(self.l1)\n",
    "        self.l2 = nn.Linear(d2, d3) #layer 2\n",
    "\n",
    "        print(self.l2)\n",
    "    def forward(self, x): # this compute the outputs with the none linear function \n",
    "        z0 = x.view(-1) #flatten input tensor\n",
    "        s1 = self.l0(z0)\n",
    "        z1 = torch.relu(s1)\n",
    "        s2 = self.l1(z1)\n",
    "        z2 = torch.relu(s2)\n",
    "        s3 = self.l2(z2)\n",
    "        return s3\n",
    "\n",
    "model = myNet(d0, 60, 40, 10)\n",
    "out = model(image)\n",
    "\n",
    "print(\"output\",out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Neural Networks with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usefull methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for plotting purposes\n",
    "def plot_data(X, y, d=0, auto=False, zoom=1):\n",
    "    X = X.cpu()\n",
    "    y = y.cpu()\n",
    "    plt.scatter(X.numpy()[:, 0], X.numpy()[:, 1], c=y, s=20, cmap=plt.cm.Spectral)\n",
    "    plt.axis('square')\n",
    "    plt.axis(np.array((-1.1, 1.1, -1.1, 1.1)) * zoom)\n",
    "    if auto is True: plt.axis('equal')\n",
    "    plt.axis('off')\n",
    "\n",
    "    _m, _c = 0, '.15'\n",
    "    plt.axvline(0, ymin=_m, color=_c, lw=1, zorder=0)\n",
    "    plt.axhline(0, xmin=_m, color=_c, lw=1, zorder=0)\n",
    "\n",
    "def set_default(figsize=(10, 10), dpi=100):\n",
    "    plt.style.use(['dark_background', 'bmh'])\n",
    "    plt.rc('axes', facecolor='k')\n",
    "    plt.rc('figure', facecolor='k')\n",
    "    plt.rc('figure', figsize=figsize, dpi=dpi)\n",
    "\n",
    "def plot_model(X, y, model):\n",
    "    model.cpu()\n",
    "    mesh = np.arange(-1.1, 1.1, 0.01)\n",
    "    xx, yy = np.meshgrid(mesh, mesh)\n",
    "    with torch.no_grad():\n",
    "        data = torch.from_numpy(np.vstack((xx.reshape(-1), yy.reshape(-1))).T).float()\n",
    "        Z = model(data).detach()\n",
    "    Z = np.argmax(Z, axis=1).reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.3)\n",
    "    plot_data(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_default(dpi=50)\n",
    "\n",
    "# Create the data\n",
    "seed = 12345\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "N = 1000  # num_samples_per_class\n",
    "D = 2  # dimensions\n",
    "C = 3  # num_classes\n",
    "H = 100  # num_hidden_units\n",
    "\n",
    "X = torch.zeros(N * C, D)\n",
    "y = torch.zeros(N * C, dtype=torch.long)\n",
    "\n",
    "for c in range(C):\n",
    "    index = 0\n",
    "    t = torch.linspace(0, 1, N)\n",
    "    # When c = 0 and t = 0: start of linspace\n",
    "    # When c = 0 and t = 1: end of linpace\n",
    "    # This inner_var is for the formula inside sin() and cos() like sin(inner_var) and cos(inner_Var)\n",
    "    inner_var = torch.linspace(\n",
    "        # When t = 0\n",
    "        (2 * math.pi / C) * (c),\n",
    "        # When t = 1\n",
    "        (2 * math.pi / C) * (2 + c),\n",
    "        N\n",
    "    ) + torch.randn(N) * 0.2\n",
    "    \n",
    "    for ix in range(N * c, N * (c + 1)):\n",
    "        X[ix] = t[index] * torch.FloatTensor((\n",
    "            math.sin(inner_var[index]), math.cos(inner_var[index])\n",
    "        ))\n",
    "        y[ix] = c\n",
    "        index += 1\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(\"X:\", tuple(X.size()))\n",
    "print(\"y:\", tuple(y.size()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the data\n",
    "plot_data(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "lambda_l2 = 1e-5\n",
    "\n",
    "# nn package to create our linear model\n",
    "# each Linear module has a weight and bias\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(D, H),\n",
    "    nn.Linear(H, C)\n",
    ")\n",
    "\n",
    "# nn package also has different loss functions.\n",
    "# we use cross entropy loss for our classification task\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# we use the optim package to apply\n",
    "# stochastic gradient descent for our parameter updates\n",
    "optimizer = torch.optim.SGD(model.parameters(), \n",
    "                            lr=learning_rate,\n",
    "                            weight_decay=lambda_l2) # built-in L2\n",
    "\n",
    "# Training loop\n",
    "for t in range(1000):\n",
    "    \n",
    "    # Feed forward to get the logits\n",
    "    y_pred = model(X)\n",
    "    \n",
    "    # Compute the loss and accuracy\n",
    "    loss = criterion(y_pred, y)\n",
    "    score, predicted = torch.max(y_pred, 1)\n",
    "    acc = (y == predicted).sum().float() / len(y)\n",
    "    print(\"[EPOCH]: %i, [LOSS]: %.6f, [ACCURACY]: %.3f\" % (t, loss.item(), acc))\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    # zero the gradients before running\n",
    "    # the backward pass.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Backward pass to compute the gradient\n",
    "    # of loss w.r.t our learnable params. \n",
    "    loss.backward()\n",
    "    \n",
    "    # Update params\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot trained model\n",
    "print(model)\n",
    "plot_model(X, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-layered network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "\n",
    "#nn package to create our linear model\n",
    "# each Linear module has a weight and bias\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(D, H),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(H, C)\n",
    ")\n",
    "\n",
    "#nn package also has different loss functions.\n",
    "# we use cross entropy loss for our classification task\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#we use the optim package to apply ADAM for our parameter updates\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                            lr=learning_rate, \n",
    "                            weight_decay=lambda_l2) # built-in L2\n",
    "\n",
    "#training \n",
    "for t in range(1000):\n",
    "    \n",
    "    #1. feedforward\n",
    "    y_pred = model(X)\n",
    "\n",
    "    #2. Compute loss and accuracy\n",
    "    loss = criterion(y_pred, y)\n",
    "    score, predicted = torch.max(y_pred, 1)\n",
    "    accuracy = (y == predicted).sum().float() / len(y)\n",
    "    print(\"[EPOCH]: %i, [LOSS]: %.6f, [ACCURACY]: %.3f\" % (t, loss.item(), accuracy))\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    #3. clean gradients before running backward pass\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #4. back propagation of gradients\n",
    "    loss.backward()\n",
    "\n",
    "    #5. update params\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot trained model\n",
    "print(model)\n",
    "plot_model(X, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpopragation from scratch in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class MLP:\n",
    "    def __init__(\n",
    "        self,\n",
    "        linear_1_in_features,\n",
    "        linear_1_out_features,\n",
    "        f_function,\n",
    "        linear_2_in_features,\n",
    "        linear_2_out_features,\n",
    "        g_function\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            linear_1_in_features: the in features of first linear layer\n",
    "            linear_1_out_features: the out features of first linear layer\n",
    "            linear_2_in_features: the in features of second linear layer\n",
    "            linear_2_out_features: the out features of second linear layer\n",
    "            f_function: string for the f function: relu | sigmoid | identity\n",
    "            g_function: string for the g function: relu | sigmoid | identity\n",
    "        \"\"\"\n",
    "        self.f_function = f_function\n",
    "        self.g_function = g_function\n",
    "\n",
    "        self.parameters = dict(\n",
    "            W1 = torch.randn(linear_1_out_features, linear_1_in_features),\n",
    "            b1 = torch.randn(linear_1_out_features),\n",
    "            W2 = torch.randn(linear_2_out_features, linear_2_in_features),\n",
    "            b2 = torch.randn(linear_2_out_features),\n",
    "        )\n",
    "        self.grads = dict(\n",
    "            dJdW1 = torch.zeros(linear_1_out_features, linear_1_in_features),\n",
    "            dJdb1 = torch.zeros(linear_1_out_features),\n",
    "            dJdW2 = torch.zeros(linear_2_out_features, linear_2_in_features),\n",
    "            dJdb2 = torch.zeros(linear_2_out_features),\n",
    "        )\n",
    "\n",
    "        # put all the cache value you need in self.cache\n",
    "        self.cache = dict()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: tensor shape (batch_size, linear_1_in_features)\n",
    "        \"\"\"\n",
    "        # Done: Implement the forward function\n",
    "        self.cache[\"z0\"] = x\n",
    "        self.cache[\"s1\"] = torch.matmul(self.cache[\"z0\"], self.parameters[\"W1\"].T) \\\n",
    "                        + self.parameters[\"b1\"]\n",
    "\n",
    "        if self.f_function == \"relu\":\n",
    "            self.cache[\"z1\"] = torch.relu(self.cache[\"s1\"])\n",
    "        elif self.f_function == \"sigmoid\":\n",
    "            self.cache[\"z1\"] = torch.sigmoid(self.cache[\"s1\"])\n",
    "        elif self.f_function == \"identity\":\n",
    "            self.cache[\"z1\"] = self.cache[\"s1\"]\n",
    "            \n",
    "        self.cache[\"s2\"] = torch.matmul(self.cache[\"z1\"], self.parameters[\"W2\"].T) \\\n",
    "                        + self.parameters[\"b2\"]\n",
    "        \n",
    "        if self.f_function == \"relu\":\n",
    "            self.cache[\"y_hat\"] = torch.relu(self.cache[\"s2\"])\n",
    "        elif self.f_function == \"sigmoid\":\n",
    "            self.cache[\"y_hat\"] = torch.sigmoid(self.cache[\"s2\"])\n",
    "        elif self.f_function == \"identity\":\n",
    "            self.cache[\"y_hat\"] = self.cache[\"s2\"]    \n",
    "        \n",
    "        return self.cache[\"y_hat\"]\n",
    "        \n",
    "    def sigmoid_backward(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def relu_backward(self, x):\n",
    "        return torch.where(x<=0, torch.zeros(x.shape), torch.ones(x.shape))\n",
    "    \n",
    "    def backward(self, dJdy_hat):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dJdy_hat: The gradient tensor of shape (batch_size, linear_2_out_features)\n",
    "        \"\"\"\n",
    "        # TODO: Implement the backward function\n",
    "        dJdy_hat = dJdy_hat.clone()\n",
    "        \n",
    "        if self.g_function == \"sigmoid\":\n",
    "            self.cache[\"dy_hat_ds2\"] = self.sigmoid_backward(self.cache[\"y_hat\"])\n",
    "        elif self.g_function == \"relu\":\n",
    "            self.cache[\"dy_hat_ds2\"] = self.relu_backward(self.cache[\"s2\"])\n",
    "        elif self.g_function  == \"identity\":\n",
    "            self.cache[\"dy_hat_ds2\"] = torch.ones(self.cache[\"s2\"].shape)\n",
    "        \n",
    "        self.cache[\"dJ_ds2\"] = dJdy_hat * self.cache[\"dy_hat_ds2\"]/dJdy_hat.shape[0]\n",
    "        self.grads[\"dJdW2\"] = torch.matmul(self.cache[\"dJ_ds2\"].T, self.cache[\"z1\"])\n",
    "        self.grads[\"dJdb2\"] = torch.matmul(self.cache[\"dJ_ds2\"].T, \n",
    "                                          torch.ones(dJdy_hat.shape[0]))\n",
    "        \n",
    "        if self.g_function == \"sigmoid\":\n",
    "            self.cache[\"dz1_ds1\"] = self.sigmoid_backward(self.cache[\"z1\"])\n",
    "        elif self.g_function == \"relu\":\n",
    "            self.cache[\"dz1_ds1\"] = self.relu_backward(self.cache[\"s1\"])\n",
    "        elif self.g_function  == \"identity\":\n",
    "            self.cache[\"dz1_ds1\"] = torch.ones(self.cache[\"s1\"].shape)\n",
    "        \n",
    "        self.cache[\"dJ_dz1\"] = torch.matmul(self.cache[\"dJ_ds2\"], self.parameters[\"W2\"])\n",
    "        self.cache[\"dJ_ds1\"] = self.cache[\"dJ_dz1\"] * self.cache[\"dz1_ds1\"]\n",
    "        \n",
    "        self.grads[\"dJdW1\"] = torch.matmul(self.cache[\"dJ_ds1\"].T, self.cache[\"z0\"])\n",
    "        self.grads[\"dJdb1\"] = torch.matmul(self.cache[\"dJ_ds1\"].T, \n",
    "                                           torch.ones(self.cache[\"dJ_ds1\"].shape[0]))\n",
    "        \n",
    "    def clear_grad_and_cache(self):\n",
    "        for grad in self.grads:\n",
    "            self.grads[grad].zero_()\n",
    "        self.cache = dict()\n",
    "\n",
    "def mse_loss(y, y_hat):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        y: the label tensor (batch_size, linear_2_out_features)\n",
    "        y_hat: the prediction tensor (batch_size, linear_2_out_features)\n",
    "\n",
    "    Return:\n",
    "        J: scalar of loss\n",
    "        dJdy_hat: The gradient tensor of shape (batch_size, linear_2_out_features)\n",
    "    \"\"\"\n",
    "    # TODO: Implement the mse loss\n",
    "    dJdy_hat = y - y_hat\n",
    "    squaredDelta = dJdy_hat ** 2\n",
    "    J = squaredDelta.mean() \n",
    "    \n",
    "    # return loss, dJdy_hat\n",
    "    return J, dJdy_hat\n",
    "\n",
    "def bce_loss(y, y_hat):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        y_hat: the prediction tensor\n",
    "        y: the label tensor\n",
    "        \n",
    "    Return:\n",
    "        loss: scalar of loss\n",
    "        dJdy_hat: The gradient tensor of shape (batch_size, linear_2_out_features)\n",
    "    \"\"\"\n",
    "    # TODO: Implement the bce loss\n",
    "    J = (-(y * torch.log(y_hat) + (1-y) * torch.log(1-y_hat))).mean()\n",
    "    dJdy_hat = -y/y_hat + (1-y)/1-y_hat\n",
    "    \n",
    "    # return loss, dJdy_hat\n",
    "    return J, dJdy_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mlp import MLP, mse_loss, bce_loss\n",
    "\n",
    "net = MLP(\n",
    "    linear_1_in_features=2,\n",
    "    linear_1_out_features=20,\n",
    "    f_function='relu',\n",
    "    linear_2_in_features=20,\n",
    "    linear_2_out_features=5,\n",
    "    g_function='identity'\n",
    ")\n",
    "x = torch.randn(10, 2)\n",
    "y = torch.randn(10, 5)\n",
    "\n",
    "net.clear_grad_and_cache()\n",
    "y_hat = net.forward(x)\n",
    "J, dJdy_hat = mse_loss(y, y_hat)\n",
    "net.backward(dJdy_hat)\n",
    "\n",
    "#------------------------------------------------\n",
    "# compare the result with autograd\n",
    "net_autograd = nn.Sequential(\n",
    "    OrderedDict([\n",
    "        ('linear1', nn.Linear(2, 20)),\n",
    "        ('relu', nn.ReLU()),\n",
    "        ('linear2', nn.Linear(20, 5)),\n",
    "    ])\n",
    ")\n",
    "net_autograd.linear1.weight.data = net.parameters['W1']\n",
    "net_autograd.linear1.bias.data = net.parameters['b1']\n",
    "net_autograd.linear2.weight.data = net.parameters['W2']\n",
    "net_autograd.linear2.bias.data = net.parameters['b2']\n",
    "\n",
    "y_hat_autograd = net_autograd(x)\n",
    "\n",
    "J_autograd = F.mse_loss(y_hat_autograd, y)\n",
    "\n",
    "net_autograd.zero_grad()\n",
    "J_autograd.backward()\n",
    "\n",
    "print((net_autograd.linear1.weight.grad.data - net.grads['dJdW1']).norm() < 1e-3)\n",
    "print((net_autograd.linear1.bias.grad.data - net.grads['dJdb1']).norm() < 1e-3)\n",
    "print((net_autograd.linear2.weight.grad.data - net.grads['dJdW2']).norm() < 1e-3)\n",
    "print((net_autograd.linear2.bias.grad.data - net.grads['dJdb2']).norm()< 1e-3)\n",
    "#-----------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "7b6db0c1d442fe597d9b481cd2ea939a45b3fa778adc3bd0e8ea6ed6edc9a97e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
