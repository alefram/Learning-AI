{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent neural networks\n",
    "\n",
    "This is a example of how to use Recurrent Neural networks for clasify sequence(many to one) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import six\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usefull methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================================================\n",
    "def pad_sequences(sequences, maxlen=None, dtype='int32',\n",
    "                  padding='pre', truncating='pre', value=0.):\n",
    "    if not hasattr(sequences, '__len__'):\n",
    "        raise ValueError('`sequences` must be iterable.')\n",
    "    lengths = []\n",
    "    for x in sequences:\n",
    "        if not hasattr(x, '__len__'):\n",
    "            raise ValueError('`sequences` must be a list of iterables. '\n",
    "                             'Found non-iterable: ' + str(x))\n",
    "        lengths.append(len(x))\n",
    "\n",
    "    num_samples = len(sequences)\n",
    "    if maxlen is None:\n",
    "        maxlen = np.max(lengths)\n",
    "\n",
    "    # take the sample shape from the first non empty sequence\n",
    "    # checking for consistency in the main loop below.\n",
    "    sample_shape = tuple()\n",
    "    for s in sequences:\n",
    "        if len(s) > 0:\n",
    "            sample_shape = np.asarray(s).shape[1:]\n",
    "            break\n",
    "\n",
    "    is_dtype_str = np.issubdtype(dtype, np.str_) or np.issubdtype(dtype, np.unicode_)\n",
    "    if isinstance(value, six.string_types) and dtype != object and not is_dtype_str:\n",
    "        raise ValueError(\"`dtype` {} is not compatible with `value`'s type: {}\\n\"\n",
    "                         \"You should set `dtype=object` for variable length strings.\"\n",
    "                         .format(dtype, type(value)))\n",
    "\n",
    "    x = np.full((num_samples, maxlen) + sample_shape, value, dtype=dtype)\n",
    "    for idx, s in enumerate(sequences):\n",
    "        if not len(s):\n",
    "            continue  # empty list/array was found\n",
    "        if truncating == 'pre':\n",
    "            trunc = s[-maxlen:]\n",
    "        elif truncating == 'post':\n",
    "            trunc = s[:maxlen]\n",
    "        else:\n",
    "            raise ValueError('Truncating type \"%s\" '\n",
    "                             'not understood' % truncating)\n",
    "\n",
    "        # check `trunc` has expected shape\n",
    "        trunc = np.asarray(trunc, dtype=dtype)\n",
    "        if trunc.shape[1:] != sample_shape:\n",
    "            raise ValueError('Shape of sample %s of sequence at position %s '\n",
    "                             'is different from expected shape %s' %\n",
    "                             (trunc.shape[1:], idx, sample_shape))\n",
    "\n",
    "        if padding == 'post':\n",
    "            x[idx, :len(trunc)] = trunc\n",
    "        elif padding == 'pre':\n",
    "            x[idx, -len(trunc):] = trunc\n",
    "        else:\n",
    "            raise ValueError('Padding type \"%s\" not understood' % padding)\n",
    "    return x\n",
    "\n",
    "#===============================================================================\n",
    "\n",
    "def to_categorical(y, num_classes=None, dtype='float32'):\n",
    "    y = np.array(y, dtype='int')\n",
    "    input_shape = y.shape\n",
    "    if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:\n",
    "        input_shape = tuple(input_shape[:-1])\n",
    "    y = y.ravel()\n",
    "    if not num_classes:\n",
    "        num_classes = np.max(y) + 1\n",
    "    n = y.shape[0]\n",
    "    categorical = np.zeros((n, num_classes), dtype=dtype)\n",
    "    categorical[np.arange(n), y] = 1\n",
    "    output_shape = input_shape + (num_classes,)\n",
    "    categorical = np.reshape(categorical, output_shape)\n",
    "    return categorical\n",
    "\n",
    "#===============================================================================\n",
    "\n",
    "def test(model, test_data_gen, criterion, device):\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Store the number of sequences that were calssified correctly\n",
    "    num_correct = 0\n",
    "\n",
    "    # a context manager is used to disable gradient calculations during inference\n",
    "    # to reduce memory usage, as we typically dont need the gradients at this point\n",
    "    with torch.no_grad():\n",
    "        for batch_idx in range(len(test_data_gen)):\n",
    "            data, target = test_data_gen[batch_idx]\n",
    "            data, target = torch.from_numpy(data).float().to(device), torch.from_numpy(target).long().to(device)\n",
    "\n",
    "            output = model(data)\n",
    "            output = output[:, -1, :]\n",
    "\n",
    "            target = target.argmax(dim=1)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            y_pred = output.argmax(dim=1)\n",
    "            num_correct += (y_pred == target).sum().item()\n",
    "\n",
    "    return num_correct, loss.item()\n",
    "\n",
    "#===============================================================================\n",
    "\n",
    "def train(model, train_data_gen, criterion, optimizer, device):\n",
    "    # set the model to training mode. It turnn on layers that would otherwise\n",
    "    # behave differently during evaluation, such as dropout\n",
    "    model.train()\n",
    "\n",
    "    # Store the number of sequence that were calssified correctly\n",
    "    num_correct = 0\n",
    "\n",
    "    # Iterate over every batch of sequences. the length of a data generator \n",
    "    # is defined as the number of batches requires to produce a total of 1000\n",
    "    # sequences given a batch_size\n",
    "    for batch_idx in range(len(train_data_gen)):\n",
    "\n",
    "        # Request a batch of sequences and class labels\n",
    "        data, target = train_data_gen[batch_idx]\n",
    "        # convert them into tensors of the correct type, and the then send them \n",
    "        # to the appropiate device\n",
    "        data, target = torch.from_numpy(data).float().to(device), \\\n",
    "                        torch.from_numpy(target).long().to(device)\n",
    "\n",
    "        # do forward\n",
    "        output = model(data) # step 1\n",
    "\n",
    "        # Pick only the output corresponding to last sequence element\n",
    "        output = output[:, -1, :]\n",
    "\n",
    "        # Compute de the loss function for this batch. For CrossEntroyLoss,\n",
    "        # the second argument most be a tensor of class indices instead of one-hot\n",
    "        #enconded class labels. One approach is to take advantage of the one-hot\n",
    "        # encoding of the target and call argmax along its second dimension to \n",
    "        # create a tensor of shape (batch_size) containing the index of the class\n",
    "        # label that was hot for each sequence.\n",
    "        target = target.argmax(dim=1)\n",
    "\n",
    "        loss = criterion(output, target) # step 2 do loss function\n",
    "\n",
    "        # Clear the gradient buffer of the optimized parameters.\n",
    "        # it will prevent accumulated gradients from previous batch.\n",
    "        optimizer.zero_grad() # step 3\n",
    "\n",
    "        loss.backward() # step 4 backpropagation\n",
    "\n",
    "        optimizer.step() # step 5 gradiend descent\n",
    "\n",
    "        y_pred = output.argmax(dim=1)\n",
    "        num_correct += (y_pred == target).sum().item()\n",
    "\n",
    "    return num_correct,  loss.item()\n",
    "\n",
    "#===============================================================================\n",
    "\n",
    "def train_and_test(model, train_data_gen, test_data_gen, criterion, optimizer, max_epochs, verbose=True):\n",
    "    #check device\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    #move model\n",
    "    model.to(device)\n",
    "\n",
    "    #track loss and model accuracy across epochs\n",
    "    history_train = {'loss': [], 'accuracy': []}\n",
    "    history_test = {'loss': [],  'accuracy': []}\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        # Run training loop and get accuracy\n",
    "        num_correct, loss = train(model, train_data_gen, criterion, optimizer, device)\n",
    "        accuracy = float(num_correct) / (len(train_data_gen) * train_data_gen.batch_size) * 100\n",
    "        history_train['loss'].append(loss)\n",
    "        history_train['accuracy'].append(accuracy)\n",
    "\n",
    "        # Do the same for the testing loop\n",
    "        num_correct, loss = test(model, test_data_gen, criterion, device)\n",
    "        accuracy = float(num_correct) / (len(test_data_gen) * test_data_gen.batch_size) * 100\n",
    "        history_test['loss'].append(loss)\n",
    "        history_test['accuracy'].append(accuracy)\n",
    "\n",
    "\n",
    "        if verbose or epoch + 1 == max_epochs:\n",
    "            print(f'[Epoch {epoch + 1}/{max_epochs}]'\n",
    "                    f\" loss: {history_train['loss'][-1]:.4f}, accuracy: {history_train['accuracy'][-1]:2.2f}%\" \n",
    "                    f\" - test_loss: {history_test['loss'][-1]:.4f}, test_acc: {history_test['accuracy'][-1]:2.2f}%\")\n",
    "        \n",
    "    # Generate diagnostic plots for the loss and accuracy\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=(9, 4.5))\n",
    "    for ax, metric in zip(axes, ['loss', 'accuracy']):\n",
    "        ax.plot(history_train[metric])\n",
    "        ax.plot(history_test[metric])\n",
    "        ax.set_xlabel('epoch', fontsize=12)\n",
    "        ax.set_ylabel(metric, fontsize=12)\n",
    "        ax.legend(['Train', 'Test'], loc='best')\n",
    "    plt.show()    \n",
    "\n",
    "    return model\n",
    "\n",
    "#===============================================================================\n",
    "\n",
    "\n",
    "def evaluate_model(model, difficulty, seed=9001, verbose=False):\n",
    "    # Define a dictionary that maps class indices to labels\n",
    "    class_idx_to_label = {0: 'Q', 1: 'R', 2: 'S', 3: 'U'}\n",
    "\n",
    "    # Create a new data generator\n",
    "    data_generator = QRSU.get_predefined_generator(difficulty, seed=seed)\n",
    "\n",
    "    # Track the number of times a class appears\n",
    "    count_classes = collections.Counter()\n",
    "\n",
    "    # Keep correctly classified and misclassified sequences, and their\n",
    "    # true and predicted class labels, for diagnostic information.\n",
    "    correct = []\n",
    "    incorrect = []\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx in range(len(data_generator)):\n",
    "            data, target = test_data_gen[batch_idx]\n",
    "            data, target = torch.from_numpy(data).float().to(device), torch.from_numpy(target).long().to(device)\n",
    "\n",
    "            data_decoded = data_generator.decode_x_batch(data.cpu().numpy())\n",
    "            target_decoded = data_generator.decode_y_batch(target.cpu().numpy())\n",
    "\n",
    "            output = model(data)\n",
    "            output = output[:, -1, :]\n",
    "\n",
    "            target = target.argmax(dim=1)\n",
    "            y_pred = output.argmax(dim=1)\n",
    "            y_pred_decoded = [class_idx_to_label[y.item()] for y in y_pred]\n",
    "\n",
    "            count_classes.update(target_decoded)\n",
    "            for i, (truth, prediction) in enumerate(zip(target_decoded, y_pred_decoded)):\n",
    "                if truth == prediction:\n",
    "                    correct.append((data_decoded[i], truth, prediction))\n",
    "                else:\n",
    "                    incorrect.append((data_decoded[i], truth, prediction))\n",
    "\n",
    "    num_sequences = sum(count_classes.values())\n",
    "    accuracy = float(len(correct)) / num_sequences * 100\n",
    "    print(f'The accuracy of the model is measured to be {accuracy:.2f}%.\\n')\n",
    "\n",
    "    # Report the accuracy by class\n",
    "    for label in sorted(count_classes):\n",
    "        num_correct = sum(1 for _, truth, _ in correct if truth == label)\n",
    "        print(f'{label}: {num_correct} / {count_classes[label]} correct')\n",
    "\n",
    "    # Report some random sequences for examination\n",
    "    print('\\nHere are some example sequences:')\n",
    "    for i in range(10):\n",
    "        sequence, truth, prediction = correct[random.randrange(0, 10)]\n",
    "        print(f'{sequence} -> {truth} was labelled {prediction}')\n",
    "\n",
    "    # Report misclassified sequences for investigation\n",
    "    if incorrect and verbose:\n",
    "        print('\\nThe following sequences were misclassified:')\n",
    "        for sequence, truth, prediction in incorrect:\n",
    "            print(f'{sequence} -> {truth} was labelled {prediction}')\n",
    "    else:\n",
    "        print('\\nThere were no misclassified sequences.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalOrderExp6aSequence():\n",
    "    \"\"\"\n",
    "    From Hochreiter&Schmidhuber(1997):\n",
    "        The goal is to classify sequences. Elements and targets are represented locally\n",
    "        (input vectors with only one non-zero bit). The sequence starts with an B, ends\n",
    "        with a E (the \"trigger symbol\") and otherwise consists of randomly chosen symbols\n",
    "        from the set {a, b, c, d} except for two elements at positions t1 and t2 that are\n",
    "        either X or Y . The sequence length is randomly chosen between 100 and 110, t1 is\n",
    "        randomly chosen between 10 and 20, and t2 is randomly chosen between 50 and 60.\n",
    "        There are 4 sequence classes Q, R, S, U which depend on the temporal order of X and Y.\n",
    "        The rules are:\n",
    "            X, X -> Q,\n",
    "            X, Y -> R,\n",
    "            Y , X -> S,\n",
    "            Y , Y -> U.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, length_range=(100, 111), t1_range=(10, 21), t2_range=(50, 61),\n",
    "                 batch_size=32, seed=None):\n",
    "\n",
    "        self.classes = ['Q', 'R', 'S', 'U']\n",
    "        self.n_classes = len(self.classes)\n",
    "\n",
    "        self.relevant_symbols = ['X', 'Y']\n",
    "        self.distraction_symbols = ['a', 'b', 'c', 'd']\n",
    "        self.start_symbol = 'B'\n",
    "        self.end_symbol = 'E'\n",
    "\n",
    "        self.length_range = length_range\n",
    "        self.t1_range = t1_range\n",
    "        self.t2_range = t2_range\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        all_symbols = self.relevant_symbols + self.distraction_symbols + \\\n",
    "                      [self.start_symbol] + [self.end_symbol]\n",
    "        self.n_symbols = len(all_symbols)\n",
    "        self.s_to_idx = {s: n for n, s in enumerate(all_symbols)}\n",
    "        self.idx_to_s = {n: s for n, s in enumerate(all_symbols)}\n",
    "\n",
    "        self.c_to_idx = {c: n for n, c in enumerate(self.classes)}\n",
    "        self.idx_to_c = {n: c for n, c in enumerate(self.classes)}\n",
    "\n",
    "    def generate_pair(self):\n",
    "        length = np.random.randint(self.length_range[0], self.length_range[1])\n",
    "        t1 = np.random.randint(self.t1_range[0], self.t1_range[1])\n",
    "        t2 = np.random.randint(self.t2_range[0], self.t2_range[1])\n",
    "\n",
    "        x = np.random.choice(self.distraction_symbols, length)\n",
    "        x[0] = self.start_symbol\n",
    "        x[-1] = self.end_symbol\n",
    "\n",
    "        y = np.random.choice(self.classes)\n",
    "        if y == 'Q':\n",
    "            x[t1], x[t2] = self.relevant_symbols[0], self.relevant_symbols[0]\n",
    "        elif y == 'R':\n",
    "            x[t1], x[t2] = self.relevant_symbols[0], self.relevant_symbols[1]\n",
    "        elif y == 'S':\n",
    "            x[t1], x[t2] = self.relevant_symbols[1], self.relevant_symbols[0]\n",
    "        else:\n",
    "            x[t1], x[t2] = self.relevant_symbols[1], self.relevant_symbols[1]\n",
    "\n",
    "        return ''.join(x), y\n",
    "\n",
    "    # encoding/decoding single instance version\n",
    "\n",
    "    def encode_x(self, x):\n",
    "        idx_x = [self.s_to_idx[s] for s in x]\n",
    "        return to_categorical(idx_x, num_classes=self.n_symbols)\n",
    "\n",
    "    def encode_y(self, y):\n",
    "        idx_y = self.c_to_idx[y]\n",
    "        return to_categorical(idx_y, num_classes=self.n_classes)\n",
    "\n",
    "    def decode_x(self, x):\n",
    "        x = x[np.sum(x, axis=1) > 0]    # remove padding\n",
    "        return ''.join([self.idx_to_s[pos] for pos in np.argmax(x, axis=1)])\n",
    "\n",
    "    def decode_y(self, y):\n",
    "        return self.idx_to_c[np.argmax(y)]\n",
    "\n",
    "    # encoding/decoding batch versions\n",
    "\n",
    "    def encode_x_batch(self, x_batch):\n",
    "        return pad_sequences([self.encode_x(x) for x in x_batch],\n",
    "                             maxlen=self.length_range[1])\n",
    "\n",
    "    def encode_y_batch(self, y_batch):\n",
    "        return np.array([self.encode_y(y) for y in y_batch])\n",
    "\n",
    "    def decode_x_batch(self, x_batch):\n",
    "        return [self.decode_x(x) for x in x_batch]\n",
    "\n",
    "    def decode_y_batch(self, y_batch):\n",
    "        return [self.idx_to_c[pos] for pos in np.argmax(y_batch, axis=1)]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Let's assume 1000 sequences as the size of data. \"\"\"\n",
    "        return int(1000. / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_x, batch_y = [], []\n",
    "        for _ in range(self.batch_size):\n",
    "            x, y = self.generate_pair()\n",
    "            batch_x.append(x)\n",
    "            batch_y.append(y)\n",
    "        return self.encode_x_batch(batch_x), self.encode_y_batch(batch_y)\n",
    "\n",
    "    class DifficultyLevel:\n",
    "        \"\"\" On HARD, settings are identical to the original settings from the '97 paper.\"\"\"\n",
    "        EASY, NORMAL, MODERATE, HARD, NIGHTMARE = range(5)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_predefined_generator(difficulty_level, batch_size=32, seed=8382):\n",
    "        EASY = TemporalOrderExp6aSequence.DifficultyLevel.EASY\n",
    "        NORMAL = TemporalOrderExp6aSequence.DifficultyLevel.NORMAL\n",
    "        MODERATE = TemporalOrderExp6aSequence.DifficultyLevel.MODERATE\n",
    "        HARD = TemporalOrderExp6aSequence.DifficultyLevel.HARD\n",
    "\n",
    "        if difficulty_level == EASY:\n",
    "            length_range = (7, 9)\n",
    "            t1_range = (1, 3)\n",
    "            t2_range = (4, 6)\n",
    "        elif difficulty_level == NORMAL:\n",
    "            length_range = (30, 41)\n",
    "            t1_range = (2, 6)\n",
    "            t2_range = (20, 28)\n",
    "        elif difficulty_level == MODERATE:\n",
    "            length_range = (60, 81)\n",
    "            t1_range = (10, 21)\n",
    "            t2_range = (45, 55)\n",
    "        elif difficulty_level == HARD:\n",
    "            length_range = (100, 111)\n",
    "            t1_range = (10, 21)\n",
    "            t2_range = (50, 61)\n",
    "        else:\n",
    "            length_range = (300, 501)\n",
    "            t1_range = (10, 81)\n",
    "            t2_range = (250, 291)\n",
    "        return TemporalOrderExp6aSequence(length_range, t1_range, t2_range,\n",
    "                                          batch_size, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data generator\n",
    "\n",
    "QRSU = TemporalOrderExp6aSequence()\n",
    "\n",
    "example_generator = QRSU.get_predefined_generator(\n",
    "    difficulty_level=QRSU.DifficultyLevel.EASY,\n",
    "    batch_size=32,\n",
    ")\n",
    "\n",
    "example_batch = example_generator[1]\n",
    "print(f'The return type is a {type(example_batch)} with length {len(example_batch)}.')\n",
    "print(f'The first item in the tuple is the batch of sequences with shape {example_batch[0].shape}.')\n",
    "print(f'The first element in the batch of sequences is:\\n {example_batch[0][0, :, :]}')\n",
    "print(f'The second item in the tuple is the corresponding batch of class labels with shape {example_batch[1].shape}.')\n",
    "print(f'The first element in the batch of class labels is:\\n {example_batch[1][0, :]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding the first sequence\n",
    "sequence_decoded = example_generator.decode_x(example_batch[0][0, :, :])\n",
    "print(f'The sequence is: {sequence_decoded}')\n",
    "\n",
    "# Decoding the class label of the first sequence\n",
    "class_label_decoded = example_generator.decode_y(example_batch[1][0])\n",
    "print(f'The class label is: {class_label_decoded}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the random seed for reproducible products\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# this is a simple RNN\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__() # this call the base class constructor\n",
    "        # NN layers assigned as attributes of a module subclass\n",
    "        # have their parameters registered for training automatically.\n",
    "        self.rnn = torch.nn.RNN(input_size, hidden_size, nonlinearity='relu', batch_first=True)\n",
    "        self.linear = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The RNN returns its hidden state but here not use while the RNN can\n",
    "        # also take a hidden state as input, the RNN gets passed a hidden \n",
    "        # state initialized zeros by default\n",
    "        h = self.rnn(x)[0]\n",
    "        x = self.linear(h)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# this is simple LSTM\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.lstm = torch.nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.linear = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.lstm(x)[0]\n",
    "        x = self.linear(h)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_states_across_time(self, x):\n",
    "        h_c = None\n",
    "        h_list, c_list = list(), list()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for t in range(x.size(1)):\n",
    "                h_c = self.lstm(x[:, [t], :], h_c)[1]\n",
    "                h_list.append(h_c[0])\n",
    "                c_list.append(h_c[1])\n",
    "            h = torch.cat(h_list)\n",
    "            c = torch.cat(c_list)\n",
    "\n",
    "            return h, c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup data generator\n",
    "difficulty = QRSU.DifficultyLevel.EASY\n",
    "batch_size = 32\n",
    "train_data_gen = QRSU.get_predefined_generator(difficulty, batch_size)\n",
    "test_data_gen = QRSU.get_predefined_generator(difficulty, batch_size)\n",
    "\n",
    "# Setup RNN and training\n",
    "input_size = train_data_gen.n_symbols\n",
    "hidden_size = 4\n",
    "output_size = train_data_gen.n_classes\n",
    "model = RNN(input_size, hidden_size, output_size)\n",
    "criterion = torch.nn.CrossEntropyLoss() # loss function\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
    "max_epochs = 10\n",
    "\n",
    "# train \n",
    "model = train_and_test(model, train_data_gen, test_data_gen, criterion, optimizer, max_epochs)\n",
    "\n",
    "for parameter_group in list(model.parameters()):\n",
    "    print(parameter_group.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test in more epoch\n",
    "\n",
    "#setup data generator\n",
    "difficulty = QRSU.DifficultyLevel.EASY\n",
    "batch_size = 32\n",
    "train_data_gen = QRSU.get_predefined_generator(difficulty, batch_size)\n",
    "test_data_gen = QRSU.get_predefined_generator(difficulty, batch_size)\n",
    "\n",
    "# Setup RNN and training\n",
    "input_size = train_data_gen.n_symbols\n",
    "hidden_size = 4\n",
    "output_size = train_data_gen.n_classes\n",
    "model = RNN(input_size, hidden_size, output_size)\n",
    "criterion = torch.nn.CrossEntropyLoss() # loss function\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
    "max_epochs = 100\n",
    "\n",
    "# train \n",
    "model = train_and_test(model, train_data_gen, test_data_gen, criterion, optimizer, max_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup data generator\n",
    "difficulty = QRSU.DifficultyLevel.EASY\n",
    "batch_size = 32\n",
    "train_data_gen = QRSU.get_predefined_generator(difficulty, batch_size)\n",
    "test_data_gen = QRSU.get_predefined_generator(difficulty, batch_size)\n",
    "\n",
    "# Setup RNN and training\n",
    "input_size = train_data_gen.n_symbols\n",
    "hidden_size = 4\n",
    "output_size = train_data_gen.n_classes\n",
    "model = LSTM(input_size, hidden_size, output_size)\n",
    "criterion = torch.nn.CrossEntropyLoss() # loss function\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
    "max_epochs = 10\n",
    "\n",
    "# train \n",
    "model = train_and_test(model, train_data_gen, test_data_gen, criterion, optimizer, max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test in more epoch\n",
    "\n",
    "#setup data generator\n",
    "difficulty = QRSU.DifficultyLevel.EASY\n",
    "batch_size = 32\n",
    "train_data_gen = QRSU.get_predefined_generator(difficulty, batch_size)\n",
    "test_data_gen = QRSU.get_predefined_generator(difficulty, batch_size)\n",
    "\n",
    "# Setup RNN and training\n",
    "input_size = train_data_gen.n_symbols\n",
    "hidden_size = 4\n",
    "output_size = train_data_gen.n_classes\n",
    "model = LSTM(input_size, hidden_size, output_size)\n",
    "criterion = torch.nn.CrossEntropyLoss() # loss function\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
    "max_epochs = 100\n",
    "\n",
    "# train \n",
    "model = train_and_test(model, train_data_gen, test_data_gen, criterion, optimizer, max_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, difficulty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Setup the training and test data generators\n",
    "difficulty     = QRSU.DifficultyLevel.MODERATE\n",
    "batch_size     = 32\n",
    "train_data_gen = QRSU.get_predefined_generator(difficulty, batch_size)\n",
    "test_data_gen  = QRSU.get_predefined_generator(difficulty, batch_size)\n",
    "\n",
    "# Setup the RNN and training settings\n",
    "input_size  = train_data_gen.n_symbols\n",
    "hidden_size = 12\n",
    "output_size = train_data_gen.n_classes\n",
    "model       = LSTM(input_size, hidden_size, output_size)\n",
    "criterion   = torch.nn.CrossEntropyLoss()\n",
    "optimizer   = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
    "max_epochs  = 100\n",
    "\n",
    "# Train the model\n",
    "model = train_and_test(model, train_data_gen, test_data_gen, criterion, optimizer, max_epochs, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get hidden (H) and cell (C) batch state given a batch input (X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    data = test_data_gen[0][0]\n",
    "    X = torch.from_numpy(data).float().to(device)\n",
    "    H_t, C_t = model.get_states_across_time(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cstr(s, color='black'):\n",
    "    if s == ' ':\n",
    "        return f'<text style=color:#000;padding-left:10px;background-color:{color}> </text>'\n",
    "    else:\n",
    "        return f'<text style=color:#000;background-color:{color}>{s} </text>'\n",
    "\n",
    "def _print_color(t):\n",
    "    display(HTML(''.join([_cstr(ti, color=ci) for ti, ci in t])))\n",
    "\n",
    "def _get_clr(value):\n",
    "    colors = ('#85c2e1', '#89c4e2', '#95cae5', '#99cce6', '#a1d0e8',\n",
    "              '#b2d9ec', '#baddee', '#c2e1f0', '#eff7fb', '#f9e8e8',\n",
    "              '#f9e8e8', '#f9d4d4', '#f9bdbd', '#f8a8a8', '#f68f8f',\n",
    "              '#f47676', '#f45f5f', '#f34343', '#f33b3b', '#f42e2e')\n",
    "    value = int((value * 100) / 5)\n",
    "    if value == len(colors): value -= 1  # fixing bugs...\n",
    "    return colors[value]\n",
    "\n",
    "\n",
    "def print_colourbar():\n",
    "    color_range = torch.linspace(-2.5, 2.5, 20)\n",
    "    to_print = [(f'{x:.2f}', _get_clr((x+2.5)/5)) for x in color_range]\n",
    "    _print_color(to_print)\n",
    "\n",
    "print(\"Color range is as follows:\")\n",
    "print_colourbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _visualise_values(output_values, result_list):\n",
    "    text_colours = []\n",
    "    for i in range(len(output_values)):\n",
    "        text = (result_list[i], _get_clr(output_values[i]))\n",
    "        text_colours.append(text)\n",
    "    _print_color(text_colours)\n",
    "    \n",
    "def plot_state(data, state, b, decoder):\n",
    "    actual_data = decoder(data[b, :, :].numpy())\n",
    "    seq_len = len(actual_data)\n",
    "    seq_len_w_pad = len(state)\n",
    "    for s in range(state.size(2)):\n",
    "        states = torch.sigmoid(state[:, b, s])\n",
    "        _visualise_values(states[seq_len_w_pad - seq_len:], list(actual_data))\n",
    "\n",
    "plot_state(X.cpu(), C_t, b=9, decoder=test_data_gen.decode_x)  # 3, 6, 9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_state(X.cpu(), H_t, b=9, decoder=test_data_gen.decode_x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "7b6db0c1d442fe597d9b481cd2ea939a45b3fa778adc3bd0e8ea6ed6edc9a97e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
