{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNPB1ZfIPqCc"
   },
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCtlKsE8PqCg"
   },
   "source": [
    "the most important stuff is :\n",
    "* Compositionality : many layers\n",
    "* Convolutions: locality + stationarity of images\n",
    "* Pooling: invariance of object class to translations and reduce data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ILhmkEGnPqCi"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def get_n_params(model):\n",
    "    np = 0\n",
    "    for p in list(model.parameters()):\n",
    "        np  += p.nelement()\n",
    "    return np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "diAng-iQPqCl"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Usefull methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list = []\n",
    "\n",
    "#train loop\n",
    "def train(epoch, model, perm=torch.arange(0, 784).long()):\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        #send to gpu\n",
    "        data, target = data.to(device),  target.to(device)\n",
    "\n",
    "        #permute pixels\n",
    "        data = data.view(-1, 28*28)\n",
    "        data = data[:, perm]\n",
    "        data = data.view(-1, 1, 28, 28)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%]\\tLoss: {:.6f}'.format(\n",
    "              epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "              100 * batch_idx / len(train_loader), loss.item()\n",
    "            ))\n",
    "            \n",
    "#test model \n",
    "def test(model, perm=torch.arange(0, 784).long()):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    for data, target in test_loader:\n",
    "        #send to devide\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        #permute pixels\n",
    "        data = data.view(-1, 28*28)\n",
    "        data = data[:, perm]\n",
    "        data = data.view(-1, 1, 28, 28)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss                                                               \n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-prob\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100 * correct / len(test_loader.dataset)\n",
    "    accuracy_list.append(accuracy)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "      test_loss, correct, len(test_loader.dataset),\n",
    "      accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xdgCKgzHPqCl"
   },
   "outputs": [],
   "source": [
    "# LOAD MNIST\n",
    "input_size = 28*28 # images are 28x28 pixels\n",
    "output_size = 10 # 10 clases\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081, ))\n",
    "    ])),\n",
    "    batch_size=64, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])),\n",
    "    batch_size=1000, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "5v4J8TThTnAk",
    "outputId": "58c207d4-4c22-4bae-b346-6663d65f5025"
   },
   "outputs": [],
   "source": [
    "# SHOW IMAGES\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    image, _ = train_loader.dataset.__getitem__(i)\n",
    "    plt.imshow(image.squeeze().numpy())\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1IOdyzLUZAZ",
    "tags": []
   },
   "source": [
    " ## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S64Brj-sUKVk"
   },
   "outputs": [],
   "source": [
    "class FC2Layer(nn.Module):\n",
    "    def __init__(self, input_size, n_hidden, output_size):\n",
    "    super(FC2Layer, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.network = nn.Sequential(\n",
    "        nn.Linear(input_size, n_hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(n_hidden, n_hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(n_hidden, output_size),\n",
    "        nn.LogSoftmax(dim=1)\n",
    "    )\n",
    "\n",
    "      def forward(self, x):\n",
    "        x = x.view(-1, self.input_size)\n",
    "        return self.network(x)\n",
    "\n",
    "class CNN(nn.Module):\n",
    "      def __init__(self, input_size, n_feature, output_size, kernel_size=5):\n",
    "        super(CNN, self).__init__()\n",
    "        self.n_feature = n_feature\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=n_feature, kernel_size=kernel_size)\n",
    "        self.conv2 = nn.Conv2d(in_channels=n_feature, out_channels=n_feature, kernel_size=kernel_size)\n",
    "        self.fc1 = nn.Linear(n_feature * 4 * 4, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "          def forward(self, x, verbose=False):\n",
    "            c1 = self.conv1(x)\n",
    "            s1 = F.relu(c1)\n",
    "            p1 = F.max_pool2d(s1, kernel_size=2)\n",
    "            c2 = self.conv2(p1)\n",
    "            s2 = F.relu(c2)\n",
    "            p2 = F.max_pool2d(s2, kernel_size=2)\n",
    "            z1 = p2.view(-1, self.n_feature*4*4)\n",
    "            z2 = self.fc1(z1)\n",
    "            s3 = F.relu(z2)\n",
    "            z3 = self.fc2(s3)\n",
    "            loss = F.log_softmax(z3,  dim=1)\n",
    "\n",
    "            return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mK9HgFXYoUq",
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kxdaWyAbcPK3",
    "outputId": "57c8a9b7-3f49-4d6b-fd7c-1f8da507ddf7"
   },
   "outputs": [],
   "source": [
    " # TRAINING FULL-CONNECTED NETWORK\n",
    "\n",
    "n_hidden = 8 \n",
    "\n",
    "model_fnn = FC2Layer(input_size, n_hidden, output_size)\n",
    "model_fnn.to(device)\n",
    "optimizer = optim.SGD(model_fnn.parameters(), lr=0.01, momentum=0.5)\n",
    "print('Number of parameters: {}'.format(get_n_params(model_fnn)))\n",
    "\n",
    "for epoch in range(0,1):\n",
    "    train(epoch, model_fnn)\n",
    "    test(model_fnn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wSIpQh9ec_6p",
    "outputId": "51cbafe6-d558-427e-a63e-ebb175de8a69"
   },
   "outputs": [],
   "source": [
    "# TRAIN A CONVNET WITH THE SAME PARAMETERS\n",
    "\n",
    "n_feature = 6 # number of feature maps\n",
    "\n",
    "model_cnn = CNN(input_size, n_feature, output_size)\n",
    "model_cnn.to(device)\n",
    "optimizer = optim.SGD(model_cnn.parameters(), lr=0.01, momentum=0.5)\n",
    "print('Number of parameters: {}'.format(get_n_params(model_cnn)))\n",
    "\n",
    "for epoch in range(0, 1):\n",
    "    train(epoch, model_cnn)\n",
    "    test(model_cnn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 683
    },
    "id": "sWlRi4uGixdX",
    "outputId": "ec5ebecd-5ec3-46eb-a660-2f4544f2fd22"
   },
   "outputs": [],
   "source": [
    " # What happens if the assumptions are no longer true?\n",
    "perm = torch.randperm(784)\n",
    "plt.figure(figsize=(16, 12))\n",
    "for i in range(10):\n",
    "    image, _ = train_loader.dataset.__getitem__(i)\n",
    "    # permute pixels\n",
    "    image_perm = image.view(-1, 28*28).clone()\n",
    "    image_perm = image_perm[:, perm]\n",
    "    image_perm = image_perm.view(-1, 1, 28, 28)\n",
    "    plt.subplot(4, 5, i + 1)\n",
    "    plt.imshow(image.squeeze().numpy())\n",
    "    plt.axis('off')\n",
    "    plt.subplot(4, 5, i + 11)\n",
    "    plt.imshow(image_perm.squeeze().numpy())\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xg3xg0nfjiqG",
    "outputId": "2a5b4d82-f66c-427a-9e40-b6e9dde21512"
   },
   "outputs": [],
   "source": [
    "# CONVNET WITH PERMUTED PIXELS \n",
    "n_features = 6 # number of feature maps\n",
    "\n",
    "model_cnn = CNN(input_size, n_features, output_size)\n",
    "model_cnn.to(device)\n",
    "optimizer = optim.SGD(model_cnn.parameters(), lr=0.01, momentum=0.5)\n",
    "print('Number of parameters: {}'.format(get_n_params(model_cnn)))\n",
    "\n",
    "for epoch in range(0, 1):\n",
    "    train(epoch, model_cnn, perm)\n",
    "    test(model_cnn, perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fKM9vnFujyDX",
    "outputId": "42ff9a98-a4fe-4593-f9ee-137a009ed7d2"
   },
   "outputs": [],
   "source": [
    "# FULL-CONNECTED WITH PERMUTED PIXELS\n",
    "n_hidden = 8    # number of hidden units\n",
    "\n",
    "model_fnn = FC2Layer(input_size, n_hidden, output_size)\n",
    "model_fnn.to(device)\n",
    "optimizer = optim.SGD(model_fnn.parameters(), lr=0.01, momentum=0.5)\n",
    "print('Number of parameters: {}'.format(get_n_params(model_fnn)))\n",
    "\n",
    "for epoch in range(0, 1):\n",
    "    train(epoch, model_fnn, perm)\n",
    "    test(model_fnn, perm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Analisys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GE1daCTKkNFY"
   },
   "source": [
    "* ConvNet makes the assumption that pixels lie on a grid and are stationary/local\n",
    "* It loses performance when this assumption is wrong\n",
    "* The fully-connected network does not make this assumption\n",
    "* It does less well when it is true, since it doesn't take advantage of this prior knowledge\n",
    "* But it doesn't suffer when the assumption is wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "hNo-j-njj7Eb",
    "outputId": "36df3a50-00fe-4659-e2a2-984cb5243feb"
   },
   "outputs": [],
   "source": [
    "plt.bar(('NN image', 'CNN image',\n",
    "         'CNN scrambled', 'NN scrambled'),\n",
    "        accuracy_list, width=0.4)\n",
    "plt.ylim((min(accuracy_list)-5, 96))\n",
    "plt.ylabel('Accuracy [%]')\n",
    "for tick in plt.gca().xaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(10)\n",
    "plt.title('Performance comparison');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tLkx2HvNkXE6",
    "outputId": "c33e591b-27e8-407a-e893-eeb055f85509"
   },
   "outputs": [],
   "source": [
    "# check model parameters\n",
    "print(\"full connected\")\n",
    "print(dir(model_cnn))\n",
    "\n",
    "print(\"-----------------------------------------------------\")\n",
    "\n",
    "print(\"convolutional\")\n",
    "print(dir(model_fnn))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "7b6db0c1d442fe597d9b481cd2ea939a45b3fa778adc3bd0e8ea6ed6edc9a97e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
