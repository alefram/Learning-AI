{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81a77f0c-f949-4d14-8bf6-86c28a897c77",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# DDPG algorithm in flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304cf93a-9376-4628-a878-7eab33df86a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "\n",
    "import jax.tree_util as jtu\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "from flax.training import train_state, orbax_utils\n",
    "from flax import linen as nn  # Linen API\n",
    "\n",
    "from tqdm import tqdm\n",
    "import orbax.checkpoint\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "F_CPP_MIN_LOG_LEVEL=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5691d42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ckpt_dir = './agent' # create the agent folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25a9a29",
   "metadata": {},
   "source": [
    "## Usefull Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a764cb53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#random process N for action exploration\n",
    "@jtu.Partial(jax.jit, static_argnums=2)\n",
    "def noise(noise_scale=0.1, key=jax.random.PRNGKey(0), action_dim=2):\n",
    "    return noise_scale * jax.random.normal(key, (action_dim,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca06c72c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the method to update model parameters\n",
    "\n",
    "# update critic\n",
    "@jax.jit\n",
    "def update_critic(model, states, actions, y):\n",
    "    def compute_critic_loss(params):\n",
    "        Q = model.apply_fn(params, states, actions)\n",
    "        \n",
    "        return jnp.mean((Q - y)**2) #compute loss\n",
    "    \n",
    "    loss, grads = jax.value_and_grad(compute_critic_loss)(model.params)\n",
    "    updated_model = model.apply_gradients(grads=grads)\n",
    "   \n",
    "    return updated_model, loss\n",
    "\n",
    "# udate actor\n",
    "@jtu.Partial(jax.jit, static_argnums=(3,))\n",
    "def update_actor(model, critic, states, action_dim):\n",
    "    def compute_actor_loss(params):\n",
    "        actions = model.apply_fn(params, states, action_dim)\n",
    "        \n",
    "        Q = critic.apply_fn(critic.params, states, actions)\n",
    "\n",
    "        return -jnp.mean(Q)  # Compute the actor loss\n",
    "\n",
    "    loss, grads = jax.value_and_grad(compute_actor_loss)(model.params)\n",
    "    updated_model = model.apply_gradients(grads=grads)\n",
    "\n",
    "    return updated_model, loss\n",
    "\n",
    "# Define the soft update function\n",
    "@jax.jit\n",
    "def soft_update(target_params, source_params, tau):\n",
    "    # Convert the source_params to a JAX-compatible data structure\n",
    "    source_params_tree = jtu.tree_map(lambda x: jnp.asarray(x), source_params)\n",
    "    target_params_tree = jtu.tree_map(lambda x: jnp.asarray(x), target_params)\n",
    "\n",
    "    # Compute the updated target parameters using a soft update\n",
    "    updated_params = jtu.tree_map(lambda x, y: tau * x + (1 - tau) * y,\n",
    "                                  source_params_tree, target_params_tree)\n",
    "\n",
    "    return updated_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bb8988-b86d-4a4a-b5d5-16bc53c509e7",
   "metadata": {},
   "source": [
    "## Define ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1d002d-136a-4677-bc5e-6a29b376062b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the replay buffer\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, buffer_size, batch_size, key):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer = deque(maxlen=self.buffer_size)\n",
    "        self.key = key\n",
    "\n",
    "    def add(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample_batch(self):\n",
    "        indices = jax.random.choice(self.key, len(self.buffer),\n",
    "                                    shape=(self.batch_size,), replace=True)\n",
    "        batch = [self.buffer[i] for i in indices]\n",
    "        \n",
    "        return zip(*batch)\n",
    "    \n",
    "    #TODO:normalize buffer\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a71636",
   "metadata": {},
   "source": [
    "## Define actor and critic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c7fe1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create the actor and critic newtorks like multilayer perceptrons\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"critic model MLP\"\"\"\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, observations, actions):\n",
    "        x = jnp.concatenate([observations, actions], axis=-1)\n",
    "        x = nn.Dense(features=256)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=256)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=1)(x)\n",
    "        return jnp.squeeze(x, axis=-1)\n",
    "    \n",
    "class Actor(nn.Module):\n",
    "    \"\"\"actor model MLP\"\"\"\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x, action_dim):\n",
    "        x = nn.Dense(features=256)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=256)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=action_dim)(x)\n",
    "        x = nn.tanh(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9e8513",
   "metadata": {},
   "source": [
    "## Define algorithm parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0967cc-3733-4ef2-9b3c-6fc7e8694a75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define environment and parameters\n",
    "env = gym.make(\"InvertedPendulum-v4\")\n",
    "action_dim = env.action_space.shape[0]\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n",
    "#initialize parameters\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "key = jax.random.PRNGKey(seed)\n",
    "episodes = 1000\n",
    "gamma = 0.99 #discount factor 0:nearly rewards, 1:future rewards\n",
    "tau = 0.995 #polyak between 0-1 updating target network\n",
    "max_episode_steps = 1000\n",
    "buffer_size = 10000 #memory size\n",
    "batch_size = 128 #The number of experiences sampled from the replay buffer\n",
    "learning_rate = 0.001\n",
    "noise_scale = 0.1 #scale of the noise for random process N for action exploration\n",
    "start_steps = 500 # Number of steps for uniform-random action selection,before running real policy. Helps exploration.\n",
    "\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=max_episode_steps)\n",
    "\n",
    "if os.path.exists(ckpt_dir):\n",
    "    shutil.rmtree(ckpt_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff5afde-d37d-4a65-8af7-316be0f76dbf",
   "metadata": {},
   "source": [
    "## Initialize models and buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af776785",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Randomly initialize critic network Q(s, a|θ_Q ) and actor μ(s|θ_μ ) with weights θ_Q and θ_μ .\n",
    "obs, _ = env.reset();\n",
    "\n",
    "# Initialize the training state actor and critic models\n",
    "critic = train_state.TrainState.create(\n",
    "    apply_fn=Critic().apply,\n",
    "    params=Critic().init(key, obs, env.action_space.sample()), #init critic parameters\n",
    "    tx=optax.adam(learning_rate=learning_rate) #define optimizer\n",
    ")\n",
    "\n",
    "actor = train_state.TrainState.create(\n",
    "    apply_fn=Actor().apply,\n",
    "    params=Actor().init(key, obs, action_dim), #init actor parameters\n",
    "    tx=optax.adam(learning_rate=learning_rate) #define optimizer\n",
    ")\n",
    "\n",
    "# to save agent\n",
    "config = {'dimensions': jnp.array([5,3]), 'name': 'actor'}\n",
    "ckpt = {'model': actor, 'config': config, 'data': actor_params}\n",
    "\n",
    "# print(Actor().tabulate(key, obs, action_dim))\n",
    "# print(Critic().tabulate(key, obs, env.action_space.sample()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fede34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize target network Q_0_target and μ_0_target with weights \n",
    "# θ_Q_target ← θ_Q , θ_μ_target ← θ_μ\n",
    "\n",
    "# Initialize the training state for flax porpuses\n",
    "target_critic = train_state.TrainState.create(\n",
    "    apply_fn=Critic().apply,\n",
    "    params=critic_params,\n",
    "    tx=optax.adam(learning_rate=learning_rate)\n",
    ")\n",
    "\n",
    "target_actor = train_state.TrainState.create(\n",
    "    apply_fn=Actor().apply,\n",
    "    params=actor_params,\n",
    "    tx=optax.adam(learning_rate=learning_rate)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f4b370",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize replay buffer R\n",
    "buffer = ReplayBuffer(buffer_size, batch_size, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0dbd9c-a50a-477b-b860-f7a39d12ac9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670e041a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 806 ===> Average reward 4 Critic loss 43.4069 Actor loss -7.3792434 \n",
      "\n",
      "Episode: 807 ===> Average reward 4 Critic loss 39.870224 Actor loss -7.172924 \n",
      "\n",
      "Episode: 808 ===> Average reward 4 Critic loss 42.056725 Actor loss -7.2675285 \n",
      "\n",
      "Episode: 809 ===> Average reward 4 Critic loss 40.184128 Actor loss -7.186041 \n",
      "\n",
      "Episode: 810 ===> Average reward 4 Critic loss 42.61873 Actor loss -7.308307 \n",
      "\n",
      "Episode: 811 ===> Average reward 4 Critic loss 39.977962 Actor loss -7.1391993 \n",
      "\n",
      "Episode: 812 ===> Average reward 4 Critic loss 37.036682 Actor loss -6.9225845 \n",
      "\n",
      "Episode: 813 ===> Average reward 4 Critic loss 41.78656 Actor loss -7.2579975 \n",
      "\n",
      "Episode: 814 ===> Average reward 4 Critic loss 40.971504 Actor loss -7.212289 \n",
      "\n",
      "Episode: 815 ===> Average reward 4 Critic loss 40.714935 Actor loss -7.174796 \n",
      "\n",
      "Episode: 816 ===> Average reward 4 Critic loss 37.67344 Actor loss -6.960684 \n",
      "\n",
      "Episode: 817 ===> Average reward 4 Critic loss 37.76644 Actor loss -6.966877 \n",
      "\n",
      "Episode: 818 ===> Average reward 4 Critic loss 38.750816 Actor loss -7.0293226 \n",
      "\n",
      "Episode: 819 ===> Average reward 4 Critic loss 36.833027 Actor loss -6.8839636 \n",
      "\n",
      "Episode: 820 ===> Average reward 4 Critic loss 33.310867 Actor loss -6.644557 \n",
      "\n",
      "Episode: 821 ===> Average reward 4 Critic loss 35.17917 Actor loss -6.7614274 \n",
      "\n",
      "Episode: 822 ===> Average reward 4 Critic loss 35.213356 Actor loss -6.7662363 \n",
      "\n",
      "Episode: 823 ===> Average reward 4 Critic loss 33.191765 Actor loss -6.6158304 \n",
      "\n",
      "Episode: 824 ===> Average reward 4 Critic loss 35.5357 Actor loss -6.780225 \n",
      "\n",
      "Episode: 825 ===> Average reward 4 Critic loss 34.572823 Actor loss -6.6997957 \n",
      "\n",
      "Episode: 826 ===> Average reward 4 Critic loss 33.394608 Actor loss -6.6331806 \n",
      "\n",
      "Episode: 827 ===> Average reward 4 Critic loss 31.84029 Actor loss -6.506623 \n",
      "\n",
      "Episode: 828 ===> Average reward 4 Critic loss 31.743633 Actor loss -6.4900527 \n",
      "\n",
      "Episode: 829 ===> Average reward 4 Critic loss 30.5216 Actor loss -6.4089704 \n",
      "\n",
      "Episode: 830 ===> Average reward 4 Critic loss 30.662256 Actor loss -6.4060726 \n",
      "\n",
      "Episode: 831 ===> Average reward 4 Critic loss 32.646782 Actor loss -6.5592566 \n",
      "\n",
      "Episode: 832 ===> Average reward 4 Critic loss 31.02642 Actor loss -6.4350386 \n",
      "\n",
      "Episode: 833 ===> Average reward 4 Critic loss 31.361605 Actor loss -6.447952 \n",
      "\n",
      "Episode: 834 ===> Average reward 4 Critic loss 29.59428 Actor loss -6.2930117 \n",
      "\n",
      "Episode: 835 ===> Average reward 4 Critic loss 31.87044 Actor loss -6.490223 \n",
      "\n",
      "Episode: 836 ===> Average reward 4 Critic loss 32.29691 Actor loss -6.5303044 \n",
      "\n",
      "Episode: 837 ===> Average reward 4 Critic loss 32.145687 Actor loss -6.5054226 \n",
      "\n",
      "Episode: 838 ===> Average reward 4 Critic loss 31.449335 Actor loss -6.4303427 \n",
      "\n",
      "Episode: 839 ===> Average reward 4 Critic loss 31.470394 Actor loss -6.448144 \n",
      "\n",
      "Episode: 840 ===> Average reward 4 Critic loss 30.492992 Actor loss -6.3576555 \n",
      "\n",
      "Episode: 841 ===> Average reward 4 Critic loss 29.421453 Actor loss -6.2909827 \n",
      "\n",
      "Episode: 842 ===> Average reward 4 Critic loss 31.450256 Actor loss -6.438426 \n",
      "\n",
      "Episode: 843 ===> Average reward 4 Critic loss 28.917454 Actor loss -6.2278214 \n",
      "\n",
      "Episode: 844 ===> Average reward 4 Critic loss 29.9615 Actor loss -6.3139696 \n",
      "\n",
      "Episode: 845 ===> Average reward 4 Critic loss 28.429085 Actor loss -6.182513 \n",
      "\n",
      "Episode: 846 ===> Average reward 4 Critic loss 30.106825 Actor loss -6.340767 \n",
      "\n",
      "Episode: 847 ===> Average reward 4 Critic loss 32.034195 Actor loss -6.487237 \n",
      "\n",
      "Episode: 848 ===> Average reward 4 Critic loss 32.10042 Actor loss -6.4818587 \n",
      "\n",
      "Episode: 849 ===> Average reward 4 Critic loss 31.215797 Actor loss -6.42953 \n",
      "\n",
      "Episode: 850 ===> Average reward 4 Critic loss 32.34218 Actor loss -6.5213633 \n",
      "\n",
      "Episode: 851 ===> Average reward 4 Critic loss 31.04098 Actor loss -6.413037 \n",
      "\n",
      "Episode: 852 ===> Average reward 4 Critic loss 28.872467 Actor loss -6.239088 \n",
      "\n",
      "Episode: 853 ===> Average reward 4 Critic loss 31.911785 Actor loss -6.4951324 \n",
      "\n",
      "Episode: 854 ===> Average reward 4 Critic loss 30.217495 Actor loss -6.3430567 \n",
      "\n",
      "Episode: 855 ===> Average reward 4 Critic loss 30.169884 Actor loss -6.3528156 \n",
      "\n",
      "Episode: 856 ===> Average reward 4 Critic loss 30.563782 Actor loss -6.3567905 \n",
      "\n",
      "Episode: 857 ===> Average reward 4 Critic loss 32.541595 Actor loss -6.534208 \n",
      "\n",
      "Episode: 858 ===> Average reward 4 Critic loss 32.58362 Actor loss -6.542901 \n",
      "\n",
      "Episode: 859 ===> Average reward 4 Critic loss 30.562643 Actor loss -6.3779793 \n",
      "\n",
      "Episode: 860 ===> Average reward 4 Critic loss 30.837055 Actor loss -6.4080963 \n",
      "\n",
      "Episode: 861 ===> Average reward 4 Critic loss 32.6006 Actor loss -6.565362 \n",
      "\n",
      "Episode: 862 ===> Average reward 4 Critic loss 31.656311 Actor loss -6.458455 \n",
      "\n",
      "Episode: 863 ===> Average reward 4 Critic loss 31.053885 Actor loss -6.4435587 \n",
      "\n",
      "Episode: 864 ===> Average reward 4 Critic loss 30.674252 Actor loss -6.416357 \n",
      "\n",
      "Episode: 865 ===> Average reward 4 Critic loss 31.553093 Actor loss -6.453698 \n",
      "\n",
      "Episode: 866 ===> Average reward 4 Critic loss 31.33993 Actor loss -6.4684486 \n",
      "\n",
      "Episode: 867 ===> Average reward 4 Critic loss 33.85842 Actor loss -6.658983 \n",
      "\n",
      "Episode: 868 ===> Average reward 4 Critic loss 31.594185 Actor loss -6.46093 \n",
      "\n",
      "Episode: 869 ===> Average reward 4 Critic loss 31.108063 Actor loss -6.444631 \n",
      "\n",
      "Episode: 870 ===> Average reward 4 Critic loss 33.106583 Actor loss -6.6097383 \n",
      "\n",
      "Episode: 871 ===> Average reward 4 Critic loss 30.781609 Actor loss -6.436859 \n",
      "\n",
      "Episode: 872 ===> Average reward 4 Critic loss 29.80507 Actor loss -6.333036 \n",
      "\n",
      "Episode: 873 ===> Average reward 4 Critic loss 32.291702 Actor loss -6.5461564 \n",
      "\n",
      "Episode: 874 ===> Average reward 4 Critic loss 30.078098 Actor loss -6.3572016 \n",
      "\n",
      "Episode: 875 ===> Average reward 4 Critic loss 32.706562 Actor loss -6.59169 \n",
      "\n",
      "Episode: 876 ===> Average reward 4 Critic loss 30.974926 Actor loss -6.44211 \n",
      "\n",
      "Episode: 877 ===> Average reward 4 Critic loss 32.266697 Actor loss -6.55128 \n",
      "\n",
      "Episode: 878 ===> Average reward 4 Critic loss 33.919083 Actor loss -6.6705866 \n",
      "\n",
      "Episode: 879 ===> Average reward 4 Critic loss 32.49158 Actor loss -6.5651455 \n",
      "\n",
      "Episode: 880 ===> Average reward 4 Critic loss 34.810413 Actor loss -6.770043 \n",
      "\n",
      "Episode: 881 ===> Average reward 4 Critic loss 36.024612 Actor loss -6.825661 \n",
      "\n",
      "Episode: 882 ===> Average reward 4 Critic loss 35.389027 Actor loss -6.7846355 \n",
      "\n",
      "Episode: 883 ===> Average reward 4 Critic loss 36.06264 Actor loss -6.851861 \n",
      "\n",
      "Episode: 884 ===> Average reward 4 Critic loss 36.05563 Actor loss -6.8361726 \n",
      "\n",
      "Episode: 885 ===> Average reward 4 Critic loss 37.533028 Actor loss -6.952741 \n",
      "\n",
      "Episode: 886 ===> Average reward 4 Critic loss 35.81675 Actor loss -6.823472 \n",
      "\n",
      "Episode: 887 ===> Average reward 4 Critic loss 37.206856 Actor loss -6.9276166 \n",
      "\n",
      "Episode: 888 ===> Average reward 4 Critic loss 33.98623 Actor loss -6.6910095 \n",
      "\n",
      "Episode: 889 ===> Average reward 4 Critic loss 34.051235 Actor loss -6.6866307 \n",
      "\n",
      "Episode: 890 ===> Average reward 4 Critic loss 33.611134 Actor loss -6.67051 \n",
      "\n",
      "Episode: 891 ===> Average reward 4 Critic loss 34.763184 Actor loss -6.756137 \n",
      "\n",
      "Episode: 892 ===> Average reward 4 Critic loss 35.037918 Actor loss -6.783823 \n",
      "\n",
      "Episode: 893 ===> Average reward 4 Critic loss 34.05716 Actor loss -6.687603 \n",
      "\n",
      "Episode: 894 ===> Average reward 4 Critic loss 36.295975 Actor loss -6.850016 \n",
      "\n",
      "Episode: 895 ===> Average reward 4 Critic loss 34.051857 Actor loss -6.670884 \n",
      "\n",
      "Episode: 896 ===> Average reward 4 Critic loss 34.28831 Actor loss -6.6981497 \n",
      "\n",
      "Episode: 897 ===> Average reward 4 Critic loss 34.71169 Actor loss -6.744379 \n",
      "\n",
      "Episode: 898 ===> Average reward 4 Critic loss 36.25296 Actor loss -6.8439455 \n",
      "\n",
      "Episode: 899 ===> Average reward 4 Critic loss 33.853874 Actor loss -6.6791687 \n",
      "\n",
      "Episode: 900 ===> Average reward 4 Critic loss 35.518883 Actor loss -6.801323 \n",
      "\n",
      "Episode: 901 ===> Average reward 4 Critic loss 34.025978 Actor loss -6.6887217 \n",
      "\n",
      "Episode: 902 ===> Average reward 4 Critic loss 35.99244 Actor loss -6.843955 \n",
      "\n",
      "Episode: 903 ===> Average reward 4 Critic loss 33.671463 Actor loss -6.670608 \n",
      "\n",
      "Episode: 904 ===> Average reward 4 Critic loss 33.673008 Actor loss -6.661371 \n",
      "\n",
      "Episode: 905 ===> Average reward 4 Critic loss 33.786842 Actor loss -6.6691146 \n",
      "\n",
      "Episode: 906 ===> Average reward 4 Critic loss 33.710194 Actor loss -6.6813955 \n",
      "\n",
      "Episode: 907 ===> Average reward 4 Critic loss 35.46996 Actor loss -6.7930374 \n",
      "\n",
      "Episode: 908 ===> Average reward 4 Critic loss 35.609703 Actor loss -6.8060226 \n",
      "\n",
      "Episode: 909 ===> Average reward 4 Critic loss 34.981033 Actor loss -6.7396126 \n",
      "\n",
      "Episode: 910 ===> Average reward 4 Critic loss 32.363644 Actor loss -6.533432 \n",
      "\n",
      "Episode: 911 ===> Average reward 4 Critic loss 34.528633 Actor loss -6.720509 \n",
      "\n",
      "Episode: 912 ===> Average reward 4 Critic loss 34.080902 Actor loss -6.685776 \n",
      "\n",
      "Episode: 913 ===> Average reward 4 Critic loss 33.894894 Actor loss -6.6638865 \n",
      "\n",
      "Episode: 914 ===> Average reward 4 Critic loss 36.385166 Actor loss -6.838802 \n",
      "\n",
      "Episode: 915 ===> Average reward 4 Critic loss 34.765945 Actor loss -6.736637 \n",
      "\n",
      "Episode: 916 ===> Average reward 4 Critic loss 31.986143 Actor loss -6.5294523 \n",
      "\n",
      "Episode: 917 ===> Average reward 4 Critic loss 33.004425 Actor loss -6.6091194 \n",
      "\n",
      "Episode: 918 ===> Average reward 4 Critic loss 34.135014 Actor loss -6.6964083 \n",
      "\n",
      "Episode: 919 ===> Average reward 4 Critic loss 34.32862 Actor loss -6.6958942 \n",
      "\n",
      "Episode: 920 ===> Average reward 4 Critic loss 32.827885 Actor loss -6.6142187 \n",
      "\n",
      "Episode: 921 ===> Average reward 4 Critic loss 38.121563 Actor loss -6.985329 \n",
      "\n",
      "Episode: 922 ===> Average reward 4 Critic loss 34.999367 Actor loss -6.7511415 \n",
      "\n",
      "Episode: 923 ===> Average reward 4 Critic loss 34.755474 Actor loss -6.7400374 \n",
      "\n",
      "Episode: 924 ===> Average reward 4 Critic loss 34.139244 Actor loss -6.694558 \n",
      "\n",
      "Episode: 925 ===> Average reward 4 Critic loss 35.118233 Actor loss -6.7745094 \n",
      "\n",
      "Episode: 926 ===> Average reward 4 Critic loss 41.200592 Actor loss -7.217263 \n",
      "\n",
      "Episode: 927 ===> Average reward 4 Critic loss 36.206383 Actor loss -6.853121 \n",
      "\n",
      "Episode: 928 ===> Average reward 4 Critic loss 37.410713 Actor loss -6.9654894 \n",
      "\n",
      "Episode: 929 ===> Average reward 4 Critic loss 37.43345 Actor loss -6.9474344 \n",
      "\n",
      "Episode: 930 ===> Average reward 4 Critic loss 36.605076 Actor loss -6.9038124 \n",
      "\n",
      "Episode: 931 ===> Average reward 4 Critic loss 37.3359 Actor loss -6.9377217 \n",
      "\n",
      "Episode: 932 ===> Average reward 4 Critic loss 40.948174 Actor loss -7.1989136 \n",
      "\n",
      "Episode: 933 ===> Average reward 4 Critic loss 38.78869 Actor loss -7.0494766 \n",
      "\n",
      "Episode: 934 ===> Average reward 4 Critic loss 36.07345 Actor loss -6.8795004 \n",
      "\n",
      "Episode: 935 ===> Average reward 4 Critic loss 41.453445 Actor loss -7.2359667 \n",
      "\n",
      "Episode: 936 ===> Average reward 4 Critic loss 43.67607 Actor loss -7.4019213 \n",
      "\n",
      "Episode: 937 ===> Average reward 4 Critic loss 43.042095 Actor loss -7.3408976 \n",
      "\n",
      "Episode: 938 ===> Average reward 4 Critic loss 46.624603 Actor loss -7.6002913 \n",
      "\n",
      "Episode: 939 ===> Average reward 4 Critic loss 45.325195 Actor loss -7.5053086 \n",
      "\n",
      "Episode: 940 ===> Average reward 4 Critic loss 46.61344 Actor loss -7.6253643 \n",
      "\n",
      "Episode: 941 ===> Average reward 4 Critic loss 45.195374 Actor loss -7.5280647 \n",
      "\n",
      "Episode: 942 ===> Average reward 4 Critic loss 46.83251 Actor loss -7.622575 \n",
      "\n",
      "Episode: 943 ===> Average reward 4 Critic loss 43.870564 Actor loss -7.442827 \n",
      "\n",
      "Episode: 944 ===> Average reward 4 Critic loss 44.536213 Actor loss -7.4993877 \n",
      "\n",
      "Episode: 945 ===> Average reward 4 Critic loss 46.00792 Actor loss -7.581618 \n",
      "\n",
      "Episode: 946 ===> Average reward 4 Critic loss 47.31681 Actor loss -7.6371326 \n",
      "\n",
      "Episode: 947 ===> Average reward 4 Critic loss 46.010334 Actor loss -7.5635147 \n",
      "\n",
      "Episode: 948 ===> Average reward 4 Critic loss 43.39927 Actor loss -7.3768888 \n",
      "\n",
      "Episode: 949 ===> Average reward 4 Critic loss 42.528442 Actor loss -7.3302336 \n",
      "\n",
      "Episode: 950 ===> Average reward 4 Critic loss 41.95205 Actor loss -7.308304 \n",
      "\n",
      "Episode: 951 ===> Average reward 4 Critic loss 43.068882 Actor loss -7.38429 \n",
      "\n",
      "Episode: 952 ===> Average reward 4 Critic loss 42.494312 Actor loss -7.3367796 \n",
      "\n",
      "Episode: 953 ===> Average reward 4 Critic loss 41.603203 Actor loss -7.255234 \n",
      "\n",
      "Episode: 954 ===> Average reward 4 Critic loss 40.247696 Actor loss -7.169306 \n",
      "\n",
      "Episode: 955 ===> Average reward 4 Critic loss 42.370636 Actor loss -7.3041506 \n",
      "\n",
      "Episode: 956 ===> Average reward 4 Critic loss 41.307583 Actor loss -7.2463365 \n",
      "\n",
      "Episode: 957 ===> Average reward 4 Critic loss 37.928844 Actor loss -6.9834566 \n",
      "\n",
      "Episode: 958 ===> Average reward 4 Critic loss 40.734924 Actor loss -7.2033353 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "episodes_reward = []\n",
    "critic_loss = 0\n",
    "actor_loss = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(episodes):\n",
    "    # Initialize a random process N for action exploration\n",
    "    N = noise(noise_scale, key, action_dim)\n",
    "    # Receive initial observation state s 1\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        # Select action a_t = μ(s t |θ μ ) + N t according to the current policy and exploration noise\n",
    "        action = actor.apply_fn(actor.params, state, action_dim) + N\n",
    "        \n",
    "        # Execute action a t and observe reward r t and observe new state s t+1\n",
    "        observation, reward, terminated, truncated, _ = env.step(action)\n",
    "                \n",
    "        # Store transition (s t , a t , r t , s t+1 ) in R\n",
    "        transition = (state, action, reward, observation)\n",
    "        buffer.add(transition)\n",
    "                    \n",
    "        # Sample a random minibatch of N transitions (s i , a i , r i , s i+1 ) from R\n",
    "        states, actions, rewards, next_states = buffer.sample_batch()\n",
    "\n",
    "        # Set y = r  + γQ^0 (s_{i+1} , μ^0 (s_{i+1} |θ^μ )|θ^Q ) P\n",
    "        target_action = target_actor.apply_fn(target_actor_params,\n",
    "                                              jnp.asarray(next_states),\n",
    "                                              action_dim)\n",
    "\n",
    "\n",
    "        target_q = target_critic.apply_fn(target_critic_params,\n",
    "                                          jnp.asarray(next_states),\n",
    "                                          jnp.asarray(target_action))\n",
    "\n",
    "        rewards = jnp.asarray(rewards)\n",
    "\n",
    "        y = rewards + gamma * (1 - terminated) * target_q #corregir es un arreglo revisar paper\n",
    "\n",
    "\n",
    "        # Update critic by minimizing the loss\n",
    "        critic, critic_loss = update_critic(critic,\n",
    "                                            jnp.asarray(states),\n",
    "                                            jnp.asarray(actions),\n",
    "                                            jnp.asarray(y))\n",
    "\n",
    "        # Update the actor policy using the sampled gradient:\n",
    "        actor, actor_loss = update_actor(actor,\n",
    "                                         critic,\n",
    "                                         jnp.asarray(states),\n",
    "                                         action_dim)\n",
    "\n",
    "        # Update the target networks:\n",
    "        target_actor_params = soft_update(target_actor_params, actor.params, tau)\n",
    "        target_critic_params = soft_update(target_critic_params, critic.params, tau)\n",
    "        \n",
    "        # update if the environment is done and the current observation\n",
    "        done = terminated or truncated\n",
    "        state = observation\n",
    "    \n",
    "\n",
    "    episodes_reward.append(env.return_queue[-1]) \n",
    "    avg_reward = int(np.mean(env.return_queue))\n",
    "    \n",
    "\n",
    "    print(\"Episode:\", i+1, \"===>\", \"Average reward:\", avg_reward, \"|\",\n",
    "        \"Critic loss:\", critic_loss, \"|\",\n",
    "        \"Actor loss:\", actor_loss, \"\\n\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "# execution time\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(\"Execution time:\", execution_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d04fcd1-8596-4ac2-bc2b-4b7c625a7c02",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Save agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b42bf7-a946-4eb7-aa50-d4537c3fbda0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save agent\n",
    "orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
    "save_args = orbax_utils.save_args_from_target(ckpt)\n",
    "orbax_checkpointer.save('./agent', ckpt, save_args=save_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496fd34a",
   "metadata": {},
   "source": [
    "## Visualizing the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217629c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rewards_to_plot = [rewards for rewards in episodes_reward]\n",
    "\n",
    "plt.plot(range(episodes), episodes_reward)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Reward')\n",
    "plt.grid()\n",
    "plt.title('Rewards over episodes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5ff1e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896b582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore agent\n",
    "raw_restored = orbax_checkpointer.restore('./agent')\n",
    "actor_params = raw_restored['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53364c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"InvertedPendulum-v4\", render_mode=\"human\")\n",
    "\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(1000):\n",
    "    action = actor.apply_fn(actor_params, observation, env.action_space.shape[0])\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
