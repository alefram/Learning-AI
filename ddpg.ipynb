{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e524b141",
   "metadata": {},
   "source": [
    "# DDPG algorithm in flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b15bcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import jax.tree_util as jtu\n",
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "from flax.training import train_state, orbax_utils\n",
    "from flax import linen as nn  # Linen API\n",
    "\n",
    "from tqdm import tqdm\n",
    "import orbax.checkpoint\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "F_CPP_MIN_LOG_LEVEL=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5691d42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ckpt_dir = './agent' # create the agent folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25a9a29",
   "metadata": {},
   "source": [
    "## Usefull Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a764cb53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#random process N for action exploration\n",
    "@jtu.Partial(jax.jit, static_argnums=2)\n",
    "def noise(noise_scale=0.1, key=random.PRNGKey(0), action_dim=2):\n",
    "    return noise_scale * jax.random.normal(key, (action_dim,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca06c72c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the method to update model parameters\n",
    "\n",
    "# update critic\n",
    "@jax.jit\n",
    "def update_critic(model, states, actions, y):\n",
    "    def compute_critic_loss(params):\n",
    "        Q = model.apply_fn(params, states, actions)\n",
    "        \n",
    "        return jnp.mean((Q - y)**2) #compute loss\n",
    "    \n",
    "    loss, grads = jax.value_and_grad(compute_critic_loss)(model.params)\n",
    "    updated_model = model.apply_gradients(grads=grads)\n",
    "   \n",
    "    return updated_model, loss\n",
    "\n",
    "# udate actor\n",
    "@jtu.Partial(jax.jit, static_argnums=(2,))\n",
    "def update_actor(model, states, action_dim):\n",
    "    def compute_actor_loss(params):\n",
    "        actions = model.apply_fn(params, states, action_dim)\n",
    "\n",
    "        return -jnp.mean(actions)  # Compute the actor loss\n",
    "\n",
    "    loss, grads = jax.value_and_grad(compute_actor_loss)(model.params)\n",
    "    updated_model = model.apply_gradients(grads=grads)\n",
    "\n",
    "    return updated_model, loss\n",
    "\n",
    "# Define the soft update function\n",
    "@jax.jit\n",
    "def soft_update(target_params, source_params, tau):\n",
    "    # Convert the source_params to a JAX-compatible data structure\n",
    "    source_params_tree = jtu.tree_map(lambda x: jnp.asarray(x), source_params)\n",
    "    target_params_tree = jtu.tree_map(lambda x: jnp.asarray(x), target_params)\n",
    "\n",
    "    # Compute the updated target parameters using a soft update\n",
    "    updated_params = jtu.tree_map(lambda x, y: tau * x + (1 - tau) * y,\n",
    "                                  source_params_tree, target_params_tree)\n",
    "\n",
    "    return updated_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bb8988-b86d-4a4a-b5d5-16bc53c509e7",
   "metadata": {},
   "source": [
    "## Define ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1d002d-136a-4677-bc5e-6a29b376062b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the replay buffer\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, buffer_size, batch_size, key):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer = deque(maxlen=self.buffer_size)\n",
    "        self.key = key\n",
    "\n",
    "    def add(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample_batch(self):\n",
    "        indices = jax.random.choice(self.key, len(self.buffer),\n",
    "                                    shape=(self.batch_size,), replace=True)\n",
    "        batch = [self.buffer[i] for i in indices]\n",
    "\n",
    "        return zip(*batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a71636",
   "metadata": {},
   "source": [
    "## Define actor and critic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c7fe1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create the actor and critic newtorks like multilayer perceptrons\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"critic model MLP\"\"\"\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, observations, actions):\n",
    "        x = jnp.concatenate([observations, actions], axis=-1)\n",
    "        x = nn.Dense(features=256)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=256)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=1)(x)\n",
    "        return jnp.squeeze(x, axis=-1)\n",
    "    \n",
    "class Actor(nn.Module):\n",
    "    \"\"\"actor model MLP\"\"\"\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x, action_dim):\n",
    "        x = nn.Dense(features=256)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=256)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=action_dim)(x)\n",
    "        x = nn.tanh(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9e8513",
   "metadata": {},
   "source": [
    "## Define algorithm parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0967cc-3733-4ef2-9b3c-6fc7e8694a75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define environment and parameters\n",
    "env = gym.make(\"InvertedPendulum-v4\")\n",
    "action_dim = env.action_space.shape[0]\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n",
    "#initialize parameters\n",
    "seed = 0\n",
    "key = random.PRNGKey(seed)\n",
    "episodes = 50\n",
    "gamma = 0.99 #discount factor 0:nearly rewards, 1:future rewards\n",
    "tau = 0.995 #polyak between 0-1 updating target network\n",
    "max_episode_steps = 1000\n",
    "buffer_size = 1000000 #memory size\n",
    "batch_size = 100 #The number of experiences sampled from the replay buffer\n",
    "learning_rate = 0.001\n",
    "noise_scale = 0.1 #scale of the noise for random process N for action exploration\n",
    "start_steps = 1000 # Number of steps for uniform-random action selection,before running real policy. Helps exploration.\n",
    "\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=max_episode_steps)\n",
    "\n",
    "if os.path.exists(ckpt_dir):\n",
    "    shutil.rmtree(ckpt_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff5afde-d37d-4a65-8af7-316be0f76dbf",
   "metadata": {},
   "source": [
    "## Initialize models and buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af776785",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Randomly initialize critic network Q(s, a|θ_Q ) and actor μ(s|θ_μ ) with weights θ_Q and θ_μ .\n",
    "critic_params = Critic().init(key,\n",
    "                              jnp.zeros((1, state_dim)),\n",
    "                              jnp.zeros((1, action_dim)))\n",
    "\n",
    "actor_params = Actor().init(key, jnp.zeros((1, state_dim)), action_dim)\n",
    "\n",
    "# define optimizers\n",
    "actor_optimizer = optax.adam(learning_rate=learning_rate)\n",
    "actor_opt_state = actor_optimizer.init(actor_params)\n",
    "\n",
    "critic_optimizer = optax.adam(learning_rate=learning_rate)\n",
    "critic_opt_state = critic_optimizer.init(critic_params)\n",
    "\n",
    "# Initialize the training state for flax porpuses\n",
    "critic = train_state.TrainState.create(\n",
    "    apply_fn=Critic().apply,\n",
    "    params=critic_params,\n",
    "    tx=critic_optimizer\n",
    ")\n",
    "\n",
    "actor = train_state.TrainState.create(\n",
    "    apply_fn=Actor().apply,\n",
    "    params=actor_params,\n",
    "    tx=actor_optimizer,\n",
    ")\n",
    "\n",
    "# to save agent\n",
    "config = {'dimensions': jnp.array([5,3]), 'name': 'actor'}\n",
    "ckpt = {'model': actor, 'config': config, 'data': actor_params}\n",
    "\n",
    "# print(Actor().tabulate(key, (1, state_dim), action_dim))\n",
    "# print(Critic().tabulate(key, jnp.ones((1,action_dim)), jnp.ones((1,state_dim))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fede34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize target network Q_0_target and μ_0_target with weights \n",
    "# θ_Q_target ← θ_Q , θ_μ_target ← θ_μ\n",
    "\n",
    "target_critic_params = critic_params\n",
    "target_actor_params = actor_params\n",
    "\n",
    "# Initialize the training state for flax porpuses\n",
    "target_critic = train_state.TrainState.create(\n",
    "    apply_fn=Critic().apply,\n",
    "    params=target_critic_params,\n",
    "    tx=critic_optimizer\n",
    ")\n",
    "\n",
    "target_actor = train_state.TrainState.create(\n",
    "    apply_fn=Actor().apply,\n",
    "    params=target_actor_params,\n",
    "    tx=actor_optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f4b370",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize replay buffer R\n",
    "buffer = ReplayBuffer(buffer_size, batch_size, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0dbd9c-a50a-477b-b860-f7a39d12ac9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670e041a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "episodes_reward = []\n",
    "critic_loss = 0\n",
    "actor_loss = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for i in tqdm(range(episodes)):\n",
    "    # Initialize a random process N for action exploration\n",
    "    N = noise(noise_scale, key, action_dim)\n",
    "    # Receive initial observation state s 1\n",
    "    state, info = env.reset(seed=seed)\n",
    "    done = False\n",
    "    t = 1\n",
    "\n",
    "    while not done:\n",
    "        # Select action a_t = μ(s t |θ μ ) + N t according to the current policy and exploration noise\n",
    "        if t > start_steps:\n",
    "            action = actor.apply_fn(actor.params, state, action_dim) + N\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # Execute action a t and observe reward r t and observe new state s t+1\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        # Store transition (s t , a t , r t , s t+1 ) in R\n",
    "        transition = (state, action, reward, observation)\n",
    "        buffer.add(transition)\n",
    "            \n",
    "        # Sample a random minibatch of N transitions (s i , a i , r i , s i+1 ) from R\n",
    "        states, actions, rewards, next_states = buffer.sample_batch()\n",
    "\n",
    "        # Set y = r  + γQ^0 (s_{i+1} , μ^0 (s_{i+1} |θ^μ )|θ^Q ) P\n",
    "        target_action = target_actor.apply_fn(target_actor_params,\n",
    "                                              jnp.asarray(next_states),\n",
    "                                              action_dim)\n",
    "\n",
    "        target_q = target_critic.apply_fn(target_critic_params,\n",
    "                                          jnp.asarray(next_states),\n",
    "                                          jnp.asarray(target_action))\n",
    "\n",
    "        y = reward + gamma * (1 - terminated) * target_q\n",
    "\n",
    "        # Update critic by minimizing the loss\n",
    "        critic, critic_loss = update_critic(critic,\n",
    "                                            jnp.asarray(states),\n",
    "                                            jnp.asarray(actions),\n",
    "                                            jnp.asarray(y))\n",
    "\n",
    "        # Update the actor policy using the sampled gradient:\n",
    "        actor, actor_loss = update_actor(actor,\n",
    "                                         jnp.asarray(states),\n",
    "                                         action_dim)\n",
    "\n",
    "        # Update the target networks:\n",
    "        target_actor_params = soft_update(target_actor_params, actor.params, tau)\n",
    "        target_critic_params = soft_update(target_critic_params, critic.params, tau)\n",
    "       \n",
    "        # update if the environment is done and the current observation\n",
    "        t += 1\n",
    "        done = terminated or truncated\n",
    "\n",
    "    episodes_reward.append(env.return_queue[-1]) \n",
    "    avg_reward = int(np.mean(env.return_queue))\n",
    "    \n",
    "    print(\"Episode:\", i+1, \"\\n\")\n",
    "    # print(\"reward:\", env.return_queue[-1])\n",
    "    print(\"Average reward:\", avg_reward)\n",
    "    print(\"Critic loss:\", critic_loss)\n",
    "    print(\"Actor loss:\", actor_loss)\n",
    "\n",
    "env.close()\n",
    "\n",
    "# save agent\n",
    "orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
    "save_args = orbax_utils.save_args_from_target(ckpt)\n",
    "orbax_checkpointer.save('./agent', ckpt, save_args=save_args)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(\"Execution time:\", execution_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496fd34a",
   "metadata": {},
   "source": [
    "## Visualizing the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217629c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rewards_to_plot = [rewards for rewards in episodes_reward]\n",
    "\n",
    "plt.plot(range(episodes), episodes_reward)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Rewards over episodes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5ff1e7",
   "metadata": {},
   "source": [
    "## Test Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896b582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore agent\n",
    "raw_restored = orbax_checkpointer.restore('./agent')\n",
    "actor_params = raw_restored['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53364c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"InvertedPendulum-v4\", render_mode=\"human\")\n",
    "\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(1000):\n",
    "    action = actor.apply_fn(actor_params, observation, env.action_space.shape[0])\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
