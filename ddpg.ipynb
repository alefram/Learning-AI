{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e524b141",
   "metadata": {},
   "source": [
    "# DDPG algorithm in flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59b15bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from flax.training import train_state\n",
    "import jax.tree_util as jtu\n",
    "from jax import random\n",
    "from flax import linen as nn  # Linen API\n",
    "import optax\n",
    "from collections import deque\n",
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"MountainCarContinuous-v0\")\n",
    "seed = 0\n",
    "key = random.PRNGKey(seed)\n",
    "\n",
    "F_CPP_MIN_LOG_LEVEL=0\n",
    "action_dim = env.action_space.shape[0]\n",
    "state_dim = env.observation_space.shape[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ec141b2",
   "metadata": {},
   "source": [
    "## Create the actor and critic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2713383",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the actor and critic newtorks like multilayer perceptrons\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"critic model MLP\"\"\"\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, observations, actions):\n",
    "        x = jnp.concatenate([observations, actions], axis=-1)\n",
    "        x = nn.Dense(features=256)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=256)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=1)(x)\n",
    "        return jnp.squeeze(x, axis=-1)\n",
    "    \n",
    "class Actor(nn.Module):\n",
    "    \"\"\"actor model MLP\"\"\"\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(features=256)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=256)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=action_dim)(x)\n",
    "        x = nn.tanh(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b25a9a29",
   "metadata": {},
   "source": [
    "## Create necessary methods to the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a764cb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random process N for action exploration\n",
    "def noise(noise_scale=0.1, key=key, action_dim=action_dim):\n",
    "    return noise_scale * jax.random.normal(key, (action_dim,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca06c72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the method to update model parameters\n",
    "\n",
    "# update critic\n",
    "@jax.jit\n",
    "def update_critic(model, states, actions, y):\n",
    "    def compute_critic_loss(params):\n",
    "        Q = model.apply_fn(params, states, actions)\n",
    "        return jnp.mean((Q - y)**2) #compute loss\n",
    "    \n",
    "    grad_fn = jax.grad(compute_critic_loss)\n",
    "    grads = grad_fn(model.params)\n",
    "    updated_model = model.apply_gradients(grads=grads)\n",
    "    return updated_model\n",
    "\n",
    "# udate actor\n",
    "@jax.jit\n",
    "def update_actor(model, states):\n",
    "    def compute_actor_loss(params):\n",
    "        actions = model.apply_fn(params, states)\n",
    "        return -jnp.mean(actions)  # Compute the actor loss\n",
    "    \n",
    "    grad_fn = jax.grad(compute_actor_loss)\n",
    "    grads = grad_fn(model.params)\n",
    "    updated_model = model.apply_gradients(grads=grads)\n",
    "    return updated_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc39eef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the soft update function\n",
    "@jax.jit\n",
    "def soft_update(target_params, source_params, tau):\n",
    "    # Convert the source_params to a JAX-compatible data structure\n",
    "    source_params_tree = jtu.tree_map(lambda x: jnp.asarray(x), source_params)\n",
    "    target_params_tree = jtu.tree_map(lambda x: jnp.asarray(x), target_params)\n",
    "\n",
    "\n",
    "    # Compute the updated target parameters using a soft update\n",
    "    updated_params = jtu.tree_map(lambda x, y: tau * x + (1 - tau) * y, source_params_tree, target_params_tree)\n",
    "\n",
    "    return updated_params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc9e8513",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af776785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[3m                                Actor Summary                                \u001b[0m\n",
      "┏━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mpath   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmodule\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1minputs      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1moutputs     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mparams                  \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│         │ Actor  │ - 1          │ \u001b[2mfloat32\u001b[0m[1]   │                          │\n",
      "│         │        │ - 2          │              │                          │\n",
      "├─────────┼────────┼──────────────┼──────────────┼──────────────────────────┤\n",
      "│ Dense_0 │ Dense  │ - 1          │ \u001b[2mfloat32\u001b[0m[256] │ bias: \u001b[2mfloat32\u001b[0m[256]       │\n",
      "│         │        │ - 2          │              │ kernel: \u001b[2mfloat32\u001b[0m[2,256]   │\n",
      "│         │        │              │              │                          │\n",
      "│         │        │              │              │ \u001b[1m768 \u001b[0m\u001b[1;2m(3.1 KB)\u001b[0m             │\n",
      "├─────────┼────────┼──────────────┼──────────────┼──────────────────────────┤\n",
      "│ Dense_1 │ Dense  │ \u001b[2mfloat32\u001b[0m[256] │ \u001b[2mfloat32\u001b[0m[256] │ bias: \u001b[2mfloat32\u001b[0m[256]       │\n",
      "│         │        │              │              │ kernel: \u001b[2mfloat32\u001b[0m[256,256] │\n",
      "│         │        │              │              │                          │\n",
      "│         │        │              │              │ \u001b[1m65,792 \u001b[0m\u001b[1;2m(263.2 KB)\u001b[0m        │\n",
      "├─────────┼────────┼──────────────┼──────────────┼──────────────────────────┤\n",
      "│ Dense_2 │ Dense  │ \u001b[2mfloat32\u001b[0m[256] │ \u001b[2mfloat32\u001b[0m[1]   │ bias: \u001b[2mfloat32\u001b[0m[1]         │\n",
      "│         │        │              │              │ kernel: \u001b[2mfloat32\u001b[0m[256,1]   │\n",
      "│         │        │              │              │                          │\n",
      "│         │        │              │              │ \u001b[1m257 \u001b[0m\u001b[1;2m(1.0 KB)\u001b[0m             │\n",
      "├─────────┼────────┼──────────────┼──────────────┼──────────────────────────┤\n",
      "│\u001b[1m \u001b[0m\u001b[1m       \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m      \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m            \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m       Total\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m66,817 \u001b[0m\u001b[1;2m(267.3 KB)\u001b[0m\u001b[1m       \u001b[0m\u001b[1m \u001b[0m│\n",
      "└─────────┴────────┴──────────────┴──────────────┴──────────────────────────┘\n",
      "\u001b[1m                                                                             \u001b[0m\n",
      "\u001b[1m                     Total Parameters: 66,817 \u001b[0m\u001b[1;2m(267.3 KB)\u001b[0m\u001b[1m                     \u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\u001b[3m                                 Critic Summary                                 \u001b[0m\n",
      "┏━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mpath   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmodule\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1minputs        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1moutputs       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mparams                 \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│         │ Critic │ - \u001b[2mfloat32\u001b[0m[1,1] │ \u001b[2mfloat32\u001b[0m[1]     │                         │\n",
      "│         │        │ - \u001b[2mfloat32\u001b[0m[1,2] │                │                         │\n",
      "├─────────┼────────┼────────────────┼────────────────┼─────────────────────────┤\n",
      "│ Dense_0 │ Dense  │ \u001b[2mfloat32\u001b[0m[1,3]   │ \u001b[2mfloat32\u001b[0m[1,256] │ bias: \u001b[2mfloat32\u001b[0m[256]      │\n",
      "│         │        │                │                │ kernel: \u001b[2mfloat32\u001b[0m[3,256]  │\n",
      "│         │        │                │                │                         │\n",
      "│         │        │                │                │ \u001b[1m1,024 \u001b[0m\u001b[1;2m(4.1 KB)\u001b[0m          │\n",
      "├─────────┼────────┼────────────────┼────────────────┼─────────────────────────┤\n",
      "│ Dense_1 │ Dense  │ \u001b[2mfloat32\u001b[0m[1,256] │ \u001b[2mfloat32\u001b[0m[1,256] │ bias: \u001b[2mfloat32\u001b[0m[256]      │\n",
      "│         │        │                │                │ kernel:                 │\n",
      "│         │        │                │                │ \u001b[2mfloat32\u001b[0m[256,256]        │\n",
      "│         │        │                │                │                         │\n",
      "│         │        │                │                │ \u001b[1m65,792 \u001b[0m\u001b[1;2m(263.2 KB)\u001b[0m       │\n",
      "├─────────┼────────┼────────────────┼────────────────┼─────────────────────────┤\n",
      "│ Dense_2 │ Dense  │ \u001b[2mfloat32\u001b[0m[1,256] │ \u001b[2mfloat32\u001b[0m[1,1]   │ bias: \u001b[2mfloat32\u001b[0m[1]        │\n",
      "│         │        │                │                │ kernel: \u001b[2mfloat32\u001b[0m[256,1]  │\n",
      "│         │        │                │                │                         │\n",
      "│         │        │                │                │ \u001b[1m257 \u001b[0m\u001b[1;2m(1.0 KB)\u001b[0m            │\n",
      "├─────────┼────────┼────────────────┼────────────────┼─────────────────────────┤\n",
      "│\u001b[1m \u001b[0m\u001b[1m       \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m      \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m              \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m         Total\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m67,073 \u001b[0m\u001b[1;2m(268.3 KB)\u001b[0m\u001b[1m      \u001b[0m\u001b[1m \u001b[0m│\n",
      "└─────────┴────────┴────────────────┴────────────────┴─────────────────────────┘\n",
      "\u001b[1m                                                                                \u001b[0m\n",
      "\u001b[1m                      Total Parameters: 67,073 \u001b[0m\u001b[1;2m(268.3 KB)\u001b[0m\u001b[1m                       \u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Randomly initialize critic network Q(s, a|θ_Q ) and actor μ(s|θ_μ ) with weights θ_Q and θ_μ .\n",
    "critic_params = Critic().init(key, jnp.zeros((1,action_dim)), jnp.zeros((1,state_dim)))\n",
    "actor_params = Actor().init(key, jnp.zeros((1, state_dim)))\n",
    "\n",
    "# define optimizers\n",
    "actor_optimizer = optax.adam(learning_rate=100)\n",
    "actor_opt_state = actor_optimizer.init(actor_params)\n",
    "\n",
    "critic_optimizer = optax.adam(learning_rate=100)\n",
    "critic_opt_state = critic_optimizer.init(critic_params)\n",
    "\n",
    "# Initialize the training state for flax porpuses\n",
    "critic = train_state.TrainState.create(\n",
    "    apply_fn=Critic().apply,\n",
    "    params=critic_params,\n",
    "    tx=critic_optimizer\n",
    ")\n",
    "\n",
    "actor = train_state.TrainState.create(\n",
    "    apply_fn=Actor().apply,\n",
    "    params=actor_params,\n",
    "    tx=actor_optimizer\n",
    ")\n",
    "\n",
    "print(Actor().tabulate(key, (1, state_dim) ))\n",
    "print(Critic().tabulate(key, jnp.ones((1,action_dim)), jnp.ones((1,state_dim))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4fede34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize target network Q_0_target and μ_0_target with weights θ_Q_target ← θ_Q , θ_μ_target ← θ_μ\n",
    "target_critic_params = critic_params\n",
    "target_actor_params = actor_params\n",
    "\n",
    "# Initialize the training state for flax porpuses\n",
    "target_critic = train_state.TrainState.create(\n",
    "    apply_fn=Critic().apply,\n",
    "    params=target_critic_params,\n",
    "    tx=critic_optimizer\n",
    ")\n",
    "\n",
    "target_actor = train_state.TrainState.create(\n",
    "    apply_fn=Actor().apply,\n",
    "    params=target_actor_params,\n",
    "    tx=actor_optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80f4b370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize replay buffer R\n",
    "buffer_size = 100000\n",
    "batch_size = 10\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, buffer_size, batch_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer = deque(maxlen=self.buffer_size)\n",
    "\n",
    "    def add(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample_batch(self):\n",
    "        indices = jax.random.choice(key, len(self.buffer), shape=(self.batch_size,), replace=True)\n",
    "        batch = [self.buffer[i] for i in indices]\n",
    "\n",
    "        return zip(*batch)\n",
    "    \n",
    "buffer = ReplayBuffer(buffer_size, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "670e041a",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = 1\n",
    "T = 2\n",
    "gamma = 0.1\n",
    "tau = 0.001\n",
    "\n",
    "for i in range(episode):\n",
    "    # Initialize a random process N for action exploration\n",
    "    N = noise(0.1)\n",
    "    # Receive initial observation state s 1\n",
    "    state, info = env.reset(seed=seed)\n",
    "\n",
    "    for t in range(T):\n",
    "\n",
    "        # Select action a_t = μ(s t |θ μ ) + N t according to the current policy and exploration noise\n",
    "        action = noise() + actor.apply_fn(actor.params, state)\n",
    "\n",
    "        # Execute action a t and observe reward r t and observe new state s t+1\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # Store transition (s t , a t , r t , s t+1 ) in R\n",
    "        transition = (state, action, reward, observation)\n",
    "\n",
    "        # buffer.append(transition)\n",
    "        buffer.add(transition)\n",
    "\n",
    "        # Sample a random minibatch of N transitions (s i , a i , r i , s i+1 ) from R\n",
    "        states, actions, rewards, next_states = buffer.sample_batch()\n",
    "\n",
    "        # Set y = r  + γQ^0 (s_{i+1} , μ^0 (s_{i+1} |θ^μ )|θ^Q ) P\n",
    "        target_action = target_actor.apply_fn(target_actor_params, next_states)\n",
    "        target_q = target_critic.apply_fn(target_critic_params, jnp.asarray(next_states), jnp.asarray(target_action))\n",
    "\n",
    "        y = reward + gamma * (1 - terminated) * target_q\n",
    "\n",
    "        # Update critic by minimizing the loss\n",
    "        critic = update_critic(critic, jnp.asarray(states), jnp.asarray(actions), jnp.asarray(y))\n",
    "\n",
    "        # Update the actor policy using the sampled gradient:\n",
    "        actor = update_actor(actor, jnp.asarray(states))\n",
    "\n",
    "        # Update the target networks:\n",
    "        target_actor_params = soft_update(target_actor_params, actor.params, tau)\n",
    "        target_critic_params = soft_update(target_critic_params, critic.params, tau)\n",
    "\n",
    "        state = observation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5c8e8a",
   "metadata": {},
   "source": [
    "## Stable baselines training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cb3097a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m action_noise \u001b[39m=\u001b[39m NormalActionNoise(mean\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mzeros(n_actions), sigma\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mones(n_actions))\n\u001b[1;32m     10\u001b[0m model \u001b[39m=\u001b[39m DDPG(\u001b[39m\"\u001b[39m\u001b[39mMlpPolicy\u001b[39m\u001b[39m\"\u001b[39m, env, action_noise\u001b[39m=\u001b[39maction_noise, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m10000\u001b[39;49m, log_interval\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[1;32m     12\u001b[0m model\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mddpg_MountainCarContinuous\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m vec_env \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mget_env()\n",
      "File \u001b[0;32m~/repos/notebooks/venv/lib/python3.10/site-packages/stable_baselines3/ddpg/ddpg.py:123\u001b[0m, in \u001b[0;36mDDPG.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    115\u001b[0m     \u001b[39mself\u001b[39m: SelfDDPG,\n\u001b[1;32m    116\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    122\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfDDPG:\n\u001b[0;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    124\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    125\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    126\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    127\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    128\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    129\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    130\u001b[0m     )\n",
      "File \u001b[0;32m~/repos/notebooks/venv/lib/python3.10/site-packages/stable_baselines3/td3/td3.py:222\u001b[0m, in \u001b[0;36mTD3.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    214\u001b[0m     \u001b[39mself\u001b[39m: SelfTD3,\n\u001b[1;32m    215\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    221\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfTD3:\n\u001b[0;32m--> 222\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    223\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    224\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    225\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    226\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    227\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    228\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    229\u001b[0m     )\n",
      "File \u001b[0;32m~/repos/notebooks/venv/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py:331\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[39m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[1;32m    330\u001b[0m         \u001b[39mif\u001b[39;00m gradient_steps \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 331\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(batch_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size, gradient_steps\u001b[39m=\u001b[39;49mgradient_steps)\n\u001b[1;32m    333\u001b[0m callback\u001b[39m.\u001b[39mon_training_end()\n\u001b[1;32m    335\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/repos/notebooks/venv/lib/python3.10/site-packages/stable_baselines3/td3/td3.py:202\u001b[0m, in \u001b[0;36mTD3.train\u001b[0;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[1;32m    199\u001b[0m actor_loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m    200\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m--> 202\u001b[0m polyak_update(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcritic\u001b[39m.\u001b[39;49mparameters(), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcritic_target\u001b[39m.\u001b[39;49mparameters(), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtau)\n\u001b[1;32m    203\u001b[0m polyak_update(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor\u001b[39m.\u001b[39mparameters(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor_target\u001b[39m.\u001b[39mparameters(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtau)\n\u001b[1;32m    204\u001b[0m \u001b[39m# Copy running stats, see GH issue #996\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/notebooks/venv/lib/python3.10/site-packages/stable_baselines3/common/utils.py:469\u001b[0m, in \u001b[0;36mpolyak_update\u001b[0;34m(params, target_params, tau)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[39mPerform a Polyak average update on ``target_params`` using ``params``:\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[39mtarget parameters are slowly updated towards the main parameters.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[39m:param tau: the soft update coefficient (\"Polyak update\", between 0 and 1)\u001b[39;00m\n\u001b[1;32m    466\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[39mwith\u001b[39;00m th\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m    468\u001b[0m     \u001b[39m# zip does not raise an exception if length of parameters does not match.\u001b[39;00m\n\u001b[0;32m--> 469\u001b[0m     \u001b[39mfor\u001b[39;00m param, target_param \u001b[39min\u001b[39;00m zip_strict(params, target_params):\n\u001b[1;32m    470\u001b[0m         target_param\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mmul_(\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m tau)\n\u001b[1;32m    471\u001b[0m         th\u001b[39m.\u001b[39madd(target_param\u001b[39m.\u001b[39mdata, param\u001b[39m.\u001b[39mdata, alpha\u001b[39m=\u001b[39mtau, out\u001b[39m=\u001b[39mtarget_param\u001b[39m.\u001b[39mdata)\n",
      "File \u001b[0;32m~/repos/notebooks/venv/lib/python3.10/site-packages/stable_baselines3/common/utils.py:442\u001b[0m, in \u001b[0;36mzip_strict\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    440\u001b[0m sentinel \u001b[39m=\u001b[39m \u001b[39mobject\u001b[39m()\n\u001b[1;32m    441\u001b[0m \u001b[39mfor\u001b[39;00m combo \u001b[39min\u001b[39;00m zip_longest(\u001b[39m*\u001b[39miterables, fillvalue\u001b[39m=\u001b[39msentinel):\n\u001b[0;32m--> 442\u001b[0m     \u001b[39mif\u001b[39;00m sentinel \u001b[39min\u001b[39;00m combo:\n\u001b[1;32m    443\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mIterables have different lengths\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    444\u001b[0m     \u001b[39myield\u001b[39;00m combo\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "import numpy as np\n",
    "\n",
    "# The noise objects for DDPG\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "model = DDPG(\"MlpPolicy\", env, action_noise=action_noise, verbose=1)\n",
    "model.learn(total_timesteps=10000, log_interval=10)\n",
    "model.save(\"ddpg_MountainCarContinuous\")\n",
    "vec_env = model.get_env()\n",
    "\n",
    "del model # remove to demonstrate saving and loading\n",
    "\n",
    "model = DDPG.load(\"ddpg_MountainCarContinuous\")\n",
    "\n",
    "obs = vec_env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    env.render(\"human\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
