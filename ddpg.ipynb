{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e524b141",
   "metadata": {},
   "source": [
    "# DDPG algorithm in flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59b15bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from flax.training import train_state\n",
    "import jax.tree_util as jtu\n",
    "from jax import random\n",
    "from flax import linen as nn  # Linen API\n",
    "import optax\n",
    "from collections import deque\n",
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"MountainCarContinuous-v0\")\n",
    "seed = 0\n",
    "key = random.PRNGKey(seed)\n",
    "\n",
    "F_CPP_MIN_LOG_LEVEL=0\n",
    "action_dim = env.action_space.shape[0]\n",
    "state_dim = env.observation_space.shape[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ec141b2",
   "metadata": {},
   "source": [
    "## Create the actor and critic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2713383",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the actor and critic newtorks like multilayer perceptrons\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"critic model MLP\"\"\"\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, observations, actions):\n",
    "        x = jnp.concatenate([observations, actions], axis=-1)\n",
    "        x = nn.Dense(features=256)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=256)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=1)(x)\n",
    "        return jnp.squeeze(x, axis=-1)\n",
    "    \n",
    "class Actor(nn.Module):\n",
    "    \"\"\"actor model MLP\"\"\"\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(features=256)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=256)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=action_dim)(x)\n",
    "        x = nn.tanh(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b25a9a29",
   "metadata": {},
   "source": [
    "## Create necessary methods to the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a764cb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random process N for action exploration\n",
    "def noise(noise_scale=0.1, key=key, action_dim=action_dim):\n",
    "    return noise_scale * jax.random.normal(key, (action_dim,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca06c72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the method to update model parameters\n",
    "\n",
    "# update critic\n",
    "@jax.jit\n",
    "def update_critic(model, states, actions, y):\n",
    "    def compute_critic_loss(params):\n",
    "        Q = model.apply_fn(params, states, actions)\n",
    "        return jnp.mean((Q - y)**2) #compute loss\n",
    "    \n",
    "    grad_fn = jax.grad(compute_critic_loss)\n",
    "    grads = grad_fn(model.params)\n",
    "    updated_model = model.apply_gradients(grads=grads)\n",
    "    return updated_model\n",
    "\n",
    "# udate actor\n",
    "@jax.jit\n",
    "def update_actor(model, states):\n",
    "    def compute_actor_loss(params):\n",
    "        actions = model.apply_fn(params, states)\n",
    "        return -jnp.mean(actions)  # Compute the actor loss\n",
    "    \n",
    "    grad_fn = jax.grad(compute_actor_loss)\n",
    "    grads = grad_fn(model.params)\n",
    "    updated_model = model.apply_gradients(grads=grads)\n",
    "    return updated_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc39eef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the soft update function\n",
    "@jax.jit\n",
    "def soft_update(target_params, source_params, tau):\n",
    "    # Convert the source_params to a JAX-compatible data structure\n",
    "    source_params_tree = jtu.tree_map(lambda x: jnp.asarray(x), source_params)\n",
    "    target_params_tree = jtu.tree_map(lambda x: jnp.asarray(x), target_params)\n",
    "\n",
    "\n",
    "    # Compute the updated target parameters using a soft update\n",
    "    updated_params = jtu.tree_map(lambda x, y: tau * x + (1 - tau) * y, source_params_tree, target_params_tree)\n",
    "\n",
    "    return updated_params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc9e8513",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af776785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[3m                                Actor Summary                                \u001b[0m\n",
      "┏━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mpath   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmodule\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1minputs      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1moutputs     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mparams                  \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│         │ Actor  │ - 1          │ \u001b[2mfloat32\u001b[0m[1]   │                          │\n",
      "│         │        │ - 2          │              │                          │\n",
      "├─────────┼────────┼──────────────┼──────────────┼──────────────────────────┤\n",
      "│ Dense_0 │ Dense  │ - 1          │ \u001b[2mfloat32\u001b[0m[256] │ bias: \u001b[2mfloat32\u001b[0m[256]       │\n",
      "│         │        │ - 2          │              │ kernel: \u001b[2mfloat32\u001b[0m[2,256]   │\n",
      "│         │        │              │              │                          │\n",
      "│         │        │              │              │ \u001b[1m768 \u001b[0m\u001b[1;2m(3.1 KB)\u001b[0m             │\n",
      "├─────────┼────────┼──────────────┼──────────────┼──────────────────────────┤\n",
      "│ Dense_1 │ Dense  │ \u001b[2mfloat32\u001b[0m[256] │ \u001b[2mfloat32\u001b[0m[256] │ bias: \u001b[2mfloat32\u001b[0m[256]       │\n",
      "│         │        │              │              │ kernel: \u001b[2mfloat32\u001b[0m[256,256] │\n",
      "│         │        │              │              │                          │\n",
      "│         │        │              │              │ \u001b[1m65,792 \u001b[0m\u001b[1;2m(263.2 KB)\u001b[0m        │\n",
      "├─────────┼────────┼──────────────┼──────────────┼──────────────────────────┤\n",
      "│ Dense_2 │ Dense  │ \u001b[2mfloat32\u001b[0m[256] │ \u001b[2mfloat32\u001b[0m[1]   │ bias: \u001b[2mfloat32\u001b[0m[1]         │\n",
      "│         │        │              │              │ kernel: \u001b[2mfloat32\u001b[0m[256,1]   │\n",
      "│         │        │              │              │                          │\n",
      "│         │        │              │              │ \u001b[1m257 \u001b[0m\u001b[1;2m(1.0 KB)\u001b[0m             │\n",
      "├─────────┼────────┼──────────────┼──────────────┼──────────────────────────┤\n",
      "│\u001b[1m \u001b[0m\u001b[1m       \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m      \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m            \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m       Total\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m66,817 \u001b[0m\u001b[1;2m(267.3 KB)\u001b[0m\u001b[1m       \u001b[0m\u001b[1m \u001b[0m│\n",
      "└─────────┴────────┴──────────────┴──────────────┴──────────────────────────┘\n",
      "\u001b[1m                                                                             \u001b[0m\n",
      "\u001b[1m                     Total Parameters: 66,817 \u001b[0m\u001b[1;2m(267.3 KB)\u001b[0m\u001b[1m                     \u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\u001b[3m                                 Critic Summary                                 \u001b[0m\n",
      "┏━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mpath   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmodule\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1minputs        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1moutputs       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mparams                 \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│         │ Critic │ - \u001b[2mfloat32\u001b[0m[1,1] │ \u001b[2mfloat32\u001b[0m[1]     │                         │\n",
      "│         │        │ - \u001b[2mfloat32\u001b[0m[1,2] │                │                         │\n",
      "├─────────┼────────┼────────────────┼────────────────┼─────────────────────────┤\n",
      "│ Dense_0 │ Dense  │ \u001b[2mfloat32\u001b[0m[1,3]   │ \u001b[2mfloat32\u001b[0m[1,256] │ bias: \u001b[2mfloat32\u001b[0m[256]      │\n",
      "│         │        │                │                │ kernel: \u001b[2mfloat32\u001b[0m[3,256]  │\n",
      "│         │        │                │                │                         │\n",
      "│         │        │                │                │ \u001b[1m1,024 \u001b[0m\u001b[1;2m(4.1 KB)\u001b[0m          │\n",
      "├─────────┼────────┼────────────────┼────────────────┼─────────────────────────┤\n",
      "│ Dense_1 │ Dense  │ \u001b[2mfloat32\u001b[0m[1,256] │ \u001b[2mfloat32\u001b[0m[1,256] │ bias: \u001b[2mfloat32\u001b[0m[256]      │\n",
      "│         │        │                │                │ kernel:                 │\n",
      "│         │        │                │                │ \u001b[2mfloat32\u001b[0m[256,256]        │\n",
      "│         │        │                │                │                         │\n",
      "│         │        │                │                │ \u001b[1m65,792 \u001b[0m\u001b[1;2m(263.2 KB)\u001b[0m       │\n",
      "├─────────┼────────┼────────────────┼────────────────┼─────────────────────────┤\n",
      "│ Dense_2 │ Dense  │ \u001b[2mfloat32\u001b[0m[1,256] │ \u001b[2mfloat32\u001b[0m[1,1]   │ bias: \u001b[2mfloat32\u001b[0m[1]        │\n",
      "│         │        │                │                │ kernel: \u001b[2mfloat32\u001b[0m[256,1]  │\n",
      "│         │        │                │                │                         │\n",
      "│         │        │                │                │ \u001b[1m257 \u001b[0m\u001b[1;2m(1.0 KB)\u001b[0m            │\n",
      "├─────────┼────────┼────────────────┼────────────────┼─────────────────────────┤\n",
      "│\u001b[1m \u001b[0m\u001b[1m       \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m      \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m              \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m         Total\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m67,073 \u001b[0m\u001b[1;2m(268.3 KB)\u001b[0m\u001b[1m      \u001b[0m\u001b[1m \u001b[0m│\n",
      "└─────────┴────────┴────────────────┴────────────────┴─────────────────────────┘\n",
      "\u001b[1m                                                                                \u001b[0m\n",
      "\u001b[1m                      Total Parameters: 67,073 \u001b[0m\u001b[1;2m(268.3 KB)\u001b[0m\u001b[1m                       \u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Randomly initialize critic network Q(s, a|θ_Q ) and actor μ(s|θ_μ ) with weights θ_Q and θ_μ .\n",
    "critic_params = Critic().init(key, jnp.zeros((1,action_dim)), jnp.zeros((1,state_dim)))\n",
    "actor_params = Actor().init(key, jnp.zeros((1, state_dim)))\n",
    "\n",
    "# define optimizers\n",
    "actor_optimizer = optax.adam(learning_rate=100)\n",
    "actor_opt_state = actor_optimizer.init(actor_params)\n",
    "\n",
    "critic_optimizer = optax.adam(learning_rate=100)\n",
    "critic_opt_state = critic_optimizer.init(critic_params)\n",
    "\n",
    "# Initialize the training state for flax porpuses\n",
    "critic = train_state.TrainState.create(\n",
    "    apply_fn=Critic().apply,\n",
    "    params=critic_params,\n",
    "    tx=critic_optimizer\n",
    ")\n",
    "\n",
    "actor = train_state.TrainState.create(\n",
    "    apply_fn=Actor().apply,\n",
    "    params=actor_params,\n",
    "    tx=actor_optimizer\n",
    ")\n",
    "\n",
    "print(Actor().tabulate(key, (1, state_dim) ))\n",
    "print(Critic().tabulate(key, jnp.ones((1,action_dim)), jnp.ones((1,state_dim))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4fede34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize target network Q_0_target and μ_0_target with weights θ_Q_target ← θ_Q , θ_μ_target ← θ_μ\n",
    "target_critic_params = critic_params\n",
    "target_actor_params = actor_params\n",
    "\n",
    "# Initialize the training state for flax porpuses\n",
    "target_critic = train_state.TrainState.create(\n",
    "    apply_fn=Critic().apply,\n",
    "    params=target_critic_params,\n",
    "    tx=critic_optimizer\n",
    ")\n",
    "\n",
    "target_actor = train_state.TrainState.create(\n",
    "    apply_fn=Actor().apply,\n",
    "    params=target_actor_params,\n",
    "    tx=actor_optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80f4b370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize replay buffer R\n",
    "buffer_size = 100000\n",
    "batch_size = 10\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, buffer_size, batch_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer = deque(maxlen=self.buffer_size)\n",
    "\n",
    "    def add(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample_batch(self):\n",
    "        indices = jax.random.choice(key, len(self.buffer), shape=(self.batch_size,), replace=True)\n",
    "        batch = [self.buffer[i] for i in indices]\n",
    "\n",
    "        return zip(*batch)\n",
    "    \n",
    "buffer = ReplayBuffer(buffer_size, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "670e041a",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = 1\n",
    "T = 2\n",
    "gamma = 0.1\n",
    "tau = 0.001\n",
    "\n",
    "for i in range(episode):\n",
    "    # Initialize a random process N for action exploration\n",
    "    N = noise(0.1)\n",
    "    # Receive initial observation state s 1\n",
    "    state, info = env.reset(seed=seed)\n",
    "\n",
    "    for t in range(T):\n",
    "\n",
    "        # Select action a_t = μ(s t |θ μ ) + N t according to the current policy and exploration noise\n",
    "        action = noise() + actor.apply_fn(actor.params, state)\n",
    "\n",
    "        # Execute action a t and observe reward r t and observe new state s t+1\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # Store transition (s t , a t , r t , s t+1 ) in R\n",
    "        transition = (state, action, reward, observation)\n",
    "\n",
    "        # buffer.append(transition)\n",
    "        buffer.add(transition)\n",
    "\n",
    "        # Sample a random minibatch of N transitions (s i , a i , r i , s i+1 ) from R\n",
    "        states, actions, rewards, next_states = buffer.sample_batch()\n",
    "\n",
    "        # Set y = r  + γQ^0 (s_{i+1} , μ^0 (s_{i+1} |θ^μ )|θ^Q ) P\n",
    "        target_action = target_actor.apply_fn(target_actor_params, next_states)\n",
    "        target_q = target_critic.apply_fn(target_critic_params, jnp.asarray(next_states), jnp.asarray(target_action))\n",
    "\n",
    "        y = reward + gamma * (1 - terminated) * target_q\n",
    "\n",
    "        # Update critic by minimizing the loss\n",
    "        critic = update_critic(critic, jnp.asarray(states), jnp.asarray(actions), jnp.asarray(y))\n",
    "\n",
    "        # Update the actor policy using the sampled gradient:\n",
    "        actor = update_actor(actor, jnp.asarray(states))\n",
    "\n",
    "        # Update the target networks:\n",
    "        target_actor_params = soft_update(target_actor_params, actor.params, tau)\n",
    "        target_critic_params = soft_update(target_critic_params, critic.params, tau)\n",
    "\n",
    "        state = observation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
