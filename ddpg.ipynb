{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81a77f0c-f949-4d14-8bf6-86c28a897c77",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# DDPG algorithm in Jax&Flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7bb8bac-c5ac-4072-88c0-a124a2151112",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "\n",
    "import jax.tree_util as jtu\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "from flax.training import train_state, orbax_utils\n",
    "from flax import linen as nn  # Linen API\n",
    "\n",
    "from tqdm import tqdm\n",
    "import orbax.checkpoint\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "F_CPP_MIN_LOG_LEVEL=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5691d42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ckpt_dir = './agent' # create the agent folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25a9a29",
   "metadata": {},
   "source": [
    "## Usefull Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a764cb53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#random process N for action exploration \n",
    "class OUActionNoise:\n",
    "    def __init__(self, key, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * jnp.sqrt(self.dt) * jax.random.normal(key, shape=self.mean.shape)\n",
    "        )\n",
    "\n",
    "        self.x_prev = x\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = jnp.zeros_like(self.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca06c72c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the method to update model parameters\n",
    "\n",
    "# update critic\n",
    "@jax.jit\n",
    "def update_critic(model, states, actions, y):\n",
    "    def compute_critic_loss(params):\n",
    "        Q = model.apply_fn(params, states, actions)\n",
    "        \n",
    "        return jnp.mean((Q - y)**2) #compute loss\n",
    "    \n",
    "    loss, grads = jax.value_and_grad(compute_critic_loss)(model.params)\n",
    "    updated_model = model.apply_gradients(grads=grads)\n",
    "   \n",
    "    return updated_model, loss\n",
    "\n",
    "# udate actor\n",
    "@jtu.Partial(jax.jit, static_argnums=(2,))\n",
    "def update_actor(model, critic, states):\n",
    "    def compute_actor_loss(params):\n",
    "        actions = model.apply_fn(params, states)\n",
    "        \n",
    "        Q = critic.apply_fn(critic.params, states, actions)\n",
    "\n",
    "        return -jnp.mean(Q)  # Compute the actor loss\n",
    "\n",
    "    loss, grads = jax.value_and_grad(compute_actor_loss)(model.params)\n",
    "    updated_model = model.apply_gradients(grads=grads)\n",
    "\n",
    "    return updated_model, loss\n",
    "\n",
    "# Define the soft update function\n",
    "@jax.jit\n",
    "def soft_update(target_params, source_params, tau):\n",
    "    # Convert the source_params to a JAX-compatible data structure\n",
    "    source_params_tree = jtu.tree_map(lambda x: jnp.asarray(x), source_params)\n",
    "    target_params_tree = jtu.tree_map(lambda x: jnp.asarray(x), target_params)\n",
    "\n",
    "    # Compute the updated target parameters using a soft update\n",
    "    updated_params = jtu.tree_map(lambda x, y: tau * x + (1 - tau) * y,\n",
    "                                  source_params_tree, target_params_tree)\n",
    "\n",
    "    return updated_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bb8988-b86d-4a4a-b5d5-16bc53c509e7",
   "metadata": {},
   "source": [
    "## Define ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b1d002d-136a-4677-bc5e-6a29b376062b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the replay buffer\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, buffer_size, batch_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer = deque(maxlen=self.buffer_size)\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "    def add(self, transition):\n",
    "        if len(self.buffer) >= self.buffer_size:\n",
    "            self.buffer.pop(0)\n",
    "            \n",
    "        self.buffer.append(transition)\n",
    "        \n",
    "        self.buffer_counter += 1\n",
    "\n",
    "    #TODO: finish to update fix the batching\n",
    "    def sample_batch(self, key):\n",
    "        record_range = min(self.buffer_counter, self.buffer_size)\n",
    "        \n",
    "        # if record_range < self.batch_size:\n",
    "        #     print(len(self.buffer))\n",
    "        #     raise ValueError(\"Replay buffer is too small to sample.\")\n",
    "            \n",
    "        indices = jax.random.choice(\n",
    "            key, \n",
    "            record_range,\n",
    "            shape=(self.batch_size,), replace=True\n",
    "        )\n",
    "        \n",
    "        batch = [self.buffer[i] for i in indices]\n",
    "        \n",
    "        return zip(*batch)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a71636",
   "metadata": {},
   "source": [
    "## Define actor and critic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44c7fe1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create the actor and critic newtorks like multilayer perceptrons\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"critic model MLP\"\"\"\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, observations, actions):\n",
    "        x = jnp.concatenate([observations, actions], axis=-1)\n",
    "        x = nn.Dense(features=256)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=256)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=1)(x)\n",
    "        return jnp.squeeze(x, axis=-1)\n",
    "    \n",
    "class Actor(nn.Module):\n",
    "    \"\"\"actor model MLP\"\"\"\n",
    "    action_dim: int\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(features=256)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=256)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=action_dim)(x)\n",
    "        x = nn.tanh(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9e8513",
   "metadata": {},
   "source": [
    "## Define algorithm parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "618e7ec1-2ec7-4230-b34d-e693c7bea28b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#define key\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "key = jax.random.PRNGKey(seed)\n",
    "\n",
    "key, actor_key, critic_key = jax.random.split(key, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d0967cc-3733-4ef2-9b3c-6fc7e8694a75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define environment and parameters\n",
    "env = gym.make(\"InvertedPendulum-v4\")\n",
    "# env  = gym.make(\"LunarLander-v2\", continuous=True)\n",
    "action_dim = env.action_space.shape[0]\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n",
    "#initialize parameters\n",
    "episodes = 100\n",
    "gamma = 0.99 #discount factor 0:nearly rewards, 1:future rewards\n",
    "tau = 0.001 #polyak between 0-1 updating target network\n",
    "max_episode_steps = 1000\n",
    "buffer_size = int(1e6) #memory size\n",
    "batch_size = 64 #The number of experiences sampled from the replay buffer\n",
    "actor_learning_rate = 1e-3\n",
    "critic_learning_rate = 1e-4\n",
    "std_dev = 0.2  #scale of the noise for random process N for action exploration\n",
    "noise = OUActionNoise(key, mean=jnp.zeros(1), std_deviation=float(std_dev) * jnp.ones(1))\n",
    "\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=max_episode_steps)\n",
    "\n",
    "if os.path.exists(ckpt_dir):\n",
    "    shutil.rmtree(ckpt_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff5afde-d37d-4a65-8af7-316be0f76dbf",
   "metadata": {},
   "source": [
    "## Initialize models and buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af776785",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Randomly initialize critic network Q(s, a|θ_Q ) and actor μ(s|θ_μ ) with weights θ_Q and θ_μ .\n",
    "obs, _ = env.reset();\n",
    "\n",
    "# Initialize the training state actor and critic models\n",
    "actor_model = Actor(action_dim=env.action_space.shape)\n",
    "\n",
    "critic = train_state.TrainState.create(\n",
    "    apply_fn=Critic().apply,\n",
    "    params=Critic().init(critic_key, obs, env.action_space.sample()), #init critic parameters\n",
    "    tx=optax.adamw(learning_rate=critic_learning_rate, weight_decay=1e-2) #define optimizer\n",
    ")\n",
    "\n",
    "actor = train_state.TrainState.create(\n",
    "    apply_fn=actor_model.apply,\n",
    "    params=actor_model.init(actor_key, obs), #init actor parameters\n",
    "    tx=optax.adam(learning_rate=actor_learning_rate) #define optimizer\n",
    ")\n",
    "\n",
    "# to save agent\n",
    "config = {'dimensions': jnp.array([5,3]), 'name': 'actor'}\n",
    "ckpt = {'model': actor, 'config': config, 'data': actor.params}\n",
    "\n",
    "# print(Actor().tabulate(key, obs, action_dim))\n",
    "# print(Critic().tabulate(key, obs, env.action_space.sample()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b4fede34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize target network Q_0_target and μ_0_target with weights \n",
    "# θ_Q_target ← θ_Q , θ_μ_target ← θ_μ\n",
    "\n",
    "# Initialize the training state for flax porpuses\n",
    "target_critic = train_state.TrainState.create(\n",
    "    apply_fn=Critic().apply,\n",
    "    params=Critic().init(critic_key, obs, env.action_space.sample()),\n",
    "    tx=optax.adamw(learning_rate=critic_learning_rate, weight_decay=1e-2)\n",
    ")\n",
    "\n",
    "target_actor = train_state.TrainState.create(\n",
    "    apply_fn=actor_model.apply,\n",
    "    params=actor_model.init(actor_key, obs),\n",
    "    tx=optax.adam(learning_rate=actor_learning_rate)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80f4b370",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize replay buffer R\n",
    "buffer = ReplayBuffer(buffer_size, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0dbd9c-a50a-477b-b860-f7a39d12ac9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "670e041a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Non-hashable static arguments are not supported. An error occurred during a call to 'update_actor' while trying to hash an object of type <class 'jaxlib.xla_extension.ArrayImpl'>, [[-0.00898329 -0.00437226  0.00984377  0.0093159 ]\n [ 0.00955733 -0.00139923  0.00882936 -0.00520764]\n [ 0.00647788  0.00697272  0.00098062 -0.0098719 ]\n [ 0.0045059  -0.00414265  0.00838278  0.0053087 ]\n [ 0.00955733 -0.00139923  0.00882936 -0.00520764]\n [-0.00525912 -0.00863321  0.00573321  0.00934029]\n [ 0.0045059  -0.00414265  0.00838278  0.0053087 ]\n [ 0.00955733 -0.00139923  0.00882936 -0.00520764]\n [-0.00525912 -0.00863321  0.00573321  0.00934029]\n [-0.00680077 -0.00235379 -0.00993035  0.00454693]\n [ 0.0045059  -0.00414265  0.00838278  0.0053087 ]\n [-0.00680077 -0.00235379 -0.00993035  0.00454693]\n [ 0.00647788  0.00697272  0.00098062 -0.0098719 ]\n [-0.00525912 -0.00863321  0.00573321  0.00934029]\n [ 0.00647788  0.00697272  0.00098062 -0.0098719 ]\n [ 0.00955733 -0.00139923  0.00882936 -0.00520764]\n [-0.00898329 -0.00437226  0.00984377  0.0093159 ]\n [ 0.0045059  -0.00414265  0.00838278  0.0053087 ]\n [-0.00898329 -0.00437226  0.00984377  0.0093159 ]\n [-0.00525912 -0.00863321  0.00573321  0.00934029]\n [-0.00525912 -0.00863321  0.00573321  0.00934029]\n [ 0.00955733 -0.00139923  0.00882936 -0.00520764]\n [ 0.00647788  0.00697272  0.00098062 -0.0098719 ]\n [ 0.00955733 -0.00139923  0.00882936 -0.00520764]\n [-0.00680077 -0.00235379 -0.00993035  0.00454693]\n [-0.00898329 -0.00437226  0.00984377  0.0093159 ]\n [-0.00680077 -0.00235379 -0.00993035  0.00454693]\n [-0.00525912 -0.00863321  0.00573321  0.00934029]\n [-0.00898329 -0.00437226  0.00984377  0.0093159 ]\n [-0.00898329 -0.00437226  0.00984377  0.0093159 ]\n [ 0.00647788  0.00697272  0.00098062 -0.0098719 ]\n [ 0.00955733 -0.00139923  0.00882936 -0.00520764]\n [ 0.00647788  0.00697272  0.00098062 -0.0098719 ]\n [-0.00525912 -0.00863321  0.00573321  0.00934029]\n [-0.00680077 -0.00235379 -0.00993035  0.00454693]\n [-0.00680077 -0.00235379 -0.00993035  0.00454693]\n [-0.00680077 -0.00235379 -0.00993035  0.00454693]\n [ 0.00647788  0.00697272  0.00098062 -0.0098719 ]\n [ 0.00955733 -0.00139923  0.00882936 -0.00520764]\n [ 0.0045059  -0.00414265  0.00838278  0.0053087 ]\n [ 0.0045059  -0.00414265  0.00838278  0.0053087 ]\n [-0.00898329 -0.00437226  0.00984377  0.0093159 ]\n [-0.00898329 -0.00437226  0.00984377  0.0093159 ]\n [ 0.00647788  0.00697272  0.00098062 -0.0098719 ]\n [ 0.0045059  -0.00414265  0.00838278  0.0053087 ]\n [-0.00525912 -0.00863321  0.00573321  0.00934029]\n [ 0.00647788  0.00697272  0.00098062 -0.0098719 ]\n [-0.00525912 -0.00863321  0.00573321  0.00934029]\n [-0.00898329 -0.00437226  0.00984377  0.0093159 ]\n [-0.00525912 -0.00863321  0.00573321  0.00934029]\n [-0.00680077 -0.00235379 -0.00993035  0.00454693]\n [ 0.0045059  -0.00414265  0.00838278  0.0053087 ]\n [ 0.00955733 -0.00139923  0.00882936 -0.00520764]\n [ 0.0045059  -0.00414265  0.00838278  0.0053087 ]\n [ 0.0045059  -0.00414265  0.00838278  0.0053087 ]\n [ 0.00647788  0.00697272  0.00098062 -0.0098719 ]\n [ 0.00955733 -0.00139923  0.00882936 -0.00520764]\n [ 0.0045059  -0.00414265  0.00838278  0.0053087 ]\n [-0.00525912 -0.00863321  0.00573321  0.00934029]\n [ 0.00647788  0.00697272  0.00098062 -0.0098719 ]\n [-0.00680077 -0.00235379 -0.00993035  0.00454693]\n [-0.00525912 -0.00863321  0.00573321  0.00934029]\n [ 0.0045059  -0.00414265  0.00838278  0.0053087 ]\n [ 0.00955733 -0.00139923  0.00882936 -0.00520764]]. The error was:\nTypeError: unhashable type: 'ArrayImpl'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 53\u001b[0m\n\u001b[1;32m     47\u001b[0m critic, critic_loss \u001b[38;5;241m=\u001b[39m update_critic(critic,\n\u001b[1;32m     48\u001b[0m                                     jnp\u001b[38;5;241m.\u001b[39masarray(states),\n\u001b[1;32m     49\u001b[0m                                     jnp\u001b[38;5;241m.\u001b[39masarray(actions),\n\u001b[1;32m     50\u001b[0m                                     jnp\u001b[38;5;241m.\u001b[39masarray(y))\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Update the actor policy using the sampled gradient:\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m actor, actor_loss \u001b[38;5;241m=\u001b[39m \u001b[43mupdate_actor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mcritic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Update the target networks:\u001b[39;00m\n\u001b[1;32m     58\u001b[0m target_actor_params \u001b[38;5;241m=\u001b[39m soft_update(target_actor\u001b[38;5;241m.\u001b[39mparams, actor\u001b[38;5;241m.\u001b[39mparams, tau)\n",
      "\u001b[0;31mValueError\u001b[0m: Non-hashable static arguments are not supported. An error occurred during a call to 'update_actor' while trying to hash an object of type <class 'jaxlib.xla_extension.ArrayImpl'>, [[-0.00898329 -0.00437226  0.00984377  0.0093159 ]\n [ 0.00955733 -0.00139923  0.00882936 -0.00520764]\n [ 0.00647788  0.00697272  0.00098062 -0.0098719 ]\n [ 0.0045059  -0.00414265  0.00838278  0.0053087 ]\n [ 0.00955733 -0.00139923  0.00882936 -0.00520764]\n [-0.00525912 -0.00863321  0.00573321  0.00934029]\n [ 0.0045059  -0.00414265  0.00838278  0.0053087 ]\n [ 0.00955733 -0.00139923  0.00882936 -0.00520764]\n [-0.00525912 -0.00863321  0.00573321  0.00934029]\n [-0.00680077 -0.00235379 -0.00993035  0.00454693]\n [ 0.0045059  -0.00414265  0.00838278  0.0053087 ]\n [-0.00680077 -0.00235379 -0.00993035  0.00454693]\n [ 0.00647788  0.00697272  0.00098062 -0.0098719 ]\n [-0.00525912 -0.00863321  0.00573321  0.00934029]\n [ 0.00647788  0.00697272  0.00098062 -0.0098719 ]\n [ 0.00955733 -0.00139923  0.00882936 -0.00520764]\n [-0.00898329 -0.00437226  0.00984377  0.0093159 ]\n [ 0.0045059  -0.00414265  0.00838278  0.0053087 ]\n [-0.00898329 -0.00437226  0.00984377  0.0093159 ]\n [-0.00525912 -0.00863321  0.00573321  0.00934029]\n [-0.00525912 -0.00863321  0.00573321  0.00934029]\n [ 0.00955733 -0.00139923  0.00882936 -0.00520764]\n [ 0.00647788  0.00697272  0.00098062 -0.0098719 ]\n [ 0.00955733 -0.00139923  0.00882936 -0.00520764]\n [-0.00680077 -0.00235379 -0.00993035  0.00454693]\n [-0.00898329 -0.00437226  0.00984377  0.0093159 ]\n [-0.00680077 -0.00235379 -0.00993035  0.00454693]\n [-0.00525912 -0.00863321  0.00573321  0.00934029]\n [-0.00898329 -0.00437226  0.00984377  0.0093159 ]\n [-0.00898329 -0.00437226  0.00984377  0.0093159 ]\n [ 0.00647788  0.00697272  0.00098062 -0.0098719 ]\n [ 0.00955733 -0.00139923  0.00882936 -0.00520764]\n [ 0.00647788  0.00697272  0.00098062 -0.0098719 ]\n [-0.00525912 -0.00863321  0.00573321  0.00934029]\n [-0.00680077 -0.00235379 -0.00993035  0.00454693]\n [-0.00680077 -0.00235379 -0.00993035  0.00454693]\n [-0.00680077 -0.00235379 -0.00993035  0.00454693]\n [ 0.00647788  0.00697272  0.00098062 -0.0098719 ]\n [ 0.00955733 -0.00139923  0.00882936 -0.00520764]\n [ 0.0045059  -0.00414265  0.00838278  0.0053087 ]\n [ 0.0045059  -0.00414265  0.00838278  0.0053087 ]\n [-0.00898329 -0.00437226  0.00984377  0.0093159 ]\n [-0.00898329 -0.00437226  0.00984377  0.0093159 ]\n [ 0.00647788  0.00697272  0.00098062 -0.0098719 ]\n [ 0.0045059  -0.00414265  0.00838278  0.0053087 ]\n [-0.00525912 -0.00863321  0.00573321  0.00934029]\n [ 0.00647788  0.00697272  0.00098062 -0.0098719 ]\n [-0.00525912 -0.00863321  0.00573321  0.00934029]\n [-0.00898329 -0.00437226  0.00984377  0.0093159 ]\n [-0.00525912 -0.00863321  0.00573321  0.00934029]\n [-0.00680077 -0.00235379 -0.00993035  0.00454693]\n [ 0.0045059  -0.00414265  0.00838278  0.0053087 ]\n [ 0.00955733 -0.00139923  0.00882936 -0.00520764]\n [ 0.0045059  -0.00414265  0.00838278  0.0053087 ]\n [ 0.0045059  -0.00414265  0.00838278  0.0053087 ]\n [ 0.00647788  0.00697272  0.00098062 -0.0098719 ]\n [ 0.00955733 -0.00139923  0.00882936 -0.00520764]\n [ 0.0045059  -0.00414265  0.00838278  0.0053087 ]\n [-0.00525912 -0.00863321  0.00573321  0.00934029]\n [ 0.00647788  0.00697272  0.00098062 -0.0098719 ]\n [-0.00680077 -0.00235379 -0.00993035  0.00454693]\n [-0.00525912 -0.00863321  0.00573321  0.00934029]\n [ 0.0045059  -0.00414265  0.00838278  0.0053087 ]\n [ 0.00955733 -0.00139923  0.00882936 -0.00520764]]. The error was:\nTypeError: unhashable type: 'ArrayImpl'\n"
     ]
    }
   ],
   "source": [
    "episodes_reward = []\n",
    "critic_loss = 0\n",
    "actor_loss = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(episodes):\n",
    "    # Initialize a random process N for action exploration we do this in => noise()\n",
    "    \n",
    "    # Receive initial observation state s_1\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    episode_len = 0\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        # Select action a_t = μ(s t |θ μ ) + N t according to the current policy and exploration noise\n",
    "        action = actor.apply_fn(actor.params, state) + noise()\n",
    "        \n",
    "        # Execute action a t and observe reward r t and observe new state s t+1\n",
    "        observation, reward, terminated, truncated, _ = env.step(action)\n",
    "                \n",
    "        # Store transition (s t , a t , r t , s t+1 ) in R\n",
    "        transition = (state, action, reward, observation)\n",
    "        buffer.add(transition)\n",
    "                    \n",
    "        # Sample a random minibatch of N transitions (s i , a i , r i , s i+1 ) from R\n",
    "        # key, subkey = jax.random.split(key)\n",
    "        states, actions, rewards, next_states = buffer.sample_batch(key)\n",
    "\n",
    "        # Set y = r  + γQ^0 (s_{i+1} , μ^0 (s_{i+1} |θ^μ )|θ^Q ) P\n",
    "        target_action = target_actor.apply_fn(target_actor.params,\n",
    "                                              jnp.asarray(next_states))\n",
    "\n",
    "\n",
    "        target_q = target_critic.apply_fn(target_critic.params,\n",
    "                                          jnp.asarray(next_states),\n",
    "                                          jnp.asarray(target_action))\n",
    "\n",
    "        rewards = jnp.asarray(rewards)\n",
    "\n",
    "        # y = rewards + gamma * (1 - terminated) * target_q #corregir es un arreglo revisar paper\n",
    "        y = rewards + gamma * target_q #corregir es un arreglo revisar paper\n",
    "\n",
    "\n",
    "        # Update critic by minimizing the loss\n",
    "        critic, critic_loss = update_critic(critic,\n",
    "                                            jnp.asarray(states),\n",
    "                                            jnp.asarray(actions),\n",
    "                                            jnp.asarray(y))\n",
    "\n",
    "        # Update the actor policy using the sampled gradient:\n",
    "        actor, actor_loss = update_actor(actor,\n",
    "                                         critic,\n",
    "                                         jnp.asarray(states))\n",
    "\n",
    "        # Update the target networks:\n",
    "        target_actor_params = soft_update(target_actor.params, actor.params, tau)\n",
    "        target_critic_params = soft_update(target_critic.params, critic.params, tau)\n",
    "        \n",
    "        # update if the environment is done and the current observation\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        episode_len += 1\n",
    "        state = observation\n",
    "\n",
    "\n",
    "    episodes_reward.append(env.return_queue[-1]) \n",
    "    avg_reward = int(np.mean(env.return_queue))\n",
    "    \n",
    "\n",
    "    print(\"Episode:\", i+1)\n",
    "    \n",
    "    print(\"Average reward =>\", avg_reward,\n",
    "          \"Episode len =>\", episode_len,\n",
    "          \"Critic loss =>\", critic_loss, \n",
    "          \"Actor loss =>\", actor_loss, \"\\n\") \n",
    "\n",
    "env.close()\n",
    "\n",
    "\n",
    "# execution time\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(\"Execution time:\", execution_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d04fcd1-8596-4ac2-bc2b-4b7c625a7c02",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Save agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b42bf7-a946-4eb7-aa50-d4537c3fbda0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save agent\n",
    "orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
    "save_args = orbax_utils.save_args_from_target(ckpt)\n",
    "orbax_checkpointer.save('./agent', ckpt, save_args=save_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496fd34a",
   "metadata": {},
   "source": [
    "## Visualizing the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217629c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rewards_to_plot = [rewards for rewards in episodes_reward]\n",
    "\n",
    "plt.plot(range(episodes), episodes_reward)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Reward')\n",
    "plt.grid()\n",
    "plt.title('Rewards over episodes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5ff1e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896b582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore agent\n",
    "raw_restored = orbax_checkpointer.restore('./agent')\n",
    "actor_params = raw_restored['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53364c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"InvertedPendulum-v4\", render_mode=\"human\")\n",
    "\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(1000):\n",
    "    action = actor.apply_fn(actor_params, observation, env.action_space.shape[0])\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
